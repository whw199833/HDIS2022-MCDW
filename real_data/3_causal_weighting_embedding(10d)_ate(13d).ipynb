{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypgk9VLvSyWC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from gensim.models import word2vec\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import random\n",
        "random.seed(2022)\n",
        "np.random.seed(2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvg78-OAfWzb",
        "outputId": "7555f4c2-d89a-4d9b-b646-6ca50eda35d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: node2vec in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (0.4.4)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.1.2 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from node2vec) (4.2.0)\n",
            "Requirement already satisfied: networkx<3.0,>=2.5 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from node2vec) (2.6.3)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.5 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from node2vec) (1.20.3)\n",
            "Requirement already satisfied: joblib<2.0.0,>=1.1.0 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from node2vec) (1.1.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.55.1 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from node2vec) (4.62.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.7.1)\n",
            "Requirement already satisfied: gensim in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.20.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qlU_k_HFkNy"
      },
      "outputs": [],
      "source": [
        "df_train=pd.read_csv(\"criteo_sampled/criteo_train.csv\",index_col=0)\n",
        "df_test=pd.read_csv(\"criteo_sampled/criteo_test.csv\",index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI3sbjmrSyWF",
        "outputId": "317acbb6-c602-43be-fc2a-63bc904637f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-0.01388679662724193,\n",
              " 0.05071538371534517,\n",
              " 0.001328853128405821,\n",
              " -0.009331114629769072,\n",
              " 0.1933108932875426,\n",
              " 0.0008912893595676433,\n",
              " -0.0002958249515911775,\n",
              " -0.009201954095841196,\n",
              " 0.1034574925533489,\n",
              " 0.0741312293943781,\n",
              " -0.01664079180275199,\n",
              " 0.1331562899243786,\n",
              " 0.9310119697738223,\n",
              " 0.007447354046319239]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load causal weighting\n",
        "ate_list=[]\n",
        "ate=pd.read_excel(\"feats_ate_x13.xlsx\",index_col=0)\n",
        "for i in [c for c in df_train.columns[:-4]]+[\"visit\",\"treatment\"]:\n",
        "    ate_list.append(float(ate[ate['Feature']==i][\"ATE\"].values))\n",
        "ate_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXWeRuMvFkN1"
      },
      "outputs": [],
      "source": [
        "# load edge index\n",
        "edge_index=pd.read_csv('edge_index_criteo.csv')\n",
        "edge_index=torch.from_numpy(np.transpose(np.array(edge_index)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-pWo7KzFkN1",
        "outputId": "ae7ec32a-143f-4ec3-c0d3-52f70ee54b5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load node embedding\n",
        "model_dw=word2vec.Word2Vec.load(\"deepwalk_10d_x13.model\")\n",
        "model_n2v=word2vec.Word2Vec.load(\"Node2Vec_10d_x13.model\")\n",
        "\n",
        "lst_dw=[]\n",
        "lst_n2v=[]\n",
        "for i in range(14):\n",
        "    lst_dw.append(model_dw.wv[i])\n",
        "    lst_n2v.append(model_n2v.wv[i])\n",
        "len(lst_n2v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gyw5M55FkN2"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import entropy\n",
        "def R2(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    R2 = 1 - np.sum(np.square(y_actual-y_predicted)) / np.sum(np.square(y_actual-np.mean(y_actual)))\n",
        "    return R2\n",
        "\n",
        "def MAE(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    MAE = np.mean(abs(y_actual-y_predicted))\n",
        "    return MAE\n",
        "\n",
        "def RMSE(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    RMSE = np.sqrt(np.mean(np.square(y_actual-y_predicted)))\n",
        "    return RMSE\n",
        "\n",
        "def CVRMSE(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    RMSE = np.sqrt(np.mean(np.square(y_actual-y_predicted)))\n",
        "    CVRMSE = RMSE/np.mean(y_actual)\n",
        "    return CVRMSE\n",
        "\n",
        "def MAPE(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    MAPE=np.mean(abs((y_actual-y_predicted)/y_actual))\n",
        "    return MAPE\n",
        "\n",
        "def MSE(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    MSE = np.mean(abs(y_actual-y_predicted)**2)\n",
        "    return MSE\n",
        "\n",
        "def kl_divergence(p, q):\n",
        "\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    q = np.asarray(q, dtype=float)\n",
        "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
        "\n",
        "def kl_divergence(y_predicted, y_actual):\n",
        "\n",
        "    stacked_values = np.hstack((y_predicted, y_actual))\n",
        "    stacked_low = np.percentile(stacked_values, 0.1)\n",
        "    stacked_high = np.percentile(stacked_values, 99.9)\n",
        "    bins = np.linspace(stacked_low, stacked_high, 100)\n",
        "\n",
        "    distr = np.histogram(y_predicted, bins=bins)[0]\n",
        "    distr = np.clip(distr / distr.sum(), 0.001, 0.999)\n",
        "    true_distr = np.histogram( y_actual, bins=bins)[0]\n",
        "    true_distr = np.clip(true_distr / true_distr.sum(), 0.001, 0.999)\n",
        "\n",
        "    kl = entropy(distr, true_distr)\n",
        "    return kl "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "H3emQp0YFkN3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.data import InMemoryDataset, download_url\n",
        "\n",
        "\n",
        "class Mytrain(InMemoryDataset):\n",
        "    def __init__(self, root,transform=None, pre_transform=None):\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "       # ,self.edge_index,self.y=x,edge_index,y\n",
        "        \n",
        "    \n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return ['some_file_1', 'some_file_2', ...]\n",
        "    \n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "\n",
        "    def process(self):\n",
        "        data_list=[]\n",
        "        weighted_feats=[]\n",
        "        for i in range(x_train.shape[0]):\n",
        "            Edge_index = edge_index.type(torch.long)\n",
        "            X =x_train[i]\n",
        "            if feats_mode =='causal':\n",
        "                t=torch.zeros(14,25) # 25 dimensions = 1+10+14 = node number, node embedding, features\n",
        "              \n",
        "                causal_weighted= np.multiply(np.array(X),np.array(ate_list))\n",
        "                weighted_feats.append(causal_weighted)\n",
        "                \n",
        "                for j in range(14): # for j-th feature\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        list(causal_weighted) # feature (10d), causally weighted features        \n",
        "                                                        ))) \n",
        "            elif feats_mode =='equal':\n",
        "                t=torch.zeros(14,25) # 25 dimensions = 1+10+14 = node number, node embedding, features\n",
        "              \n",
        "                weighted= np.array(X)\n",
        "                weighted_feats.append(weighted)\n",
        "                \n",
        "                for j in range(14): # for j-th feature\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        list(weighted) # feature (10d), equally weighted features        \n",
        "                                                        ))) \n",
        "            else:\n",
        "                t=torch.zeros(14,11) # 11 dimensions = 1+10 = node number, node embedding\n",
        "                                    \n",
        "                \n",
        "                for j in range(14):\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        ))) \n",
        "                  \n",
        "\n",
        "            Y = y_train[i].reshape(-1,1).to(torch.float32)\n",
        "            data = Data(x=t, edge_index=Edge_index, y=Y)\n",
        "      \n",
        "            data_list.append(data)\n",
        "\n",
        "        if self.pre_filter is not None:\n",
        "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data_list = [self.pre_transform(data) for data in data_list]\n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "\n",
        "class Mytest(InMemoryDataset):\n",
        "    def __init__(self, root,transform=None, pre_transform=None):\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "       # ,self.edge_index,self.y=x,edge_index,y\n",
        "        \n",
        "    \n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return ['some_file_1', 'some_file_2', ...]\n",
        "    \n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "\n",
        "    def process(self):\n",
        "        data_list=[]\n",
        "        weighted_feats=[]\n",
        "        for i in range(x_test.shape[0]):\n",
        "            Edge_index = edge_index.type(torch.long)\n",
        "            X =x_test[i]\n",
        "\n",
        "            if feats_mode =='causal':\n",
        "                t=torch.zeros(14,25) # 25 dimensions = 1+10+14 = node number, node embedding, features\n",
        "              \n",
        "                causal_weighted= np.multiply(np.array(X),np.array(ate_list))\n",
        "                weighted_feats.append(causal_weighted)\n",
        "                \n",
        "                for j in range(14): # for j-th feature\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        list(causal_weighted) # feature (10d), causally weighted features        \n",
        "                                                        ))) \n",
        "            elif feats_mode =='equal':\n",
        "                t=torch.zeros(14,25) # 25 dimensions = 1+10+14 = node number, node embedding, features\n",
        "              \n",
        "                weighted= np.array(X)\n",
        "                weighted_feats.append(weighted)\n",
        "                \n",
        "                for j in range(14): # for j-th feature\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        list(weighted) # feature (10d), equally weighted features        \n",
        "                                                        ))) \n",
        "            else:\n",
        "                t=torch.zeros(14,11) # 11 dimensions = 1+10 = node number, node embedding\n",
        "                                    \n",
        "                \n",
        "                for j in range(14):\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        ))) \n",
        "            \n",
        "            Y = y_test[i].reshape(-1,1).to(torch.float32)\n",
        "            data = Data(x=t, edge_index=Edge_index, y=Y)\n",
        "        \n",
        "            data_list.append(data)\n",
        "\n",
        "        if self.pre_filter is not None:\n",
        "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data_list = [self.pre_transform(data) for data in data_list]\n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feats_mode='causal'\n",
        "#feats_mode ='equal' \n",
        "#feats_mode ='noweighting' \n",
        "\n",
        "col=df_train.columns\n",
        "y_train=torch.from_numpy(np.array(df_train['y'])).reshape(df_train.shape[0],1).to(torch.float32)\n",
        "x_train=torch.from_numpy(np.array(df_train[[i for i in col[:-4]]+[\"visit\",\"T\"]])).to(torch.float32)\n",
        "Mydata_train=Mytrain(\".\\mydata_sampled_ate\\MYdata_train\")\n",
        "\n",
        "y_test=torch.from_numpy(np.array(df_test['y'])).reshape(df_test.shape[0],1).to(torch.float32)\n",
        "x_test=torch.from_numpy(np.array(df_test[[i for i in col[:-4]]+[\"visit\",\"T\"]])).to(torch.float32)\n",
        "Mydata_test=Mytest(\".\\mydata_sampled_ate\\MYdata_test\")"
      ],
      "metadata": {
        "id": "vpIiW0Npfq5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dDvnHBgFkN6"
      },
      "source": [
        "# GCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKmQvnebFkN7"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "    y_actual = []\n",
        "    y_predicted = []\n",
        "    loss_all=0\n",
        "    for data in train_loader:\n",
        "        loss = 0\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        label = data.y.to(device)\n",
        "        loss = crit(output, label)\n",
        "        loss.backward()\n",
        "        loss_all += data.num_graphs * loss.item()\n",
        "        optimizer.step()\n",
        "        y_actual +=(label).cpu().detach().ravel().tolist()\n",
        "        y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "    \n",
        "    \n",
        "    loss=loss_all/len(Mydata_train)\n",
        "    r2=R2(y_predicted, y_actual)\n",
        "    mse = MSE(y_predicted, y_actual)\n",
        "    kl=kl_divergence(y_predicted, y_actual)\n",
        "\n",
        "    print(\"R2:%f\" % (R2(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MSE:%f\" % (MSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"KL:%f\" % (kl_divergence(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MAE:%f\" % (MAE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"RMSE:%f\" % (RMSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"CVRMSE:%f\" % (CVRMSE(y_predicted, y_actual)),end='  ')\n",
        "\n",
        "    return loss,r2,mse,kl\n",
        "\n",
        "\n",
        "def val():\n",
        "    model.eval()\n",
        "    y_actual = []\n",
        "    y_predicted = []\n",
        "    loss_all=0\n",
        "    for data in test_loader:\n",
        "      loss = 0\n",
        "      data = data.to(device)\n",
        "      output = model(data)\n",
        "      label = data.y.to(device)\n",
        "      y_actual +=(label).cpu().detach().ravel().tolist()\n",
        "      y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "      loss = crit(output, label)\n",
        "      loss_all += loss.item()\n",
        "\n",
        "    \n",
        "    loss = loss_all / len(Mydata_test)\n",
        "    r2=R2(y_predicted, y_actual)\n",
        "    mse = MSE(y_predicted, y_actual)\n",
        "    kl=kl_divergence(y_predicted, y_actual)\n",
        "\n",
        "    print(\"R2:%f\" % (R2(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MSE:%f\" % (MSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"KL:%f\" % (kl_divergence(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MAE:%f\" % (MAE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"RMSE:%f\" % (RMSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"CVRMSE:%f\" % (CVRMSE(y_predicted, y_actual)),end='  ')\n",
        "\n",
        "    return loss,r2, mse,kl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8VbbpTvFkN8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_scatter import scatter_add\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = GCNConv(25, 64)\n",
        "        self.conv2 = GCNConv(64, 10)\n",
        "        # self-attention layer\n",
        "        \n",
        "        self.f1 = torch.nn.Linear(140,32)\n",
        "        self.f2 = torch.nn.Linear(32,1)\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.attention\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = x.reshape(-1,140)\n",
        "        x = self.f1(x)\n",
        "        x = self.f2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvzm6UHUFkN8"
      },
      "outputs": [],
      "source": [
        "num_epochs = 2560\n",
        "batch_size = 512\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=5e-4)\n",
        "crit = F.mse_loss\n",
        "train_loader = DataLoader(Mydata_train, batch_size=batch_size)\n",
        "test_loader = DataLoader(Mydata_test, batch_size=512)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss,r2,mse,kl=train()\n",
        "    if epoch %5==0:      \n",
        "        print('train_loss:')\n",
        "        print(loss)\n",
        "\n",
        "        loss,r2,mse,kl=val()\n",
        "        print('test_loss:')\n",
        "        print(loss)\n",
        "\n",
        "y_predicted = []\n",
        "for data in test_loader:\n",
        "    loss = 0\n",
        "    data = data.to(device)\n",
        "    output = model(data)\n",
        "    label = data.y.to(device)\n",
        "    y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "df_test[\"y_hat\"]=y_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhsJfdeIFkN9",
        "outputId": "66c947e0-6595-456e-85f6-4823c4fd858a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.105320  MSE:0.002612  KL:0.745297  MAE:0.007095  RMSE:0.051111  CVRMSE:17.453546  train_loss:\n",
            "0.002612286880433142\n",
            "R2:0.118954  MSE:0.002656  KL:0.051733  MAE:0.005017  RMSE:0.051539  CVRMSE:17.043041  test_loss:\n",
            "5.217784955154607e-06\n",
            "R2:0.105390  MSE:0.002612  KL:0.714130  MAE:0.007091  RMSE:0.051109  CVRMSE:17.452867  R2:0.108151  MSE:0.002604  KL:0.539344  MAE:0.007064  RMSE:0.051030  CVRMSE:17.425910  R2:0.106468  MSE:0.002609  KL:0.862432  MAE:0.007137  RMSE:0.051078  CVRMSE:17.442346  R2:0.106643  MSE:0.002608  KL:0.731545  MAE:0.007117  RMSE:0.051073  CVRMSE:17.440633  R2:0.106307  MSE:0.002609  KL:0.880130  MAE:0.007083  RMSE:0.051082  CVRMSE:17.443920  train_loss:\n",
            "0.0026094063319495827\n",
            "R2:0.116539  MSE:0.002664  KL:0.052820  MAE:0.005135  RMSE:0.051610  CVRMSE:17.066383  test_loss:\n",
            "5.232232244290908e-06\n",
            "R2:0.104541  MSE:0.002615  KL:0.461058  MAE:0.007128  RMSE:0.051133  CVRMSE:17.461140  R2:0.106588  MSE:0.002609  KL:1.354241  MAE:0.007077  RMSE:0.051074  CVRMSE:17.441174  R2:0.105193  MSE:0.002613  KL:0.635896  MAE:0.007089  RMSE:0.051114  CVRMSE:17.454780  R2:0.105362  MSE:0.002612  KL:1.001935  MAE:0.007067  RMSE:0.051109  CVRMSE:17.453133  R2:0.103825  MSE:0.002617  KL:0.879093  MAE:0.007100  RMSE:0.051153  CVRMSE:17.468122  train_loss:\n",
            "0.002616652116799231\n",
            "R2:0.119716  MSE:0.002654  KL:0.053647  MAE:0.005539  RMSE:0.051517  CVRMSE:17.035678  test_loss:\n",
            "5.2130809291417805e-06\n",
            "R2:0.105088  MSE:0.002613  KL:0.674388  MAE:0.007125  RMSE:0.051117  CVRMSE:17.455813  R2:0.104444  MSE:0.002615  KL:0.777064  MAE:0.007105  RMSE:0.051136  CVRMSE:17.462091  R2:0.105317  MSE:0.002612  KL:0.830958  MAE:0.007128  RMSE:0.051111  CVRMSE:17.453576  R2:0.107065  MSE:0.002607  KL:0.855475  MAE:0.007106  RMSE:0.051061  CVRMSE:17.436515  R2:0.106404  MSE:0.002609  KL:0.551465  MAE:0.007048  RMSE:0.051080  CVRMSE:17.442971  train_loss:\n",
            "0.00260912233034309\n",
            "R2:0.114504  MSE:0.002670  KL:0.052877  MAE:0.005218  RMSE:0.051669  CVRMSE:17.086036  test_loss:\n",
            "5.244434791233882e-06\n",
            "R2:0.106670  MSE:0.002608  KL:0.583240  MAE:0.007072  RMSE:0.051072  CVRMSE:17.440371  R2:0.105351  MSE:0.002612  KL:0.643781  MAE:0.007081  RMSE:0.051110  CVRMSE:17.453242  R2:0.105927  MSE:0.002611  KL:0.650525  MAE:0.007077  RMSE:0.051093  CVRMSE:17.447627  R2:0.105795  MSE:0.002611  KL:0.952926  MAE:0.007110  RMSE:0.051097  CVRMSE:17.448907  R2:0.104624  MSE:0.002614  KL:0.843420  MAE:0.007125  RMSE:0.051130  CVRMSE:17.460336  train_loss:\n",
            "0.0026143197627689946\n",
            "R2:0.110914  MSE:0.002681  KL:0.052273  MAE:0.005056  RMSE:0.051774  CVRMSE:17.120629  test_loss:\n",
            "5.265951918588421e-06\n",
            "R2:0.105474  MSE:0.002612  KL:0.940023  MAE:0.007058  RMSE:0.051106  CVRMSE:17.452042  R2:0.106172  MSE:0.002610  KL:0.761842  MAE:0.007109  RMSE:0.051086  CVRMSE:17.445231  R2:0.106685  MSE:0.002608  KL:0.698404  MAE:0.007109  RMSE:0.051072  CVRMSE:17.440223  R2:0.105994  MSE:0.002610  KL:0.660246  MAE:0.007079  RMSE:0.051091  CVRMSE:17.446971  R2:0.105312  MSE:0.002612  KL:0.807752  MAE:0.007096  RMSE:0.051111  CVRMSE:17.453626  train_loss:\n",
            "0.0026123109762927646\n",
            "R2:0.117850  MSE:0.002660  KL:0.054115  MAE:0.005249  RMSE:0.051571  CVRMSE:17.053724  test_loss:\n",
            "5.22427945079856e-06\n",
            "R2:0.105642  MSE:0.002611  KL:0.994294  MAE:0.007093  RMSE:0.051101  CVRMSE:17.450408  R2:0.106262  MSE:0.002610  KL:0.719290  MAE:0.007074  RMSE:0.051084  CVRMSE:17.444356  R2:0.105902  MSE:0.002611  KL:0.692133  MAE:0.007079  RMSE:0.051094  CVRMSE:17.447865  R2:0.105450  MSE:0.002612  KL:0.821219  MAE:0.007106  RMSE:0.051107  CVRMSE:17.452275  R2:0.105515  MSE:0.002612  KL:0.836885  MAE:0.007140  RMSE:0.051105  CVRMSE:17.451640  train_loss:\n",
            "0.0026117164070052034\n",
            "R2:0.114404  MSE:0.002670  KL:0.053548  MAE:0.005274  RMSE:0.051672  CVRMSE:17.086997  test_loss:\n",
            "5.245015767846661e-06\n",
            "R2:0.105629  MSE:0.002611  KL:0.804992  MAE:0.007103  RMSE:0.051102  CVRMSE:17.450527  R2:0.105166  MSE:0.002613  KL:0.824049  MAE:0.007117  RMSE:0.051115  CVRMSE:17.455044  R2:0.105704  MSE:0.002611  KL:0.719918  MAE:0.007115  RMSE:0.051100  CVRMSE:17.449804  R2:0.105924  MSE:0.002611  KL:0.743436  MAE:0.007050  RMSE:0.051093  CVRMSE:17.447658  R2:0.105247  MSE:0.002612  KL:0.680523  MAE:0.007098  RMSE:0.051113  CVRMSE:17.454255  train_loss:\n",
            "0.002612499132911309\n",
            "R2:0.117979  MSE:0.002659  KL:0.050787  MAE:0.005408  RMSE:0.051568  CVRMSE:17.052477  test_loss:\n",
            "5.223677747190838e-06\n",
            "R2:0.108121  MSE:0.002604  KL:0.873046  MAE:0.007056  RMSE:0.051030  CVRMSE:17.426202  R2:0.106651  MSE:0.002608  KL:0.889191  MAE:0.007076  RMSE:0.051073  CVRMSE:17.440559  R2:0.106295  MSE:0.002609  KL:0.809419  MAE:0.007076  RMSE:0.051083  CVRMSE:17.444030  R2:0.105320  MSE:0.002612  KL:0.808709  MAE:0.007137  RMSE:0.051111  CVRMSE:17.453550  R2:0.106787  MSE:0.002608  KL:0.538451  MAE:0.007074  RMSE:0.051069  CVRMSE:17.439228  train_loss:\n",
            "0.002608002679538581\n",
            "R2:0.120892  MSE:0.002650  KL:0.053178  MAE:0.006100  RMSE:0.051482  CVRMSE:17.024287  test_loss:\n",
            "5.206015450426182e-06\n",
            "R2:0.107533  MSE:0.002606  KL:0.432716  MAE:0.007074  RMSE:0.051047  CVRMSE:17.431946  R2:0.105749  MSE:0.002611  KL:0.853610  MAE:0.007100  RMSE:0.051098  CVRMSE:17.449363  R2:0.103708  MSE:0.002617  KL:0.856692  MAE:0.007123  RMSE:0.051157  CVRMSE:17.469261  R2:0.105907  MSE:0.002611  KL:0.730858  MAE:0.007085  RMSE:0.051094  CVRMSE:17.447817  R2:0.105123  MSE:0.002613  KL:0.797479  MAE:0.007093  RMSE:0.051116  CVRMSE:17.455469  train_loss:\n",
            "0.0026128625422732034\n",
            "R2:0.118587  MSE:0.002657  KL:0.052483  MAE:0.005055  RMSE:0.051550  CVRMSE:17.046595  test_loss:\n",
            "5.219961918565567e-06\n",
            "R2:0.104570  MSE:0.002614  KL:1.127893  MAE:0.007112  RMSE:0.051132  CVRMSE:17.460863  R2:0.105058  MSE:0.002613  KL:0.959061  MAE:0.007091  RMSE:0.051118  CVRMSE:17.456103  R2:0.107010  MSE:0.002607  KL:0.671143  MAE:0.007066  RMSE:0.051062  CVRMSE:17.437052  R2:0.106287  MSE:0.002609  KL:0.767404  MAE:0.007091  RMSE:0.051083  CVRMSE:17.444107  R2:0.107045  MSE:0.002607  KL:0.581853  MAE:0.007084  RMSE:0.051061  CVRMSE:17.436712  train_loss:\n",
            "0.002607250398455658\n",
            "R2:0.119239  MSE:0.002655  KL:0.056028  MAE:0.005860  RMSE:0.051531  CVRMSE:17.040292  test_loss:\n",
            "5.215883658256654e-06\n",
            "R2:0.106092  MSE:0.002610  KL:0.953403  MAE:0.007077  RMSE:0.051088  CVRMSE:17.446013  R2:0.106344  MSE:0.002609  KL:0.891140  MAE:0.007119  RMSE:0.051081  CVRMSE:17.443551  R2:0.104935  MSE:0.002613  KL:0.744062  MAE:0.007078  RMSE:0.051122  CVRMSE:17.457302  R2:0.106267  MSE:0.002610  KL:0.960558  MAE:0.007115  RMSE:0.051083  CVRMSE:17.444303  R2:0.104877  MSE:0.002614  KL:0.882942  MAE:0.007126  RMSE:0.051123  CVRMSE:17.457863  train_loss:\n",
            "0.0026135792634751896\n",
            "R2:0.117064  MSE:0.002662  KL:0.052549  MAE:0.005035  RMSE:0.051594  CVRMSE:17.061319  test_loss:\n",
            "5.2290715775689675e-06\n",
            "R2:0.105462  MSE:0.002612  KL:0.640861  MAE:0.007072  RMSE:0.051106  CVRMSE:17.452157  R2:0.104432  MSE:0.002615  KL:0.820483  MAE:0.007091  RMSE:0.051136  CVRMSE:17.462208  R2:0.106390  MSE:0.002609  KL:0.954895  MAE:0.007121  RMSE:0.051080  CVRMSE:17.443109  R2:0.103971  MSE:0.002616  KL:0.673062  MAE:0.007091  RMSE:0.051149  CVRMSE:17.466700  R2:0.104021  MSE:0.002616  KL:0.736166  MAE:0.007097  RMSE:0.051148  CVRMSE:17.466207  train_loss:\n",
            "0.002616078234338934\n",
            "R2:0.117097  MSE:0.002662  KL:0.051926  MAE:0.005015  RMSE:0.051593  CVRMSE:17.061000  test_loss:\n",
            "5.2288844199723535e-06\n",
            "R2:0.105155  MSE:0.002613  KL:0.642792  MAE:0.007094  RMSE:0.051115  CVRMSE:17.455158  R2:0.104927  MSE:0.002613  KL:0.737801  MAE:0.007073  RMSE:0.051122  CVRMSE:17.457374  R2:0.103826  MSE:0.002617  KL:0.956538  MAE:0.007111  RMSE:0.051153  CVRMSE:17.468115  R2:0.105190  MSE:0.002613  KL:0.695056  MAE:0.007098  RMSE:0.051114  CVRMSE:17.454817  R2:0.105806  MSE:0.002611  KL:0.914845  MAE:0.007100  RMSE:0.051097  CVRMSE:17.448809  train_loss:\n",
            "0.002610869115183449\n",
            "R2:0.113918  MSE:0.002671  KL:0.052556  MAE:0.004823  RMSE:0.051686  CVRMSE:17.091687  test_loss:\n",
            "5.247962043631406e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.107520  MSE:0.002606  KL:0.680402  MAE:0.007082  RMSE:0.051048  CVRMSE:17.432072  R2:0.103540  MSE:0.002617  KL:1.003745  MAE:0.007120  RMSE:0.051161  CVRMSE:17.470901  R2:0.103894  MSE:0.002616  KL:0.886429  MAE:0.007085  RMSE:0.051151  CVRMSE:17.467453  R2:0.105056  MSE:0.002613  KL:0.839914  MAE:0.007086  RMSE:0.051118  CVRMSE:17.456124  R2:0.105354  MSE:0.002612  KL:0.595972  MAE:0.007100  RMSE:0.051110  CVRMSE:17.453216  train_loss:\n",
            "0.002612188061713194\n",
            "R2:0.115847  MSE:0.002666  KL:0.049730  MAE:0.004786  RMSE:0.051630  CVRMSE:17.073073  test_loss:\n",
            "5.2364522453798605e-06\n",
            "R2:0.105749  MSE:0.002611  KL:1.016499  MAE:0.007079  RMSE:0.051098  CVRMSE:17.449361  R2:0.104809  MSE:0.002614  KL:0.570558  MAE:0.007074  RMSE:0.051125  CVRMSE:17.458526  R2:0.104898  MSE:0.002614  KL:0.775535  MAE:0.007112  RMSE:0.051123  CVRMSE:17.457664  R2:0.106508  MSE:0.002609  KL:0.700011  MAE:0.007105  RMSE:0.051077  CVRMSE:17.441957  R2:0.106149  MSE:0.002610  KL:0.632322  MAE:0.007096  RMSE:0.051087  CVRMSE:17.445453  train_loss:\n",
            "0.0026098649692246774\n",
            "R2:0.115667  MSE:0.002666  KL:0.051986  MAE:0.005481  RMSE:0.051635  CVRMSE:17.074810  test_loss:\n",
            "5.237474473597856e-06\n",
            "R2:0.106418  MSE:0.002609  KL:0.756990  MAE:0.007088  RMSE:0.051079  CVRMSE:17.442834  R2:0.105814  MSE:0.002611  KL:0.704740  MAE:0.007094  RMSE:0.051096  CVRMSE:17.448729  R2:0.105484  MSE:0.002612  KL:0.663548  MAE:0.007106  RMSE:0.051106  CVRMSE:17.451948  R2:0.107487  MSE:0.002606  KL:0.848422  MAE:0.007093  RMSE:0.051049  CVRMSE:17.432394  R2:0.105816  MSE:0.002611  KL:0.982211  MAE:0.007101  RMSE:0.051096  CVRMSE:17.448703  train_loss:\n",
            "0.002610837369176415\n",
            "R2:0.118675  MSE:0.002657  KL:0.053542  MAE:0.005183  RMSE:0.051547  CVRMSE:17.045743  test_loss:\n",
            "5.219364352968116e-06\n",
            "R2:0.105131  MSE:0.002613  KL:0.802299  MAE:0.007090  RMSE:0.051116  CVRMSE:17.455390  R2:0.106170  MSE:0.002610  KL:0.693683  MAE:0.007067  RMSE:0.051086  CVRMSE:17.445255  R2:0.106189  MSE:0.002610  KL:0.573543  MAE:0.007078  RMSE:0.051086  CVRMSE:17.445066  R2:0.106942  MSE:0.002608  KL:1.042316  MAE:0.007085  RMSE:0.051064  CVRMSE:17.437718  R2:0.106444  MSE:0.002609  KL:0.544031  MAE:0.007109  RMSE:0.051078  CVRMSE:17.442577  train_loss:\n",
            "0.002609004625651759\n",
            "R2:0.122442  MSE:0.002646  KL:0.051731  MAE:0.005548  RMSE:0.051437  CVRMSE:17.009278  test_loss:\n",
            "5.196734125384302e-06\n",
            "R2:0.105610  MSE:0.002611  KL:0.695289  MAE:0.007111  RMSE:0.051102  CVRMSE:17.450718  R2:0.106402  MSE:0.002609  KL:0.770419  MAE:0.007090  RMSE:0.051080  CVRMSE:17.442986  R2:0.106479  MSE:0.002609  KL:0.533163  MAE:0.007087  RMSE:0.051077  CVRMSE:17.442239  R2:0.104917  MSE:0.002613  KL:0.764807  MAE:0.007134  RMSE:0.051122  CVRMSE:17.457480  R2:0.104185  MSE:0.002616  KL:0.896155  MAE:0.007093  RMSE:0.051143  CVRMSE:17.464608  train_loss:\n",
            "0.0026155993678074374\n",
            "R2:0.121140  MSE:0.002650  KL:0.054798  MAE:0.006019  RMSE:0.051475  CVRMSE:17.021893  test_loss:\n",
            "5.204399627192424e-06\n",
            "R2:0.105478  MSE:0.002612  KL:0.459486  MAE:0.007092  RMSE:0.051106  CVRMSE:17.452007  R2:0.106031  MSE:0.002610  KL:0.623391  MAE:0.007102  RMSE:0.051090  CVRMSE:17.446610  R2:0.107328  MSE:0.002606  KL:0.908610  MAE:0.007104  RMSE:0.051053  CVRMSE:17.433950  R2:0.105637  MSE:0.002611  KL:0.709426  MAE:0.007083  RMSE:0.051101  CVRMSE:17.450452  R2:0.104998  MSE:0.002613  KL:0.732471  MAE:0.007065  RMSE:0.051120  CVRMSE:17.456687  train_loss:\n",
            "0.0026132271313637095\n",
            "R2:0.119056  MSE:0.002656  KL:0.052494  MAE:0.005313  RMSE:0.051536  CVRMSE:17.042059  test_loss:\n",
            "5.217160520102936e-06\n",
            "R2:0.106362  MSE:0.002609  KL:0.748438  MAE:0.007102  RMSE:0.051081  CVRMSE:17.443377  R2:0.107307  MSE:0.002606  KL:0.613306  MAE:0.007064  RMSE:0.051054  CVRMSE:17.434154  R2:0.105485  MSE:0.002612  KL:0.789343  MAE:0.007102  RMSE:0.051106  CVRMSE:17.451933  R2:0.103989  MSE:0.002616  KL:0.777642  MAE:0.007099  RMSE:0.051149  CVRMSE:17.466525  R2:0.107314  MSE:0.002606  KL:0.618787  MAE:0.007090  RMSE:0.051054  CVRMSE:17.434082  train_loss:\n",
            "0.0026064638741547038\n",
            "R2:0.121927  MSE:0.002647  KL:0.052241  MAE:0.005816  RMSE:0.051452  CVRMSE:17.014266  test_loss:\n",
            "5.199793820348742e-06\n",
            "R2:0.104558  MSE:0.002615  KL:0.738592  MAE:0.007099  RMSE:0.051132  CVRMSE:17.460977  R2:0.107118  MSE:0.002607  KL:0.743766  MAE:0.007064  RMSE:0.051059  CVRMSE:17.435996  R2:0.104602  MSE:0.002614  KL:0.498089  MAE:0.007129  RMSE:0.051131  CVRMSE:17.460543  R2:0.105197  MSE:0.002613  KL:0.790264  MAE:0.007126  RMSE:0.051114  CVRMSE:17.454741  R2:0.105576  MSE:0.002612  KL:0.947745  MAE:0.007098  RMSE:0.051103  CVRMSE:17.451051  train_loss:\n",
            "0.002611540085197423\n",
            "R2:0.117943  MSE:0.002659  KL:0.053323  MAE:0.005537  RMSE:0.051569  CVRMSE:17.052817  test_loss:\n",
            "5.223756926685379e-06\n",
            "R2:0.106020  MSE:0.002610  KL:0.387038  MAE:0.007049  RMSE:0.051091  CVRMSE:17.446712  R2:0.105968  MSE:0.002610  KL:0.591613  MAE:0.007070  RMSE:0.051092  CVRMSE:17.447228  R2:0.104409  MSE:0.002615  KL:0.730652  MAE:0.007074  RMSE:0.051137  CVRMSE:17.462428  R2:0.106712  MSE:0.002608  KL:0.835645  MAE:0.007122  RMSE:0.051071  CVRMSE:17.439962  R2:0.105641  MSE:0.002611  KL:0.550508  MAE:0.007110  RMSE:0.051101  CVRMSE:17.450415  train_loss:\n",
            "0.0026113499472313006\n",
            "R2:0.120624  MSE:0.002651  KL:0.055030  MAE:0.005813  RMSE:0.051490  CVRMSE:17.026886  test_loss:\n",
            "5.207584079748515e-06\n",
            "R2:0.105301  MSE:0.002612  KL:0.786503  MAE:0.007090  RMSE:0.051111  CVRMSE:17.453735  R2:0.106897  MSE:0.002608  KL:0.786629  MAE:0.007092  RMSE:0.051065  CVRMSE:17.438160  R2:0.104151  MSE:0.002616  KL:0.721218  MAE:0.007127  RMSE:0.051144  CVRMSE:17.464946  R2:0.106164  MSE:0.002610  KL:0.995712  MAE:0.007079  RMSE:0.051086  CVRMSE:17.445313  R2:0.106357  MSE:0.002609  KL:0.774450  MAE:0.007096  RMSE:0.051081  CVRMSE:17.443423  train_loss:\n",
            "0.002609257706283646\n",
            "R2:0.115969  MSE:0.002665  KL:0.052837  MAE:0.005168  RMSE:0.051626  CVRMSE:17.071888  test_loss:\n",
            "5.235637849849586e-06\n",
            "R2:0.104729  MSE:0.002614  KL:0.731000  MAE:0.007095  RMSE:0.051127  CVRMSE:17.459306  R2:0.105553  MSE:0.002612  KL:0.855306  MAE:0.007097  RMSE:0.051104  CVRMSE:17.451276  R2:0.104371  MSE:0.002615  KL:0.551162  MAE:0.007121  RMSE:0.051138  CVRMSE:17.462800  R2:0.104992  MSE:0.002613  KL:0.625669  MAE:0.007109  RMSE:0.051120  CVRMSE:17.456741  R2:0.105166  MSE:0.002613  KL:0.963991  MAE:0.007077  RMSE:0.051115  CVRMSE:17.455050  train_loss:\n",
            "0.002612737085648904\n",
            "R2:0.118000  MSE:0.002659  KL:0.053040  MAE:0.005886  RMSE:0.051567  CVRMSE:17.052274  test_loss:\n",
            "5.223425249622069e-06\n",
            "R2:0.105945  MSE:0.002610  KL:1.143621  MAE:0.007129  RMSE:0.051093  CVRMSE:17.447444  R2:0.103739  MSE:0.002617  KL:0.603534  MAE:0.007135  RMSE:0.051156  CVRMSE:17.468964  R2:0.105927  MSE:0.002611  KL:0.538320  MAE:0.007114  RMSE:0.051093  CVRMSE:17.447627  R2:0.105584  MSE:0.002612  KL:0.719891  MAE:0.007080  RMSE:0.051103  CVRMSE:17.450966  R2:0.105592  MSE:0.002611  KL:0.579696  MAE:0.007072  RMSE:0.051103  CVRMSE:17.450893  train_loss:\n",
            "0.00261149283814439\n",
            "R2:0.119330  MSE:0.002655  KL:0.052220  MAE:0.005111  RMSE:0.051528  CVRMSE:17.039406  test_loss:\n",
            "5.21547018170985e-06\n",
            "R2:0.106545  MSE:0.002609  KL:0.715391  MAE:0.007073  RMSE:0.051076  CVRMSE:17.441591  R2:0.106441  MSE:0.002609  KL:0.692235  MAE:0.007108  RMSE:0.051078  CVRMSE:17.442604  R2:0.106919  MSE:0.002608  KL:0.860993  MAE:0.007105  RMSE:0.051065  CVRMSE:17.437945  R2:0.105537  MSE:0.002612  KL:0.768856  MAE:0.007090  RMSE:0.051104  CVRMSE:17.451428  R2:0.106428  MSE:0.002609  KL:0.674008  MAE:0.007079  RMSE:0.051079  CVRMSE:17.442734  train_loss:\n",
            "0.002609051520107899\n",
            "R2:0.119513  MSE:0.002655  KL:0.053841  MAE:0.005561  RMSE:0.051523  CVRMSE:17.037642  test_loss:\n",
            "5.21425956206662e-06\n",
            "R2:0.107020  MSE:0.002607  KL:0.838054  MAE:0.007083  RMSE:0.051062  CVRMSE:17.436954  R2:0.106058  MSE:0.002610  KL:0.977097  MAE:0.007124  RMSE:0.051089  CVRMSE:17.446349  R2:0.106466  MSE:0.002609  KL:0.663162  MAE:0.007089  RMSE:0.051078  CVRMSE:17.442365  R2:0.105580  MSE:0.002612  KL:0.803249  MAE:0.007114  RMSE:0.051103  CVRMSE:17.451009  R2:0.104777  MSE:0.002614  KL:0.678808  MAE:0.007079  RMSE:0.051126  CVRMSE:17.458838  train_loss:\n",
            "0.0026138712120006994\n",
            "R2:0.118030  MSE:0.002659  KL:0.052937  MAE:0.005650  RMSE:0.051566  CVRMSE:17.051984  test_loss:\n",
            "5.223274730356827e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.108192  MSE:0.002604  KL:0.649563  MAE:0.007091  RMSE:0.051028  CVRMSE:17.425511  R2:0.105347  MSE:0.002612  KL:0.920678  MAE:0.007114  RMSE:0.051110  CVRMSE:17.453278  R2:0.105730  MSE:0.002611  KL:0.744888  MAE:0.007101  RMSE:0.051099  CVRMSE:17.449545  R2:0.105781  MSE:0.002611  KL:0.827922  MAE:0.007077  RMSE:0.051097  CVRMSE:17.449044  R2:0.107289  MSE:0.002607  KL:0.764855  MAE:0.007053  RMSE:0.051054  CVRMSE:17.434330  train_loss:\n",
            "0.0026065378713083644\n",
            "R2:0.118887  MSE:0.002656  KL:0.051814  MAE:0.005392  RMSE:0.051541  CVRMSE:17.043697  test_loss:\n",
            "5.218185796272565e-06\n",
            "R2:0.106766  MSE:0.002608  KL:0.788523  MAE:0.007106  RMSE:0.051069  CVRMSE:17.439438  R2:0.107390  MSE:0.002606  KL:0.753475  MAE:0.007080  RMSE:0.051051  CVRMSE:17.433341  R2:0.107386  MSE:0.002606  KL:0.835855  MAE:0.007105  RMSE:0.051051  CVRMSE:17.433379  R2:0.106288  MSE:0.002609  KL:0.606328  MAE:0.007121  RMSE:0.051083  CVRMSE:17.444105  R2:0.105284  MSE:0.002612  KL:0.539918  MAE:0.007081  RMSE:0.051112  CVRMSE:17.453897  train_loss:\n",
            "0.0026123919411710625\n",
            "R2:0.121354  MSE:0.002649  KL:0.053179  MAE:0.005375  RMSE:0.051469  CVRMSE:17.019813  test_loss:\n",
            "5.2032898789471325e-06\n",
            "R2:0.106024  MSE:0.002610  KL:0.561604  MAE:0.007046  RMSE:0.051090  CVRMSE:17.446677  R2:0.103714  MSE:0.002617  KL:0.846150  MAE:0.007095  RMSE:0.051156  CVRMSE:17.469199  R2:0.104750  MSE:0.002614  KL:0.865597  MAE:0.007105  RMSE:0.051127  CVRMSE:17.459105  R2:0.106682  MSE:0.002608  KL:0.783374  MAE:0.007100  RMSE:0.051072  CVRMSE:17.440258  R2:0.105899  MSE:0.002611  KL:0.742702  MAE:0.007084  RMSE:0.051094  CVRMSE:17.447897  train_loss:\n",
            "0.002610596376038668\n",
            "R2:0.118327  MSE:0.002658  KL:0.053497  MAE:0.005539  RMSE:0.051557  CVRMSE:17.049106  test_loss:\n",
            "5.221424136149018e-06\n",
            "R2:0.106385  MSE:0.002609  KL:0.864032  MAE:0.007089  RMSE:0.051080  CVRMSE:17.443152  R2:0.105173  MSE:0.002613  KL:0.719175  MAE:0.007091  RMSE:0.051115  CVRMSE:17.454978  R2:0.107484  MSE:0.002606  KL:0.709942  MAE:0.007076  RMSE:0.051049  CVRMSE:17.432428  R2:0.105610  MSE:0.002611  KL:0.645812  MAE:0.007058  RMSE:0.051102  CVRMSE:17.450721  R2:0.107616  MSE:0.002606  KL:1.025893  MAE:0.007095  RMSE:0.051045  CVRMSE:17.431140  train_loss:\n",
            "0.002605584100371928\n",
            "R2:0.119573  MSE:0.002654  KL:0.052311  MAE:0.005683  RMSE:0.051521  CVRMSE:17.037057  test_loss:\n",
            "5.214028929678608e-06\n",
            "R2:0.106446  MSE:0.002609  KL:1.196095  MAE:0.007096  RMSE:0.051078  CVRMSE:17.442563  R2:0.107704  MSE:0.002605  KL:0.780700  MAE:0.007103  RMSE:0.051042  CVRMSE:17.430278  R2:0.105868  MSE:0.002611  KL:0.978718  MAE:0.007131  RMSE:0.051095  CVRMSE:17.448204  R2:0.105694  MSE:0.002611  KL:0.657300  MAE:0.007112  RMSE:0.051100  CVRMSE:17.449894  R2:0.105820  MSE:0.002611  KL:0.850014  MAE:0.007101  RMSE:0.051096  CVRMSE:17.448664  train_loss:\n",
            "0.0026108257514459512\n",
            "R2:0.118851  MSE:0.002657  KL:0.050971  MAE:0.005087  RMSE:0.051542  CVRMSE:17.044041  test_loss:\n",
            "5.218453277691078e-06\n",
            "R2:0.105942  MSE:0.002610  KL:0.876185  MAE:0.007073  RMSE:0.051093  CVRMSE:17.447480  R2:0.106597  MSE:0.002609  KL:0.923626  MAE:0.007125  RMSE:0.051074  CVRMSE:17.441089  R2:0.106322  MSE:0.002609  KL:0.657535  MAE:0.007117  RMSE:0.051082  CVRMSE:17.443770  R2:0.105227  MSE:0.002613  KL:0.698507  MAE:0.007059  RMSE:0.051113  CVRMSE:17.454454  R2:0.104270  MSE:0.002615  KL:1.110638  MAE:0.007104  RMSE:0.051141  CVRMSE:17.463786  train_loss:\n",
            "0.0026153530698883654\n",
            "R2:0.116724  MSE:0.002663  KL:0.054513  MAE:0.005222  RMSE:0.051604  CVRMSE:17.064605  test_loss:\n",
            "5.231015625311049e-06\n",
            "R2:0.105417  MSE:0.002612  KL:0.683608  MAE:0.007083  RMSE:0.051108  CVRMSE:17.452602  R2:0.104236  MSE:0.002615  KL:0.720691  MAE:0.007115  RMSE:0.051141  CVRMSE:17.464114  R2:0.104012  MSE:0.002616  KL:0.669888  MAE:0.007089  RMSE:0.051148  CVRMSE:17.466296  R2:0.104848  MSE:0.002614  KL:0.548790  MAE:0.007039  RMSE:0.051124  CVRMSE:17.458152  R2:0.106172  MSE:0.002610  KL:1.068438  MAE:0.007085  RMSE:0.051086  CVRMSE:17.445237  train_loss:\n",
            "0.002609800212219713\n",
            "R2:0.118710  MSE:0.002657  KL:0.051709  MAE:0.005026  RMSE:0.051546  CVRMSE:17.045406  test_loss:\n",
            "5.2192436419629885e-06\n",
            "R2:0.105858  MSE:0.002611  KL:0.714944  MAE:0.007046  RMSE:0.051095  CVRMSE:17.448295  R2:0.103904  MSE:0.002616  KL:0.708670  MAE:0.007143  RMSE:0.051151  CVRMSE:17.467348  R2:0.106063  MSE:0.002610  KL:0.727354  MAE:0.007121  RMSE:0.051089  CVRMSE:17.446298  R2:0.105630  MSE:0.002611  KL:0.620209  MAE:0.007083  RMSE:0.051102  CVRMSE:17.450526  R2:0.107083  MSE:0.002607  KL:1.013623  MAE:0.007119  RMSE:0.051060  CVRMSE:17.436342  train_loss:\n",
            "0.00260713975919543\n",
            "R2:0.117800  MSE:0.002660  KL:0.051636  MAE:0.005345  RMSE:0.051573  CVRMSE:17.054207  test_loss:\n",
            "5.224782245618095e-06\n",
            "R2:0.106866  MSE:0.002608  KL:0.593104  MAE:0.007084  RMSE:0.051066  CVRMSE:17.438461  R2:0.105943  MSE:0.002610  KL:0.796504  MAE:0.007063  RMSE:0.051093  CVRMSE:17.447467  R2:0.107050  MSE:0.002607  KL:0.527532  MAE:0.007085  RMSE:0.051061  CVRMSE:17.436661  R2:0.106984  MSE:0.002607  KL:0.805422  MAE:0.007100  RMSE:0.051063  CVRMSE:17.437304  R2:0.105993  MSE:0.002610  KL:0.819141  MAE:0.007071  RMSE:0.051091  CVRMSE:17.446979  train_loss:\n",
            "0.0026103216871394357\n",
            "R2:0.115526  MSE:0.002667  KL:0.054039  MAE:0.005072  RMSE:0.051639  CVRMSE:17.076169  test_loss:\n",
            "5.238216227838477e-06\n",
            "R2:0.104855  MSE:0.002614  KL:0.500657  MAE:0.007080  RMSE:0.051124  CVRMSE:17.458079  R2:0.105521  MSE:0.002612  KL:0.630497  MAE:0.007138  RMSE:0.051105  CVRMSE:17.451581  R2:0.107419  MSE:0.002606  KL:0.673657  MAE:0.007077  RMSE:0.051051  CVRMSE:17.433058  R2:0.107617  MSE:0.002606  KL:0.722197  MAE:0.007031  RMSE:0.051045  CVRMSE:17.431123  R2:0.106354  MSE:0.002609  KL:0.970251  MAE:0.007112  RMSE:0.051081  CVRMSE:17.443459  train_loss:\n",
            "0.002609268472948122\n",
            "R2:0.118761  MSE:0.002657  KL:0.051030  MAE:0.004964  RMSE:0.051545  CVRMSE:17.044913  test_loss:\n",
            "5.219021667470613e-06\n",
            "R2:0.104901  MSE:0.002614  KL:0.569413  MAE:0.007106  RMSE:0.051123  CVRMSE:17.457637  R2:0.107066  MSE:0.002607  KL:0.762763  MAE:0.007073  RMSE:0.051061  CVRMSE:17.436504  R2:0.106901  MSE:0.002608  KL:0.844877  MAE:0.007129  RMSE:0.051065  CVRMSE:17.438114  R2:0.105558  MSE:0.002612  KL:0.548761  MAE:0.007069  RMSE:0.051104  CVRMSE:17.451219  R2:0.104962  MSE:0.002613  KL:0.733140  MAE:0.007081  RMSE:0.051121  CVRMSE:17.457033  train_loss:\n",
            "0.002613330734841916\n",
            "R2:0.118438  MSE:0.002658  KL:0.051918  MAE:0.005383  RMSE:0.051554  CVRMSE:17.048035  test_loss:\n",
            "5.220854361549362e-06\n",
            "R2:0.105333  MSE:0.002612  KL:0.745795  MAE:0.007065  RMSE:0.051110  CVRMSE:17.453419  R2:0.105800  MSE:0.002611  KL:0.971593  MAE:0.007091  RMSE:0.051097  CVRMSE:17.448862  R2:0.106772  MSE:0.002608  KL:0.838026  MAE:0.007098  RMSE:0.051069  CVRMSE:17.439374  R2:0.107114  MSE:0.002607  KL:0.813854  MAE:0.007082  RMSE:0.051059  CVRMSE:17.436040  R2:0.104553  MSE:0.002615  KL:0.826194  MAE:0.007085  RMSE:0.051132  CVRMSE:17.461030  train_loss:\n",
            "0.0026145275973812896\n",
            "R2:0.115884  MSE:0.002666  KL:0.054246  MAE:0.005186  RMSE:0.051629  CVRMSE:17.072714  test_loss:\n",
            "5.23609669556722e-06\n",
            "R2:0.104291  MSE:0.002615  KL:0.792971  MAE:0.007060  RMSE:0.051140  CVRMSE:17.463581  R2:0.105501  MSE:0.002612  KL:0.662363  MAE:0.007079  RMSE:0.051105  CVRMSE:17.451784  R2:0.105920  MSE:0.002611  KL:0.861638  MAE:0.007144  RMSE:0.051093  CVRMSE:17.447694  R2:0.103397  MSE:0.002618  KL:0.768243  MAE:0.007096  RMSE:0.051165  CVRMSE:17.472293  R2:0.106210  MSE:0.002610  KL:0.479191  MAE:0.007121  RMSE:0.051085  CVRMSE:17.444865  train_loss:\n",
            "0.002609689157778494\n",
            "R2:0.116784  MSE:0.002663  KL:0.051555  MAE:0.004886  RMSE:0.051602  CVRMSE:17.064021  test_loss:\n",
            "5.2308414218138405e-06\n",
            "R2:0.104871  MSE:0.002614  KL:0.981190  MAE:0.007117  RMSE:0.051123  CVRMSE:17.457921  R2:0.106135  MSE:0.002610  KL:0.848897  MAE:0.007113  RMSE:0.051087  CVRMSE:17.445589  R2:0.106161  MSE:0.002610  KL:0.663083  MAE:0.007055  RMSE:0.051087  CVRMSE:17.445343  R2:0.105667  MSE:0.002611  KL:0.604626  MAE:0.007089  RMSE:0.051101  CVRMSE:17.450164  R2:0.106078  MSE:0.002610  KL:0.602655  MAE:0.007068  RMSE:0.051089  CVRMSE:17.446147  train_loss:\n",
            "0.0026100726488712915\n",
            "R2:0.116462  MSE:0.002664  KL:0.054175  MAE:0.005393  RMSE:0.051612  CVRMSE:17.067135  test_loss:\n",
            "5.232643787709312e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.104656  MSE:0.002614  KL:0.734616  MAE:0.007106  RMSE:0.051130  CVRMSE:17.460023  R2:0.106312  MSE:0.002609  KL:0.937904  MAE:0.007080  RMSE:0.051082  CVRMSE:17.443862  R2:0.106651  MSE:0.002608  KL:0.980654  MAE:0.007121  RMSE:0.051073  CVRMSE:17.440559  R2:0.107924  MSE:0.002605  KL:0.617047  MAE:0.007052  RMSE:0.051036  CVRMSE:17.428128  R2:0.105703  MSE:0.002611  KL:0.891235  MAE:0.007062  RMSE:0.051100  CVRMSE:17.449813  train_loss:\n",
            "0.002611169578775699\n",
            "R2:0.113129  MSE:0.002674  KL:0.055513  MAE:0.004984  RMSE:0.051709  CVRMSE:17.099288  test_loss:\n",
            "5.2526312532136404e-06\n",
            "R2:0.106314  MSE:0.002609  KL:0.566312  MAE:0.007105  RMSE:0.051082  CVRMSE:17.443849  R2:0.106753  MSE:0.002608  KL:0.714857  MAE:0.007087  RMSE:0.051070  CVRMSE:17.439567  R2:0.104381  MSE:0.002615  KL:0.864122  MAE:0.007094  RMSE:0.051137  CVRMSE:17.462700  R2:0.105543  MSE:0.002612  KL:0.693670  MAE:0.007163  RMSE:0.051104  CVRMSE:17.451374  R2:0.106413  MSE:0.002609  KL:0.805046  MAE:0.007066  RMSE:0.051079  CVRMSE:17.442882  train_loss:\n",
            "0.002609095739081916\n",
            "R2:0.119022  MSE:0.002656  KL:0.053801  MAE:0.005570  RMSE:0.051537  CVRMSE:17.042389  test_loss:\n",
            "5.217276062128726e-06\n",
            "R2:0.106483  MSE:0.002609  KL:0.700129  MAE:0.007096  RMSE:0.051077  CVRMSE:17.442201  R2:0.104825  MSE:0.002614  KL:0.714106  MAE:0.007089  RMSE:0.051125  CVRMSE:17.458373  R2:0.106867  MSE:0.002608  KL:0.624897  MAE:0.007072  RMSE:0.051066  CVRMSE:17.438446  R2:0.106198  MSE:0.002610  KL:0.800840  MAE:0.007110  RMSE:0.051085  CVRMSE:17.444976  R2:0.106407  MSE:0.002609  KL:0.801525  MAE:0.007109  RMSE:0.051079  CVRMSE:17.442939  train_loss:\n",
            "0.0026091129426421555\n",
            "R2:0.115668  MSE:0.002666  KL:0.051062  MAE:0.005057  RMSE:0.051635  CVRMSE:17.074796  test_loss:\n",
            "5.23751279239238e-06\n",
            "R2:0.105362  MSE:0.002612  KL:0.718555  MAE:0.007075  RMSE:0.051109  CVRMSE:17.453134  R2:0.106560  MSE:0.002609  KL:0.595414  MAE:0.007086  RMSE:0.051075  CVRMSE:17.441450  R2:0.104935  MSE:0.002613  KL:0.652188  MAE:0.007137  RMSE:0.051122  CVRMSE:17.457301  R2:0.106773  MSE:0.002608  KL:0.686540  MAE:0.007120  RMSE:0.051069  CVRMSE:17.439372  R2:0.105767  MSE:0.002611  KL:0.685095  MAE:0.007069  RMSE:0.051098  CVRMSE:17.449186  train_loss:\n",
            "0.0026109820833730385\n",
            "R2:0.119281  MSE:0.002655  KL:0.052534  MAE:0.005650  RMSE:0.051529  CVRMSE:17.039884  test_loss:\n",
            "5.215760837670901e-06\n",
            "R2:0.106819  MSE:0.002608  KL:0.770706  MAE:0.007096  RMSE:0.051068  CVRMSE:17.438915  R2:0.107885  MSE:0.002605  KL:0.789324  MAE:0.007092  RMSE:0.051037  CVRMSE:17.428508  R2:0.106212  MSE:0.002610  KL:0.812363  MAE:0.007062  RMSE:0.051085  CVRMSE:17.444847  R2:0.106473  MSE:0.002609  KL:0.995018  MAE:0.007100  RMSE:0.051078  CVRMSE:17.442293  R2:0.106433  MSE:0.002609  KL:0.828834  MAE:0.007071  RMSE:0.051079  CVRMSE:17.442689  train_loss:\n",
            "0.002609038000420154\n",
            "R2:0.113113  MSE:0.002674  KL:0.051459  MAE:0.004882  RMSE:0.051710  CVRMSE:17.099443  test_loss:\n",
            "5.252834679536934e-06\n",
            "R2:0.106586  MSE:0.002609  KL:0.640778  MAE:0.007098  RMSE:0.051074  CVRMSE:17.441190  R2:0.106773  MSE:0.002608  KL:0.813856  MAE:0.007074  RMSE:0.051069  CVRMSE:17.439369  R2:0.104583  MSE:0.002614  KL:0.495562  MAE:0.007077  RMSE:0.051132  CVRMSE:17.460733  R2:0.106671  MSE:0.002608  KL:0.781417  MAE:0.007094  RMSE:0.051072  CVRMSE:17.440364  R2:0.102583  MSE:0.002620  KL:0.810220  MAE:0.007131  RMSE:0.051189  CVRMSE:17.480222  train_loss:\n",
            "0.002620278244552233\n",
            "R2:0.112625  MSE:0.002675  KL:0.052249  MAE:0.004804  RMSE:0.051724  CVRMSE:17.104146  test_loss:\n",
            "5.255682163805066e-06\n",
            "R2:0.105618  MSE:0.002611  KL:0.750536  MAE:0.007154  RMSE:0.051102  CVRMSE:17.450636  R2:0.104072  MSE:0.002616  KL:0.768808  MAE:0.007092  RMSE:0.051146  CVRMSE:17.465711  R2:0.104635  MSE:0.002614  KL:1.028555  MAE:0.007144  RMSE:0.051130  CVRMSE:17.460223  R2:0.103892  MSE:0.002616  KL:0.776929  MAE:0.007119  RMSE:0.051151  CVRMSE:17.467466  R2:0.106834  MSE:0.002608  KL:0.781213  MAE:0.007061  RMSE:0.051067  CVRMSE:17.438771  train_loss:\n",
            "0.0026078659593120187\n",
            "R2:0.118768  MSE:0.002657  KL:0.052827  MAE:0.005558  RMSE:0.051544  CVRMSE:17.044841  test_loss:\n",
            "5.218796467770902e-06\n",
            "R2:0.106802  MSE:0.002608  KL:0.690435  MAE:0.007096  RMSE:0.051068  CVRMSE:17.439085  R2:0.105509  MSE:0.002612  KL:0.959791  MAE:0.007087  RMSE:0.051105  CVRMSE:17.451705  R2:0.105848  MSE:0.002611  KL:0.516746  MAE:0.007070  RMSE:0.051095  CVRMSE:17.448396  R2:0.107229  MSE:0.002607  KL:0.707320  MAE:0.007111  RMSE:0.051056  CVRMSE:17.434914  R2:0.103025  MSE:0.002619  KL:0.796131  MAE:0.007122  RMSE:0.051176  CVRMSE:17.475916  train_loss:\n",
            "0.0026189876723304107\n",
            "R2:0.114269  MSE:0.002670  KL:0.055786  MAE:0.005646  RMSE:0.051676  CVRMSE:17.088302  test_loss:\n",
            "5.245751551890449e-06\n",
            "R2:0.107084  MSE:0.002607  KL:0.590699  MAE:0.007108  RMSE:0.051060  CVRMSE:17.436333  R2:0.105090  MSE:0.002613  KL:0.639843  MAE:0.007122  RMSE:0.051117  CVRMSE:17.455790  R2:0.107769  MSE:0.002605  KL:0.787460  MAE:0.007070  RMSE:0.051041  CVRMSE:17.429640  R2:0.104273  MSE:0.002615  KL:0.765287  MAE:0.007080  RMSE:0.051140  CVRMSE:17.463754  R2:0.104379  MSE:0.002615  KL:0.902661  MAE:0.007097  RMSE:0.051137  CVRMSE:17.462724  train_loss:\n",
            "0.0026150349318601962\n",
            "R2:0.119075  MSE:0.002656  KL:0.052154  MAE:0.005308  RMSE:0.051535  CVRMSE:17.041871  test_loss:\n",
            "5.21706240391522e-06\n",
            "R2:0.103850  MSE:0.002617  KL:0.834721  MAE:0.007150  RMSE:0.051153  CVRMSE:17.467879  R2:0.103728  MSE:0.002617  KL:0.900292  MAE:0.007103  RMSE:0.051156  CVRMSE:17.469064  R2:0.104906  MSE:0.002613  KL:0.813308  MAE:0.007109  RMSE:0.051122  CVRMSE:17.457578  R2:0.104500  MSE:0.002615  KL:0.700946  MAE:0.007095  RMSE:0.051134  CVRMSE:17.461542  R2:0.105982  MSE:0.002610  KL:0.659674  MAE:0.007102  RMSE:0.051092  CVRMSE:17.447086  train_loss:\n",
            "0.0026103535138730675\n",
            "R2:0.117844  MSE:0.002660  KL:0.054496  MAE:0.005908  RMSE:0.051571  CVRMSE:17.053774  test_loss:\n",
            "5.224250921917921e-06\n",
            "R2:0.105743  MSE:0.002611  KL:0.718665  MAE:0.007087  RMSE:0.051098  CVRMSE:17.449422  R2:0.104022  MSE:0.002616  KL:0.971589  MAE:0.007094  RMSE:0.051148  CVRMSE:17.466204  R2:0.108878  MSE:0.002602  KL:0.636489  MAE:0.007088  RMSE:0.051009  CVRMSE:17.418807  R2:0.106237  MSE:0.002610  KL:0.811977  MAE:0.007132  RMSE:0.051084  CVRMSE:17.444601  R2:0.103201  MSE:0.002618  KL:0.777198  MAE:0.007120  RMSE:0.051171  CVRMSE:17.474200  train_loss:\n",
            "0.00261847312094762\n",
            "R2:0.114661  MSE:0.002669  KL:0.052123  MAE:0.004926  RMSE:0.051664  CVRMSE:17.084518  test_loss:\n",
            "5.2434622926945135e-06\n",
            "R2:0.107139  MSE:0.002607  KL:0.640434  MAE:0.007100  RMSE:0.051059  CVRMSE:17.435792  R2:0.107379  MSE:0.002606  KL:0.605894  MAE:0.007043  RMSE:0.051052  CVRMSE:17.433455  R2:0.104328  MSE:0.002615  KL:1.332080  MAE:0.007139  RMSE:0.051139  CVRMSE:17.463219  R2:0.103083  MSE:0.002619  KL:1.058676  MAE:0.007113  RMSE:0.051174  CVRMSE:17.475356  R2:0.105362  MSE:0.002612  KL:1.038476  MAE:0.007071  RMSE:0.051109  CVRMSE:17.453140  train_loss:\n",
            "0.002612165447617977\n",
            "R2:0.115227  MSE:0.002668  KL:0.051951  MAE:0.004983  RMSE:0.051648  CVRMSE:17.079050  test_loss:\n",
            "5.240127416229728e-06\n",
            "R2:0.106728  MSE:0.002608  KL:0.736419  MAE:0.007097  RMSE:0.051070  CVRMSE:17.439801  R2:0.106609  MSE:0.002609  KL:0.767063  MAE:0.007045  RMSE:0.051074  CVRMSE:17.440971  R2:0.104477  MSE:0.002615  KL:0.608323  MAE:0.007094  RMSE:0.051135  CVRMSE:17.461764  R2:0.104752  MSE:0.002614  KL:0.825617  MAE:0.007122  RMSE:0.051127  CVRMSE:17.459080  R2:0.105972  MSE:0.002610  KL:0.794394  MAE:0.007095  RMSE:0.051092  CVRMSE:17.447186  train_loss:\n",
            "0.002610383606118731\n",
            "R2:0.121163  MSE:0.002650  KL:0.050901  MAE:0.005107  RMSE:0.051474  CVRMSE:17.021670  test_loss:\n",
            "5.204544115261788e-06\n",
            "R2:0.105827  MSE:0.002611  KL:0.897206  MAE:0.007066  RMSE:0.051096  CVRMSE:17.448599  R2:0.105526  MSE:0.002612  KL:0.886097  MAE:0.007089  RMSE:0.051105  CVRMSE:17.451537  R2:0.104500  MSE:0.002615  KL:0.533525  MAE:0.007136  RMSE:0.051134  CVRMSE:17.461542  R2:0.105801  MSE:0.002611  KL:0.941642  MAE:0.007145  RMSE:0.051097  CVRMSE:17.448851  R2:0.105347  MSE:0.002612  KL:0.685248  MAE:0.007054  RMSE:0.051110  CVRMSE:17.453280  train_loss:\n",
            "0.0026122072149016577\n",
            "R2:0.115979  MSE:0.002665  KL:0.053705  MAE:0.005278  RMSE:0.051626  CVRMSE:17.071800  test_loss:\n",
            "5.235559155553124e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.106700  MSE:0.002608  KL:0.941678  MAE:0.007127  RMSE:0.051071  CVRMSE:17.440075  R2:0.104553  MSE:0.002615  KL:0.673601  MAE:0.007107  RMSE:0.051132  CVRMSE:17.461023  R2:0.104070  MSE:0.002616  KL:0.799537  MAE:0.007143  RMSE:0.051146  CVRMSE:17.465735  R2:0.106765  MSE:0.002608  KL:0.593426  MAE:0.007098  RMSE:0.051069  CVRMSE:17.439450  R2:0.104502  MSE:0.002615  KL:0.912197  MAE:0.007126  RMSE:0.051134  CVRMSE:17.461521  train_loss:\n",
            "0.0026146748076220166\n",
            "R2:0.117667  MSE:0.002660  KL:0.050327  MAE:0.004950  RMSE:0.051577  CVRMSE:17.055493  test_loss:\n",
            "5.225512288413146e-06\n",
            "R2:0.105509  MSE:0.002612  KL:0.621188  MAE:0.007081  RMSE:0.051105  CVRMSE:17.451707  R2:0.106567  MSE:0.002609  KL:0.390337  MAE:0.007078  RMSE:0.051075  CVRMSE:17.441381  R2:0.105545  MSE:0.002612  KL:0.967647  MAE:0.007077  RMSE:0.051104  CVRMSE:17.451352  R2:0.104524  MSE:0.002615  KL:0.906601  MAE:0.007084  RMSE:0.051133  CVRMSE:17.461310  R2:0.103570  MSE:0.002617  KL:0.708960  MAE:0.007092  RMSE:0.051160  CVRMSE:17.470604  train_loss:\n",
            "0.0026173956076542067\n",
            "R2:0.116650  MSE:0.002663  KL:0.053063  MAE:0.005400  RMSE:0.051606  CVRMSE:17.065312  test_loss:\n",
            "5.231519493794428e-06\n",
            "R2:0.104105  MSE:0.002616  KL:0.929395  MAE:0.007098  RMSE:0.051145  CVRMSE:17.465396  R2:0.106431  MSE:0.002609  KL:0.432452  MAE:0.007076  RMSE:0.051079  CVRMSE:17.442709  R2:0.106740  MSE:0.002608  KL:0.924077  MAE:0.007070  RMSE:0.051070  CVRMSE:17.439688  R2:0.105539  MSE:0.002612  KL:0.901540  MAE:0.007151  RMSE:0.051104  CVRMSE:17.451409  R2:0.106632  MSE:0.002608  KL:0.827863  MAE:0.007088  RMSE:0.051073  CVRMSE:17.440742  train_loss:\n",
            "0.002608455645030464\n",
            "R2:0.119113  MSE:0.002656  KL:0.052374  MAE:0.005615  RMSE:0.051534  CVRMSE:17.041509  test_loss:\n",
            "5.216780483503876e-06\n",
            "R2:0.105438  MSE:0.002612  KL:0.837639  MAE:0.007108  RMSE:0.051107  CVRMSE:17.452394  R2:0.104841  MSE:0.002614  KL:0.736753  MAE:0.007099  RMSE:0.051124  CVRMSE:17.458218  R2:0.104119  MSE:0.002616  KL:0.725991  MAE:0.007128  RMSE:0.051145  CVRMSE:17.465252  R2:0.109209  MSE:0.002601  KL:0.640766  MAE:0.007078  RMSE:0.050999  CVRMSE:17.415569  R2:0.106061  MSE:0.002610  KL:0.709165  MAE:0.007071  RMSE:0.051089  CVRMSE:17.446320  train_loss:\n",
            "0.0026101242585088264\n",
            "R2:0.113494  MSE:0.002673  KL:0.054110  MAE:0.005088  RMSE:0.051698  CVRMSE:17.095774  test_loss:\n",
            "5.25048369445484e-06\n",
            "R2:0.104751  MSE:0.002614  KL:1.084387  MAE:0.007092  RMSE:0.051127  CVRMSE:17.459091  R2:0.104833  MSE:0.002614  KL:0.556155  MAE:0.007106  RMSE:0.051124  CVRMSE:17.458299  R2:0.108382  MSE:0.002603  KL:0.893125  MAE:0.007114  RMSE:0.051023  CVRMSE:17.423651  R2:0.104764  MSE:0.002614  KL:0.759008  MAE:0.007124  RMSE:0.051126  CVRMSE:17.458968  R2:0.105152  MSE:0.002613  KL:0.676584  MAE:0.007097  RMSE:0.051115  CVRMSE:17.455183  train_loss:\n",
            "0.0026127769401678414\n",
            "R2:0.118108  MSE:0.002659  KL:0.054361  MAE:0.005136  RMSE:0.051564  CVRMSE:17.051230  test_loss:\n",
            "5.222804461136954e-06\n",
            "R2:0.106013  MSE:0.002610  KL:0.785306  MAE:0.007105  RMSE:0.051091  CVRMSE:17.446783  R2:0.104925  MSE:0.002613  KL:0.724868  MAE:0.007073  RMSE:0.051122  CVRMSE:17.457400  R2:0.105762  MSE:0.002611  KL:0.654242  MAE:0.007116  RMSE:0.051098  CVRMSE:17.449237  R2:0.106124  MSE:0.002610  KL:0.595577  MAE:0.007070  RMSE:0.051088  CVRMSE:17.445701  R2:0.105858  MSE:0.002611  KL:0.804268  MAE:0.007101  RMSE:0.051095  CVRMSE:17.448300  train_loss:\n",
            "0.002610716816031684\n",
            "R2:0.120021  MSE:0.002653  KL:0.051856  MAE:0.005401  RMSE:0.051508  CVRMSE:17.032721  test_loss:\n",
            "5.211384127901307e-06\n",
            "R2:0.105974  MSE:0.002610  KL:0.665320  MAE:0.007106  RMSE:0.051092  CVRMSE:17.447169  R2:0.104926  MSE:0.002613  KL:0.863084  MAE:0.007098  RMSE:0.051122  CVRMSE:17.457385  R2:0.105847  MSE:0.002611  KL:0.840400  MAE:0.007103  RMSE:0.051095  CVRMSE:17.448401  R2:0.105688  MSE:0.002611  KL:0.877378  MAE:0.007099  RMSE:0.051100  CVRMSE:17.449959  R2:0.105572  MSE:0.002612  KL:0.660101  MAE:0.007071  RMSE:0.051103  CVRMSE:17.451089  train_loss:\n",
            "0.002611551543597201\n",
            "R2:0.119598  MSE:0.002654  KL:0.053115  MAE:0.005696  RMSE:0.051520  CVRMSE:17.036816  test_loss:\n",
            "5.213822776305753e-06\n",
            "R2:0.107646  MSE:0.002605  KL:0.642765  MAE:0.007085  RMSE:0.051044  CVRMSE:17.430842  R2:0.106034  MSE:0.002610  KL:0.601039  MAE:0.007072  RMSE:0.051090  CVRMSE:17.446580  R2:0.107118  MSE:0.002607  KL:0.762699  MAE:0.007074  RMSE:0.051059  CVRMSE:17.435995  R2:0.104119  MSE:0.002616  KL:0.859554  MAE:0.007108  RMSE:0.051145  CVRMSE:17.465259  R2:0.104986  MSE:0.002613  KL:0.856344  MAE:0.007102  RMSE:0.051120  CVRMSE:17.456800  train_loss:\n",
            "0.002613261145768715\n",
            "R2:0.121762  MSE:0.002648  KL:0.049703  MAE:0.005810  RMSE:0.051457  CVRMSE:17.015865  test_loss:\n",
            "5.200926784494392e-06\n",
            "R2:0.105052  MSE:0.002613  KL:0.923544  MAE:0.007090  RMSE:0.051118  CVRMSE:17.456158  R2:0.105233  MSE:0.002613  KL:0.833742  MAE:0.007076  RMSE:0.051113  CVRMSE:17.454390  R2:0.105108  MSE:0.002613  KL:0.814740  MAE:0.007083  RMSE:0.051117  CVRMSE:17.455614  R2:0.105551  MSE:0.002612  KL:0.611782  MAE:0.007090  RMSE:0.051104  CVRMSE:17.451287  R2:0.104275  MSE:0.002615  KL:0.865466  MAE:0.007069  RMSE:0.051140  CVRMSE:17.463740  train_loss:\n",
            "0.0026153392175311077\n",
            "R2:0.115307  MSE:0.002667  KL:0.055042  MAE:0.005416  RMSE:0.051646  CVRMSE:17.078285  test_loss:\n",
            "5.2395079809396985e-06\n",
            "R2:0.105756  MSE:0.002611  KL:0.628890  MAE:0.007140  RMSE:0.051098  CVRMSE:17.449291  R2:0.104665  MSE:0.002614  KL:0.715788  MAE:0.007075  RMSE:0.051129  CVRMSE:17.459932  R2:0.106975  MSE:0.002607  KL:0.480267  MAE:0.007115  RMSE:0.051063  CVRMSE:17.437393  R2:0.105568  MSE:0.002612  KL:0.696724  MAE:0.007097  RMSE:0.051103  CVRMSE:17.451127  R2:0.104139  MSE:0.002616  KL:0.889147  MAE:0.007089  RMSE:0.051144  CVRMSE:17.465064  train_loss:\n",
            "0.0026157360407591344\n",
            "R2:0.113078  MSE:0.002674  KL:0.054089  MAE:0.005010  RMSE:0.051711  CVRMSE:17.099788  test_loss:\n",
            "5.252946052561738e-06\n",
            "R2:0.106001  MSE:0.002610  KL:0.715640  MAE:0.007084  RMSE:0.051091  CVRMSE:17.446902  R2:0.106818  MSE:0.002608  KL:0.709937  MAE:0.007053  RMSE:0.051068  CVRMSE:17.438931  R2:0.104344  MSE:0.002615  KL:0.553622  MAE:0.007110  RMSE:0.051138  CVRMSE:17.463066  R2:0.104508  MSE:0.002615  KL:0.574958  MAE:0.007073  RMSE:0.051134  CVRMSE:17.461462  R2:0.103881  MSE:0.002616  KL:0.691281  MAE:0.007136  RMSE:0.051152  CVRMSE:17.467572  train_loss:\n",
            "0.0026164872479169654\n",
            "R2:0.112256  MSE:0.002676  KL:0.052721  MAE:0.004924  RMSE:0.051735  CVRMSE:17.107707  test_loss:\n",
            "5.2579139136802505e-06\n",
            "R2:0.105003  MSE:0.002613  KL:0.613038  MAE:0.007113  RMSE:0.051120  CVRMSE:17.456639  R2:0.106395  MSE:0.002609  KL:1.016998  MAE:0.007049  RMSE:0.051080  CVRMSE:17.443056  R2:0.105981  MSE:0.002610  KL:0.746193  MAE:0.007083  RMSE:0.051092  CVRMSE:17.447098  R2:0.106188  MSE:0.002610  KL:0.725650  MAE:0.007062  RMSE:0.051086  CVRMSE:17.445081  R2:0.105769  MSE:0.002611  KL:0.794307  MAE:0.007098  RMSE:0.051098  CVRMSE:17.449169  train_loss:\n",
            "0.0026109769500161093\n",
            "R2:0.118246  MSE:0.002658  KL:0.050172  MAE:0.005151  RMSE:0.051560  CVRMSE:17.049891  test_loss:\n",
            "5.2221089507652155e-06\n",
            "R2:0.104514  MSE:0.002615  KL:0.812665  MAE:0.007099  RMSE:0.051134  CVRMSE:17.461408  R2:0.106765  MSE:0.002608  KL:0.657329  MAE:0.007073  RMSE:0.051069  CVRMSE:17.439448  R2:0.106367  MSE:0.002609  KL:0.814407  MAE:0.007074  RMSE:0.051081  CVRMSE:17.443331  R2:0.105825  MSE:0.002611  KL:0.994371  MAE:0.007101  RMSE:0.051096  CVRMSE:17.448621  R2:0.104604  MSE:0.002614  KL:0.783235  MAE:0.007126  RMSE:0.051131  CVRMSE:17.460524  train_loss:\n",
            "0.002614376325093019\n",
            "R2:0.118698  MSE:0.002657  KL:0.055090  MAE:0.006041  RMSE:0.051546  CVRMSE:17.045518  test_loss:\n",
            "5.219103478619132e-06\n",
            "R2:0.105377  MSE:0.002612  KL:0.713948  MAE:0.007098  RMSE:0.051109  CVRMSE:17.452985  R2:0.105417  MSE:0.002612  KL:0.701718  MAE:0.007103  RMSE:0.051108  CVRMSE:17.452598  R2:0.108595  MSE:0.002603  KL:0.543965  MAE:0.007077  RMSE:0.051017  CVRMSE:17.421576  R2:0.105906  MSE:0.002611  KL:0.668727  MAE:0.007081  RMSE:0.051094  CVRMSE:17.447833  R2:0.108061  MSE:0.002604  KL:0.845509  MAE:0.007083  RMSE:0.051032  CVRMSE:17.426791  train_loss:\n",
            "0.0026042842671639287\n",
            "R2:0.119703  MSE:0.002654  KL:0.051511  MAE:0.005181  RMSE:0.051517  CVRMSE:17.035804  test_loss:\n",
            "5.21335009754558e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.106760  MSE:0.002608  KL:0.844041  MAE:0.007090  RMSE:0.051069  CVRMSE:17.439496  R2:0.108313  MSE:0.002604  KL:0.735323  MAE:0.007063  RMSE:0.051025  CVRMSE:17.424323  R2:0.106120  MSE:0.002610  KL:0.940473  MAE:0.007080  RMSE:0.051088  CVRMSE:17.445740  R2:0.105208  MSE:0.002613  KL:0.896401  MAE:0.007111  RMSE:0.051114  CVRMSE:17.454640  R2:0.105901  MSE:0.002611  KL:0.755718  MAE:0.007087  RMSE:0.051094  CVRMSE:17.447878  train_loss:\n",
            "0.0026105906238546206\n",
            "R2:0.116691  MSE:0.002663  KL:0.051494  MAE:0.005764  RMSE:0.051605  CVRMSE:17.064921  test_loss:\n",
            "5.231341604266051e-06\n",
            "R2:0.108670  MSE:0.002603  KL:0.729876  MAE:0.007126  RMSE:0.051015  CVRMSE:17.420837  R2:0.105690  MSE:0.002611  KL:0.774853  MAE:0.007110  RMSE:0.051100  CVRMSE:17.449934  R2:0.104155  MSE:0.002616  KL:0.980675  MAE:0.007096  RMSE:0.051144  CVRMSE:17.464910  R2:0.103114  MSE:0.002619  KL:0.500525  MAE:0.007078  RMSE:0.051173  CVRMSE:17.475045  R2:0.105090  MSE:0.002613  KL:0.909982  MAE:0.007134  RMSE:0.051117  CVRMSE:17.455788  train_loss:\n",
            "0.0026129582207887474\n",
            "R2:0.108162  MSE:0.002689  KL:0.049125  MAE:0.004706  RMSE:0.051854  CVRMSE:17.147112  test_loss:\n",
            "5.282423027158225e-06\n",
            "R2:0.106083  MSE:0.002610  KL:0.826904  MAE:0.007104  RMSE:0.051089  CVRMSE:17.446098  R2:0.105047  MSE:0.002613  KL:0.848227  MAE:0.007107  RMSE:0.051118  CVRMSE:17.456210  R2:0.106260  MSE:0.002610  KL:0.810716  MAE:0.007127  RMSE:0.051084  CVRMSE:17.444373  R2:0.104482  MSE:0.002615  KL:0.738155  MAE:0.007068  RMSE:0.051134  CVRMSE:17.461716  R2:0.104181  MSE:0.002616  KL:0.909810  MAE:0.007103  RMSE:0.051143  CVRMSE:17.464648  train_loss:\n",
            "0.002615611236372841\n",
            "R2:0.114897  MSE:0.002669  KL:0.055498  MAE:0.005409  RMSE:0.051658  CVRMSE:17.082237  test_loss:\n",
            "5.2419488696147e-06\n",
            "R2:0.105722  MSE:0.002611  KL:0.647650  MAE:0.007067  RMSE:0.051099  CVRMSE:17.449623  R2:0.106012  MSE:0.002610  KL:0.709250  MAE:0.007123  RMSE:0.051091  CVRMSE:17.446796  R2:0.106487  MSE:0.002609  KL:1.144977  MAE:0.007083  RMSE:0.051077  CVRMSE:17.442160  R2:0.103824  MSE:0.002617  KL:0.678286  MAE:0.007091  RMSE:0.051153  CVRMSE:17.468134  R2:0.104527  MSE:0.002615  KL:0.654252  MAE:0.007147  RMSE:0.051133  CVRMSE:17.461276  train_loss:\n",
            "0.0026146013922627724\n",
            "R2:0.120064  MSE:0.002653  KL:0.053202  MAE:0.005542  RMSE:0.051507  CVRMSE:17.032301  test_loss:\n",
            "5.211030769256163e-06\n",
            "R2:0.104668  MSE:0.002614  KL:0.862392  MAE:0.007108  RMSE:0.051129  CVRMSE:17.459908  R2:0.105983  MSE:0.002610  KL:0.841102  MAE:0.007099  RMSE:0.051092  CVRMSE:17.447077  R2:0.106413  MSE:0.002609  KL:0.682366  MAE:0.007065  RMSE:0.051079  CVRMSE:17.442882  R2:0.106048  MSE:0.002610  KL:0.827382  MAE:0.007084  RMSE:0.051090  CVRMSE:17.446448  R2:0.106277  MSE:0.002609  KL:0.767331  MAE:0.007081  RMSE:0.051083  CVRMSE:17.444206  train_loss:\n",
            "0.0026094919240818733\n",
            "R2:0.117833  MSE:0.002660  KL:0.051602  MAE:0.005033  RMSE:0.051572  CVRMSE:17.053882  test_loss:\n",
            "5.224522719311546e-06\n",
            "R2:0.106083  MSE:0.002610  KL:0.854250  MAE:0.007082  RMSE:0.051089  CVRMSE:17.446103  R2:0.104552  MSE:0.002615  KL:0.780400  MAE:0.007095  RMSE:0.051132  CVRMSE:17.461033  R2:0.105345  MSE:0.002612  KL:1.206114  MAE:0.007095  RMSE:0.051110  CVRMSE:17.453305  R2:0.105612  MSE:0.002611  KL:0.902494  MAE:0.007082  RMSE:0.051102  CVRMSE:17.450701  R2:0.105043  MSE:0.002613  KL:0.676089  MAE:0.007108  RMSE:0.051118  CVRMSE:17.456244  train_loss:\n",
            "0.0026130945151876844\n",
            "R2:0.117302  MSE:0.002661  KL:0.053079  MAE:0.005709  RMSE:0.051587  CVRMSE:17.059018  test_loss:\n",
            "5.227626271287287e-06\n",
            "R2:0.105743  MSE:0.002611  KL:0.929936  MAE:0.007153  RMSE:0.051098  CVRMSE:17.449415  R2:0.105756  MSE:0.002611  KL:0.638593  MAE:0.007086  RMSE:0.051098  CVRMSE:17.449294  R2:0.107471  MSE:0.002606  KL:0.753529  MAE:0.007110  RMSE:0.051049  CVRMSE:17.432549  R2:0.106082  MSE:0.002610  KL:0.621165  MAE:0.007074  RMSE:0.051089  CVRMSE:17.446116  R2:0.106279  MSE:0.002609  KL:0.649274  MAE:0.007069  RMSE:0.051083  CVRMSE:17.444185  train_loss:\n",
            "0.002609485540512882\n",
            "R2:0.120214  MSE:0.002652  KL:0.051264  MAE:0.005884  RMSE:0.051502  CVRMSE:17.030854  test_loss:\n",
            "5.210152138951531e-06\n",
            "R2:0.105157  MSE:0.002613  KL:0.768120  MAE:0.007117  RMSE:0.051115  CVRMSE:17.455137  R2:0.105134  MSE:0.002613  KL:0.657383  MAE:0.007091  RMSE:0.051116  CVRMSE:17.455357  R2:0.105863  MSE:0.002611  KL:0.720164  MAE:0.007111  RMSE:0.051095  CVRMSE:17.448253  R2:0.108064  MSE:0.002604  KL:1.079117  MAE:0.007068  RMSE:0.051032  CVRMSE:17.426758  R2:0.104631  MSE:0.002614  KL:0.825851  MAE:0.007084  RMSE:0.051130  CVRMSE:17.460268  train_loss:\n",
            "0.0026142996373823267\n",
            "R2:0.113907  MSE:0.002671  KL:0.055985  MAE:0.005478  RMSE:0.051686  CVRMSE:17.091790  test_loss:\n",
            "5.2479043641365946e-06\n",
            "R2:0.107951  MSE:0.002605  KL:0.899498  MAE:0.007135  RMSE:0.051035  CVRMSE:17.427863  R2:0.108504  MSE:0.002603  KL:0.804758  MAE:0.007050  RMSE:0.051020  CVRMSE:17.422460  R2:0.107282  MSE:0.002607  KL:0.843088  MAE:0.007089  RMSE:0.051054  CVRMSE:17.434398  R2:0.106029  MSE:0.002610  KL:0.627737  MAE:0.007099  RMSE:0.051090  CVRMSE:17.446630  R2:0.105974  MSE:0.002610  KL:1.100332  MAE:0.007110  RMSE:0.051092  CVRMSE:17.447163  train_loss:\n",
            "0.0026103766788988566\n",
            "R2:0.115981  MSE:0.002665  KL:0.050822  MAE:0.004874  RMSE:0.051626  CVRMSE:17.071776  test_loss:\n",
            "5.235649744116629e-06\n",
            "R2:0.105662  MSE:0.002611  KL:0.900299  MAE:0.007083  RMSE:0.051101  CVRMSE:17.450208  R2:0.105373  MSE:0.002612  KL:0.645381  MAE:0.007082  RMSE:0.051109  CVRMSE:17.453024  R2:0.105768  MSE:0.002611  KL:1.218224  MAE:0.007116  RMSE:0.051098  CVRMSE:17.449172  R2:0.105032  MSE:0.002613  KL:0.984068  MAE:0.007104  RMSE:0.051119  CVRMSE:17.456359  R2:0.105159  MSE:0.002613  KL:0.861800  MAE:0.007072  RMSE:0.051115  CVRMSE:17.455117  train_loss:\n",
            "0.0026127572190311416\n",
            "R2:0.116527  MSE:0.002664  KL:0.053195  MAE:0.005537  RMSE:0.051610  CVRMSE:17.066504  test_loss:\n",
            "5.23226614146474e-06\n",
            "R2:0.104755  MSE:0.002614  KL:0.968531  MAE:0.007056  RMSE:0.051127  CVRMSE:17.459059  R2:0.106133  MSE:0.002610  KL:0.696489  MAE:0.007123  RMSE:0.051087  CVRMSE:17.445614  R2:0.104009  MSE:0.002616  KL:0.512304  MAE:0.007093  RMSE:0.051148  CVRMSE:17.466329  R2:0.104581  MSE:0.002614  KL:0.732715  MAE:0.007115  RMSE:0.051132  CVRMSE:17.460748  R2:0.106472  MSE:0.002609  KL:0.704456  MAE:0.007061  RMSE:0.051078  CVRMSE:17.442300  train_loss:\n",
            "0.002608921749031229\n",
            "R2:0.117588  MSE:0.002660  KL:0.054881  MAE:0.005635  RMSE:0.051579  CVRMSE:17.056252  test_loss:\n",
            "5.225860908696029e-06\n",
            "R2:0.105482  MSE:0.002612  KL:0.903503  MAE:0.007130  RMSE:0.051106  CVRMSE:17.451963  R2:0.105900  MSE:0.002611  KL:1.043083  MAE:0.007130  RMSE:0.051094  CVRMSE:17.447884  R2:0.106118  MSE:0.002610  KL:0.762581  MAE:0.007110  RMSE:0.051088  CVRMSE:17.445756  R2:0.105892  MSE:0.002611  KL:0.579545  MAE:0.007072  RMSE:0.051094  CVRMSE:17.447969  R2:0.104737  MSE:0.002614  KL:0.692625  MAE:0.007146  RMSE:0.051127  CVRMSE:17.459231  train_loss:\n",
            "0.002613989025992776\n",
            "R2:0.117064  MSE:0.002662  KL:0.052323  MAE:0.005151  RMSE:0.051594  CVRMSE:17.061317  test_loss:\n",
            "5.229080147713418e-06\n",
            "R2:0.106037  MSE:0.002610  KL:0.568501  MAE:0.007078  RMSE:0.051090  CVRMSE:17.446547  R2:0.104000  MSE:0.002616  KL:0.836853  MAE:0.007103  RMSE:0.051148  CVRMSE:17.466421  R2:0.105134  MSE:0.002613  KL:0.857765  MAE:0.007090  RMSE:0.051116  CVRMSE:17.455356  R2:0.104172  MSE:0.002616  KL:0.721251  MAE:0.007130  RMSE:0.051143  CVRMSE:17.464739  R2:0.104903  MSE:0.002614  KL:1.061089  MAE:0.007125  RMSE:0.051122  CVRMSE:17.457610  train_loss:\n",
            "0.002613503637245186\n",
            "R2:0.115727  MSE:0.002666  KL:0.051676  MAE:0.005008  RMSE:0.051633  CVRMSE:17.074226  test_loss:\n",
            "5.237182131952691e-06\n",
            "R2:0.105466  MSE:0.002612  KL:0.890142  MAE:0.007080  RMSE:0.051106  CVRMSE:17.452125  R2:0.107155  MSE:0.002607  KL:0.871578  MAE:0.007079  RMSE:0.051058  CVRMSE:17.435637  R2:0.108187  MSE:0.002604  KL:0.610455  MAE:0.007112  RMSE:0.051029  CVRMSE:17.425553  R2:0.105375  MSE:0.002612  KL:0.775169  MAE:0.007071  RMSE:0.051109  CVRMSE:17.453011  R2:0.105555  MSE:0.002612  KL:0.554810  MAE:0.007072  RMSE:0.051104  CVRMSE:17.451251  train_loss:\n",
            "0.002611599927198582\n",
            "R2:0.117732  MSE:0.002660  KL:0.055014  MAE:0.005535  RMSE:0.051575  CVRMSE:17.054858  test_loss:\n",
            "5.22499150413244e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.106422  MSE:0.002609  KL:0.677625  MAE:0.007095  RMSE:0.051079  CVRMSE:17.442792  R2:0.105744  MSE:0.002611  KL:0.833148  MAE:0.007130  RMSE:0.051098  CVRMSE:17.449412  R2:0.105288  MSE:0.002612  KL:0.988879  MAE:0.007093  RMSE:0.051111  CVRMSE:17.453857  R2:0.106289  MSE:0.002609  KL:0.710363  MAE:0.007089  RMSE:0.051083  CVRMSE:17.444095  R2:0.106667  MSE:0.002608  KL:0.481049  MAE:0.007078  RMSE:0.051072  CVRMSE:17.440399  train_loss:\n",
            "0.0026083529058936036\n",
            "R2:0.120434  MSE:0.002652  KL:0.052421  MAE:0.005369  RMSE:0.051496  CVRMSE:17.028723  test_loss:\n",
            "5.208840720060468e-06\n",
            "R2:0.107776  MSE:0.002605  KL:0.846898  MAE:0.007073  RMSE:0.051040  CVRMSE:17.429575  R2:0.105040  MSE:0.002613  KL:0.738669  MAE:0.007102  RMSE:0.051119  CVRMSE:17.456279  R2:0.104029  MSE:0.002616  KL:0.626575  MAE:0.007111  RMSE:0.051147  CVRMSE:17.466138  R2:0.105580  MSE:0.002612  KL:0.716343  MAE:0.007059  RMSE:0.051103  CVRMSE:17.451010  R2:0.104851  MSE:0.002614  KL:0.831341  MAE:0.007111  RMSE:0.051124  CVRMSE:17.458123  train_loss:\n",
            "0.0026136570788351052\n",
            "R2:0.116075  MSE:0.002665  KL:0.052821  MAE:0.005455  RMSE:0.051623  CVRMSE:17.070870  test_loss:\n",
            "5.235043537551819e-06\n",
            "R2:0.105288  MSE:0.002612  KL:0.567131  MAE:0.007064  RMSE:0.051111  CVRMSE:17.453854  R2:0.103984  MSE:0.002616  KL:0.585405  MAE:0.007093  RMSE:0.051149  CVRMSE:17.466573  R2:0.105032  MSE:0.002613  KL:0.835287  MAE:0.007074  RMSE:0.051119  CVRMSE:17.456351  R2:0.106394  MSE:0.002609  KL:1.030479  MAE:0.007067  RMSE:0.051080  CVRMSE:17.443064  R2:0.106346  MSE:0.002609  KL:0.737450  MAE:0.007097  RMSE:0.051081  CVRMSE:17.443535  train_loss:\n",
            "0.0026092911743253655\n",
            "R2:0.116796  MSE:0.002663  KL:0.052793  MAE:0.005164  RMSE:0.051602  CVRMSE:17.063906  test_loss:\n",
            "5.230658640365136e-06\n",
            "R2:0.107115  MSE:0.002607  KL:0.584918  MAE:0.007085  RMSE:0.051059  CVRMSE:17.436025  R2:0.104918  MSE:0.002613  KL:0.481487  MAE:0.007136  RMSE:0.051122  CVRMSE:17.457464  R2:0.105820  MSE:0.002611  KL:0.629915  MAE:0.007076  RMSE:0.051096  CVRMSE:17.448672  R2:0.106101  MSE:0.002610  KL:1.077837  MAE:0.007070  RMSE:0.051088  CVRMSE:17.445930  R2:0.106821  MSE:0.002608  KL:0.854629  MAE:0.007077  RMSE:0.051068  CVRMSE:17.438895  train_loss:\n",
            "0.0026079031751948065\n",
            "R2:0.121690  MSE:0.002648  KL:0.050196  MAE:0.005808  RMSE:0.051459  CVRMSE:17.016562  test_loss:\n",
            "5.201354283443607e-06\n",
            "R2:0.106416  MSE:0.002609  KL:0.888785  MAE:0.007085  RMSE:0.051079  CVRMSE:17.442853  R2:0.107331  MSE:0.002606  KL:0.952565  MAE:0.007071  RMSE:0.051053  CVRMSE:17.433920  R2:0.106421  MSE:0.002609  KL:1.079488  MAE:0.007093  RMSE:0.051079  CVRMSE:17.442798  R2:0.105369  MSE:0.002612  KL:0.930166  MAE:0.007079  RMSE:0.051109  CVRMSE:17.453065  R2:0.104277  MSE:0.002615  KL:0.738229  MAE:0.007128  RMSE:0.051140  CVRMSE:17.463713  train_loss:\n",
            "0.002615331174998792\n",
            "R2:0.117007  MSE:0.002662  KL:0.055507  MAE:0.005499  RMSE:0.051596  CVRMSE:17.061863  test_loss:\n",
            "5.2292896363794455e-06\n",
            "R2:0.105023  MSE:0.002613  KL:0.920485  MAE:0.007120  RMSE:0.051119  CVRMSE:17.456442  R2:0.106131  MSE:0.002610  KL:0.649863  MAE:0.007098  RMSE:0.051087  CVRMSE:17.445636  R2:0.106899  MSE:0.002608  KL:0.684450  MAE:0.007063  RMSE:0.051065  CVRMSE:17.438142  R2:0.105432  MSE:0.002612  KL:0.659997  MAE:0.007062  RMSE:0.051107  CVRMSE:17.452451  R2:0.105830  MSE:0.002611  KL:0.970854  MAE:0.007115  RMSE:0.051096  CVRMSE:17.448575  train_loss:\n",
            "0.0026107989966588406\n",
            "R2:0.115294  MSE:0.002667  KL:0.052496  MAE:0.005162  RMSE:0.051646  CVRMSE:17.078407  test_loss:\n",
            "5.239700214806993e-06\n",
            "R2:0.105840  MSE:0.002611  KL:0.686551  MAE:0.007083  RMSE:0.051096  CVRMSE:17.448469  R2:0.104698  MSE:0.002614  KL:0.862478  MAE:0.007075  RMSE:0.051128  CVRMSE:17.459613  R2:0.106249  MSE:0.002610  KL:0.880898  MAE:0.007073  RMSE:0.051084  CVRMSE:17.444486  R2:0.104241  MSE:0.002615  KL:0.698825  MAE:0.007129  RMSE:0.051141  CVRMSE:17.464065  R2:0.106053  MSE:0.002610  KL:0.793164  MAE:0.007071  RMSE:0.051090  CVRMSE:17.446395  train_loss:\n",
            "0.0026101468474323828\n",
            "R2:0.117208  MSE:0.002662  KL:0.052673  MAE:0.005165  RMSE:0.051590  CVRMSE:17.059926  test_loss:\n",
            "5.228213093694893e-06\n",
            "R2:0.106323  MSE:0.002609  KL:0.566888  MAE:0.007076  RMSE:0.051082  CVRMSE:17.443760  R2:0.105397  MSE:0.002612  KL:0.891303  MAE:0.007127  RMSE:0.051108  CVRMSE:17.452792  R2:0.104289  MSE:0.002615  KL:0.718595  MAE:0.007128  RMSE:0.051140  CVRMSE:17.463599  R2:0.104095  MSE:0.002616  KL:1.084929  MAE:0.007146  RMSE:0.051146  CVRMSE:17.465486  R2:0.105552  MSE:0.002612  KL:0.909007  MAE:0.007078  RMSE:0.051104  CVRMSE:17.451284  train_loss:\n",
            "0.0026116097996016284\n",
            "R2:0.119667  MSE:0.002654  KL:0.055715  MAE:0.006129  RMSE:0.051518  CVRMSE:17.036146  test_loss:\n",
            "5.213235323615899e-06\n",
            "R2:0.105592  MSE:0.002611  KL:0.853112  MAE:0.007102  RMSE:0.051103  CVRMSE:17.450891  R2:0.105238  MSE:0.002613  KL:0.928224  MAE:0.007128  RMSE:0.051113  CVRMSE:17.454345  R2:0.106307  MSE:0.002609  KL:0.518603  MAE:0.007085  RMSE:0.051082  CVRMSE:17.443913  R2:0.105795  MSE:0.002611  KL:0.838668  MAE:0.007061  RMSE:0.051097  CVRMSE:17.448912  R2:0.105998  MSE:0.002610  KL:0.875186  MAE:0.007098  RMSE:0.051091  CVRMSE:17.446930  train_loss:\n",
            "0.002610306934493388\n",
            "R2:0.118083  MSE:0.002659  KL:0.054869  MAE:0.006107  RMSE:0.051564  CVRMSE:17.051468  test_loss:\n",
            "5.222847708665276e-06\n",
            "R2:0.105363  MSE:0.002612  KL:1.028360  MAE:0.007068  RMSE:0.051109  CVRMSE:17.453124  R2:0.104842  MSE:0.002614  KL:1.027857  MAE:0.007075  RMSE:0.051124  CVRMSE:17.458209  R2:0.106252  MSE:0.002610  KL:0.614287  MAE:0.007092  RMSE:0.051084  CVRMSE:17.444452  R2:0.105582  MSE:0.002612  KL:0.642053  MAE:0.007101  RMSE:0.051103  CVRMSE:17.450986  R2:0.105887  MSE:0.002611  KL:0.747171  MAE:0.007075  RMSE:0.051094  CVRMSE:17.448010  train_loss:\n",
            "0.0026106300909540863\n",
            "R2:0.112657  MSE:0.002675  KL:0.052132  MAE:0.005185  RMSE:0.051723  CVRMSE:17.103842  test_loss:\n",
            "5.2555134962866145e-06\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/0s/r_ncjf_j67z8qchrjzz_8j640000gn/T/ipykernel_1307/881944267.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMydata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_loss:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/0s/r_ncjf_j67z8qchrjzz_8j640000gn/T/ipykernel_1307/1706515214.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mloss_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0melem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             return Batch.from_data_list(batch, self.follow_batch,\n\u001b[0m\u001b[1;32m     20\u001b[0m                                         self.exclude_keys)\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch_geometric/data/batch.py\u001b[0m in \u001b[0;36mfrom_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     66\u001b[0m         Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         batch, slice_dict, inc_dict = collate(\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mdata_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mrepeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_nodes\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mout_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepeat_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mout_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36mcumsum\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(Mydata_train, batch_size=batch_size)\n",
        "for epoch in range(num_epochs):\n",
        "    loss,r2,mse,kl=train()\n",
        "    if epoch %5==0:      \n",
        "        print('train_loss:')\n",
        "        print(loss)\n",
        "\n",
        "        loss,r2,mse,kl=val()\n",
        "        print('test_loss:')\n",
        "        print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezCCwtkgFkN_",
        "outputId": "3d36ffca-1759-428f-e452-5c169b248ce9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>auuc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y_hat</th>\n",
              "      <td>0.732616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random</th>\n",
              "      <td>0.499185</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            auuc\n",
              "y_hat   0.732616\n",
              "Random  0.499185"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# estimate area under the uplift curve (AUUC)\n",
        "from causalml.metrics import *\n",
        "uplift=df_test.copy()\n",
        "tau_hat=pd.concat([t0,t1], axis=0, join=\"inner\")\n",
        "uplift=pd.concat([uplift,tau_hat], axis=1, join=\"inner\")\n",
        "uplift = uplift.loc[:,~uplift.columns.duplicated()]\n",
        "\n",
        "auuc=auuc_score(uplift, outcome_col='y', treatment_col='T')\n",
        "gcn_auuc=pd.DataFrame(auuc[[\"y_hat\",\"Random\"]],columns=['auuc'])\n",
        "gcn_auuc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGF8AUtZFkN_"
      },
      "source": [
        "# GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1jwzm-7FkN_",
        "outputId": "cbcfc005-886e-4d9c-c50c-2dd5ccb5394c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:-0.813384  MSE:0.005295  KL:2.094833  MAE:0.017525  RMSE:0.072765  CVRMSE:24.848187  train_loss:\n",
            "0.005294719280111404\n",
            "R2:0.072054  MSE:0.002798  KL:0.126590  MAE:0.005984  RMSE:0.052893  CVRMSE:17.490782  test_loss:\n",
            "5.497692996261892e-06\n",
            "R2:0.060488  MSE:0.002743  KL:2.104842  MAE:0.009919  RMSE:0.052375  CVRMSE:17.885496  R2:0.057913  MSE:0.002751  KL:1.238602  MAE:0.008663  RMSE:0.052447  CVRMSE:17.909985  R2:0.060253  MSE:0.002744  KL:1.248182  MAE:0.008433  RMSE:0.052382  CVRMSE:17.887734  R2:0.064215  MSE:0.002732  KL:1.533469  MAE:0.008456  RMSE:0.052271  CVRMSE:17.849982  R2:0.064579  MSE:0.002731  KL:1.586445  MAE:0.008461  RMSE:0.052261  CVRMSE:17.846508  train_loss:\n",
            "0.0027312411740449236\n",
            "R2:0.056647  MSE:0.002844  KL:0.100752  MAE:0.005517  RMSE:0.053330  CVRMSE:17.635387  test_loss:\n",
            "5.589489941400884e-06\n",
            "R2:0.064812  MSE:0.002731  KL:1.744580  MAE:0.008459  RMSE:0.052255  CVRMSE:17.844289  R2:0.065108  MSE:0.002730  KL:1.738378  MAE:0.008456  RMSE:0.052247  CVRMSE:17.841467  R2:0.064888  MSE:0.002730  KL:1.717766  MAE:0.008453  RMSE:0.052253  CVRMSE:17.843561  R2:0.064689  MSE:0.002731  KL:1.686471  MAE:0.008447  RMSE:0.052258  CVRMSE:17.845459  R2:0.064419  MSE:0.002732  KL:1.771654  MAE:0.008445  RMSE:0.052266  CVRMSE:17.848038  train_loss:\n",
            "0.0027317093230096023\n",
            "R2:0.057155  MSE:0.002843  KL:0.105881  MAE:0.005612  RMSE:0.053316  CVRMSE:17.630644  test_loss:\n",
            "5.586452004709481e-06\n",
            "R2:0.064848  MSE:0.002730  KL:1.700174  MAE:0.008435  RMSE:0.052254  CVRMSE:17.843949  R2:0.064525  MSE:0.002731  KL:1.587066  MAE:0.008468  RMSE:0.052263  CVRMSE:17.847029  R2:0.064762  MSE:0.002731  KL:1.600062  MAE:0.008477  RMSE:0.052256  CVRMSE:17.844764  R2:0.064245  MSE:0.002732  KL:1.584316  MAE:0.008466  RMSE:0.052271  CVRMSE:17.849697  R2:0.064774  MSE:0.002731  KL:1.571855  MAE:0.008457  RMSE:0.052256  CVRMSE:17.844656  train_loss:\n",
            "0.0027306743580779755\n",
            "R2:0.057591  MSE:0.002841  KL:0.101139  MAE:0.005591  RMSE:0.053304  CVRMSE:17.626561  test_loss:\n",
            "5.583864519005802e-06\n",
            "R2:0.064878  MSE:0.002730  KL:1.737214  MAE:0.008471  RMSE:0.052253  CVRMSE:17.843661  R2:0.064846  MSE:0.002730  KL:1.639765  MAE:0.008485  RMSE:0.052254  CVRMSE:17.843967  R2:0.064428  MSE:0.002732  KL:1.637591  MAE:0.008440  RMSE:0.052266  CVRMSE:17.847957  R2:0.065146  MSE:0.002730  KL:1.762952  MAE:0.008471  RMSE:0.052245  CVRMSE:17.841106  R2:0.064568  MSE:0.002731  KL:1.704276  MAE:0.008404  RMSE:0.052262  CVRMSE:17.846619  train_loss:\n",
            "0.002731274975775923\n",
            "R2:0.057446  MSE:0.002842  KL:0.101947  MAE:0.005929  RMSE:0.053308  CVRMSE:17.627918  test_loss:\n",
            "5.584688566759714e-06\n",
            "R2:0.064857  MSE:0.002730  KL:1.758027  MAE:0.008460  RMSE:0.052254  CVRMSE:17.843857  R2:0.064371  MSE:0.002732  KL:1.720052  MAE:0.008430  RMSE:0.052267  CVRMSE:17.848500  R2:0.065613  MSE:0.002728  KL:1.600872  MAE:0.008512  RMSE:0.052232  CVRMSE:17.836646  R2:0.064834  MSE:0.002730  KL:1.677273  MAE:0.008472  RMSE:0.052254  CVRMSE:17.844078  R2:0.064796  MSE:0.002731  KL:1.681250  MAE:0.008489  RMSE:0.052255  CVRMSE:17.844446  train_loss:\n",
            "0.002730610110640133\n",
            "R2:0.056179  MSE:0.002846  KL:0.102293  MAE:0.005667  RMSE:0.053344  CVRMSE:17.639761  test_loss:\n",
            "5.592255682604433e-06\n",
            "R2:0.065086  MSE:0.002730  KL:1.751158  MAE:0.008489  RMSE:0.052247  CVRMSE:17.841678  R2:0.065139  MSE:0.002730  KL:1.681575  MAE:0.008498  RMSE:0.052246  CVRMSE:17.841166  R2:0.064590  MSE:0.002731  KL:1.666795  MAE:0.008445  RMSE:0.052261  CVRMSE:17.846411  R2:0.064134  MSE:0.002733  KL:1.727385  MAE:0.008471  RMSE:0.052274  CVRMSE:17.850753  R2:0.064288  MSE:0.002732  KL:1.771836  MAE:0.008466  RMSE:0.052269  CVRMSE:17.849289  train_loss:\n",
            "0.002732092466848261\n",
            "R2:0.056223  MSE:0.002845  KL:0.101320  MAE:0.005534  RMSE:0.053342  CVRMSE:17.639352  test_loss:\n",
            "5.592012955994695e-06\n",
            "R2:0.064721  MSE:0.002731  KL:1.690621  MAE:0.008480  RMSE:0.052257  CVRMSE:17.845161  R2:0.064351  MSE:0.002732  KL:1.781035  MAE:0.008435  RMSE:0.052268  CVRMSE:17.848689  R2:0.065132  MSE:0.002730  KL:1.767785  MAE:0.008495  RMSE:0.052246  CVRMSE:17.841232  R2:0.064726  MSE:0.002731  KL:1.778767  MAE:0.008454  RMSE:0.052257  CVRMSE:17.845108  R2:0.064703  MSE:0.002731  KL:1.771874  MAE:0.008453  RMSE:0.052258  CVRMSE:17.845326  train_loss:\n",
            "0.0027308791713068718\n",
            "R2:0.057436  MSE:0.002842  KL:0.101456  MAE:0.005700  RMSE:0.053308  CVRMSE:17.628015  test_loss:\n",
            "5.58477663659164e-06\n",
            "R2:0.064470  MSE:0.002732  KL:1.705905  MAE:0.008504  RMSE:0.052264  CVRMSE:17.847553  R2:0.064227  MSE:0.002732  KL:1.700385  MAE:0.008481  RMSE:0.052271  CVRMSE:17.849874  R2:0.064825  MSE:0.002731  KL:1.662722  MAE:0.008450  RMSE:0.052254  CVRMSE:17.844161  R2:0.064688  MSE:0.002731  KL:1.691676  MAE:0.008421  RMSE:0.052258  CVRMSE:17.845468  R2:0.064823  MSE:0.002731  KL:1.694118  MAE:0.008454  RMSE:0.052254  CVRMSE:17.844189  train_loss:\n",
            "0.0027305313805355584\n",
            "R2:0.055858  MSE:0.002846  KL:0.101368  MAE:0.005397  RMSE:0.053353  CVRMSE:17.642758  test_loss:\n",
            "5.5941998764456e-06\n",
            "R2:0.064248  MSE:0.002732  KL:1.797117  MAE:0.008437  RMSE:0.052271  CVRMSE:17.849672  R2:0.064843  MSE:0.002730  KL:1.655213  MAE:0.008473  RMSE:0.052254  CVRMSE:17.843990  R2:0.064444  MSE:0.002732  KL:1.651202  MAE:0.008439  RMSE:0.052265  CVRMSE:17.847803  R2:0.064508  MSE:0.002731  KL:1.614444  MAE:0.008476  RMSE:0.052263  CVRMSE:17.847187  R2:0.064360  MSE:0.002732  KL:1.642559  MAE:0.008464  RMSE:0.052267  CVRMSE:17.848602  train_loss:\n",
            "0.0027318819484405702\n",
            "R2:0.055815  MSE:0.002847  KL:0.099192  MAE:0.005478  RMSE:0.053354  CVRMSE:17.643168  test_loss:\n",
            "5.594451637670164e-06\n",
            "R2:0.064737  MSE:0.002731  KL:1.703784  MAE:0.008439  RMSE:0.052257  CVRMSE:17.845003  R2:0.064424  MSE:0.002732  KL:1.719806  MAE:0.008453  RMSE:0.052266  CVRMSE:17.847992  R2:0.064858  MSE:0.002730  KL:1.760289  MAE:0.008442  RMSE:0.052254  CVRMSE:17.843853  R2:0.064447  MSE:0.002732  KL:1.555441  MAE:0.008456  RMSE:0.052265  CVRMSE:17.847773  R2:0.065163  MSE:0.002730  KL:1.668799  MAE:0.008479  RMSE:0.052245  CVRMSE:17.840941  train_loss:\n",
            "0.002729537395457832\n",
            "R2:0.057650  MSE:0.002841  KL:0.102368  MAE:0.005638  RMSE:0.053302  CVRMSE:17.626010  test_loss:\n",
            "5.583506820034072e-06\n",
            "R2:0.064775  MSE:0.002731  KL:1.590389  MAE:0.008488  RMSE:0.052256  CVRMSE:17.844646  R2:0.064599  MSE:0.002731  KL:1.794874  MAE:0.008455  RMSE:0.052261  CVRMSE:17.846317  R2:0.064761  MSE:0.002731  KL:1.739834  MAE:0.008467  RMSE:0.052256  CVRMSE:17.844779  R2:0.064410  MSE:0.002732  KL:1.575354  MAE:0.008464  RMSE:0.052266  CVRMSE:17.848126  R2:0.064437  MSE:0.002732  KL:1.689963  MAE:0.008430  RMSE:0.052265  CVRMSE:17.847866  train_loss:\n",
            "0.002731656665804788\n",
            "R2:0.055927  MSE:0.002846  KL:0.099755  MAE:0.005441  RMSE:0.053351  CVRMSE:17.642119  test_loss:\n",
            "5.593786711159169e-06\n",
            "R2:0.064171  MSE:0.002732  KL:1.776943  MAE:0.008430  RMSE:0.052273  CVRMSE:17.850408  R2:0.064861  MSE:0.002730  KL:1.656352  MAE:0.008475  RMSE:0.052253  CVRMSE:17.843823  R2:0.064593  MSE:0.002731  KL:1.806040  MAE:0.008420  RMSE:0.052261  CVRMSE:17.846378  R2:0.064720  MSE:0.002731  KL:1.621032  MAE:0.008485  RMSE:0.052257  CVRMSE:17.845166  R2:0.063987  MSE:0.002733  KL:1.650493  MAE:0.008423  RMSE:0.052278  CVRMSE:17.852159  train_loss:\n",
            "0.002732970946834444\n",
            "R2:0.056401  MSE:0.002845  KL:0.101754  MAE:0.005423  RMSE:0.053337  CVRMSE:17.637684  test_loss:\n",
            "5.590964178772395e-06\n",
            "R2:0.064141  MSE:0.002733  KL:1.713022  MAE:0.008435  RMSE:0.052274  CVRMSE:17.850694  R2:0.064961  MSE:0.002730  KL:1.662093  MAE:0.008489  RMSE:0.052251  CVRMSE:17.842868  R2:0.064520  MSE:0.002731  KL:1.641923  MAE:0.008471  RMSE:0.052263  CVRMSE:17.847071  R2:0.064870  MSE:0.002730  KL:1.722191  MAE:0.008447  RMSE:0.052253  CVRMSE:17.843740  R2:0.065105  MSE:0.002730  KL:1.701498  MAE:0.008500  RMSE:0.052247  CVRMSE:17.841497  train_loss:\n",
            "0.0027297074964130513\n",
            "R2:0.056810  MSE:0.002844  KL:0.104619  MAE:0.005862  RMSE:0.053326  CVRMSE:17.633864  test_loss:\n",
            "5.588473594955181e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.064456  MSE:0.002732  KL:1.592869  MAE:0.008475  RMSE:0.052265  CVRMSE:17.847686  R2:0.064599  MSE:0.002731  KL:1.694082  MAE:0.008464  RMSE:0.052261  CVRMSE:17.846322  R2:0.064635  MSE:0.002731  KL:1.742830  MAE:0.008471  RMSE:0.052260  CVRMSE:17.845979  R2:0.064600  MSE:0.002731  KL:1.617236  MAE:0.008486  RMSE:0.052261  CVRMSE:17.846311  R2:0.064951  MSE:0.002730  KL:1.751280  MAE:0.008426  RMSE:0.052251  CVRMSE:17.842962  train_loss:\n",
            "0.002730155742312368\n",
            "R2:0.056884  MSE:0.002843  KL:0.100740  MAE:0.005550  RMSE:0.053324  CVRMSE:17.633171  test_loss:\n",
            "5.588077468592933e-06\n",
            "R2:0.064745  MSE:0.002731  KL:1.648253  MAE:0.008481  RMSE:0.052257  CVRMSE:17.844927  R2:0.064765  MSE:0.002731  KL:1.741393  MAE:0.008428  RMSE:0.052256  CVRMSE:17.844735  R2:0.065008  MSE:0.002730  KL:1.755607  MAE:0.008450  RMSE:0.052249  CVRMSE:17.842421  R2:0.064726  MSE:0.002731  KL:1.748324  MAE:0.008494  RMSE:0.052257  CVRMSE:17.845111  R2:0.064690  MSE:0.002731  KL:1.677063  MAE:0.008428  RMSE:0.052258  CVRMSE:17.845457  train_loss:\n",
            "0.002730919377886199\n",
            "R2:0.057752  MSE:0.002841  KL:0.101738  MAE:0.005950  RMSE:0.053299  CVRMSE:17.625056  test_loss:\n",
            "5.582871943918687e-06\n",
            "R2:0.064818  MSE:0.002731  KL:1.682847  MAE:0.008463  RMSE:0.052255  CVRMSE:17.844230  R2:0.064639  MSE:0.002731  KL:1.611868  MAE:0.008476  RMSE:0.052260  CVRMSE:17.845942  R2:0.064877  MSE:0.002730  KL:1.727803  MAE:0.008470  RMSE:0.052253  CVRMSE:17.843669  R2:0.064792  MSE:0.002731  KL:1.647050  MAE:0.008507  RMSE:0.052255  CVRMSE:17.844479  R2:0.064296  MSE:0.002732  KL:1.670514  MAE:0.008417  RMSE:0.052269  CVRMSE:17.849212  train_loss:\n",
            "0.0027320687873782336\n",
            "R2:0.056703  MSE:0.002844  KL:0.099643  MAE:0.005468  RMSE:0.053329  CVRMSE:17.634862  test_loss:\n",
            "5.5891627580563256e-06\n",
            "R2:0.064435  MSE:0.002732  KL:1.785619  MAE:0.008453  RMSE:0.052265  CVRMSE:17.847890  R2:0.064455  MSE:0.002732  KL:1.638296  MAE:0.008452  RMSE:0.052265  CVRMSE:17.847695  R2:0.064677  MSE:0.002731  KL:1.647132  MAE:0.008501  RMSE:0.052259  CVRMSE:17.845576  R2:0.064177  MSE:0.002732  KL:1.655989  MAE:0.008419  RMSE:0.052273  CVRMSE:17.850350  R2:0.064499  MSE:0.002731  KL:1.699778  MAE:0.008460  RMSE:0.052264  CVRMSE:17.847277  train_loss:\n",
            "0.0027314764952395573\n",
            "R2:0.056021  MSE:0.002846  KL:0.099867  MAE:0.005634  RMSE:0.053348  CVRMSE:17.641239  test_loss:\n",
            "5.593206878486648e-06\n",
            "R2:0.064360  MSE:0.002732  KL:1.766848  MAE:0.008418  RMSE:0.052267  CVRMSE:17.848604  R2:0.065001  MSE:0.002730  KL:1.761646  MAE:0.008470  RMSE:0.052249  CVRMSE:17.842486  R2:0.064799  MSE:0.002731  KL:1.598646  MAE:0.008444  RMSE:0.052255  CVRMSE:17.844410  R2:0.064598  MSE:0.002731  KL:1.779596  MAE:0.008444  RMSE:0.052261  CVRMSE:17.846327  R2:0.065238  MSE:0.002729  KL:1.784559  MAE:0.008451  RMSE:0.052243  CVRMSE:17.840222  train_loss:\n",
            "0.0027293174442505237\n",
            "R2:0.057232  MSE:0.002842  KL:0.099907  MAE:0.005697  RMSE:0.053314  CVRMSE:17.629924  test_loss:\n",
            "5.585997541951403e-06\n",
            "R2:0.065105  MSE:0.002730  KL:1.657976  MAE:0.008486  RMSE:0.052247  CVRMSE:17.841493  R2:0.064110  MSE:0.002733  KL:1.639189  MAE:0.008443  RMSE:0.052274  CVRMSE:17.850989  R2:0.064214  MSE:0.002732  KL:1.686351  MAE:0.008430  RMSE:0.052271  CVRMSE:17.849989  R2:0.064490  MSE:0.002732  KL:1.645759  MAE:0.008516  RMSE:0.052264  CVRMSE:17.847363  R2:0.064638  MSE:0.002731  KL:1.626688  MAE:0.008490  RMSE:0.052260  CVRMSE:17.845946  train_loss:\n",
            "0.0027310689594696774\n",
            "R2:0.055709  MSE:0.002847  KL:0.098952  MAE:0.005477  RMSE:0.053357  CVRMSE:17.644153  test_loss:\n",
            "5.595081306262181e-06\n",
            "R2:0.064428  MSE:0.002732  KL:1.654667  MAE:0.008476  RMSE:0.052266  CVRMSE:17.847951  R2:0.065021  MSE:0.002730  KL:1.610360  MAE:0.008462  RMSE:0.052249  CVRMSE:17.842298  R2:0.064430  MSE:0.002732  KL:1.718909  MAE:0.008440  RMSE:0.052265  CVRMSE:17.847933  R2:0.065105  MSE:0.002730  KL:1.674069  MAE:0.008469  RMSE:0.052247  CVRMSE:17.841495  R2:0.064707  MSE:0.002731  KL:1.719127  MAE:0.008444  RMSE:0.052258  CVRMSE:17.845289  train_loss:\n",
            "0.002730867949597845\n",
            "R2:0.058644  MSE:0.002838  KL:0.107497  MAE:0.005566  RMSE:0.053274  CVRMSE:17.616709  test_loss:\n",
            "5.5775863336024675e-06\n",
            "R2:0.064353  MSE:0.002732  KL:1.727802  MAE:0.008409  RMSE:0.052268  CVRMSE:17.848665  R2:0.064387  MSE:0.002732  KL:1.726828  MAE:0.008449  RMSE:0.052267  CVRMSE:17.848347  R2:0.064499  MSE:0.002731  KL:1.736829  MAE:0.008437  RMSE:0.052264  CVRMSE:17.847275  R2:0.065037  MSE:0.002730  KL:1.669933  MAE:0.008503  RMSE:0.052248  CVRMSE:17.842142  R2:0.064688  MSE:0.002731  KL:1.624539  MAE:0.008475  RMSE:0.052258  CVRMSE:17.845472  train_loss:\n",
            "0.0027309240503649882\n",
            "R2:0.057302  MSE:0.002842  KL:0.103007  MAE:0.005942  RMSE:0.053312  CVRMSE:17.629261  test_loss:\n",
            "5.585540136620533e-06\n",
            "R2:0.064534  MSE:0.002731  KL:1.610256  MAE:0.008472  RMSE:0.052263  CVRMSE:17.846942  R2:0.064132  MSE:0.002733  KL:1.626739  MAE:0.008468  RMSE:0.052274  CVRMSE:17.850777  R2:0.064346  MSE:0.002732  KL:1.635022  MAE:0.008432  RMSE:0.052268  CVRMSE:17.848739  R2:0.065261  MSE:0.002729  KL:1.727270  MAE:0.008456  RMSE:0.052242  CVRMSE:17.840005  R2:0.064944  MSE:0.002730  KL:1.641109  MAE:0.008486  RMSE:0.052251  CVRMSE:17.843026  train_loss:\n",
            "0.0027301755236011\n",
            "R2:0.058072  MSE:0.002840  KL:0.102273  MAE:0.005725  RMSE:0.053290  CVRMSE:17.622061  test_loss:\n",
            "5.580986415471979e-06\n",
            "R2:0.064712  MSE:0.002731  KL:1.655417  MAE:0.008482  RMSE:0.052258  CVRMSE:17.845246  R2:0.064145  MSE:0.002733  KL:1.641376  MAE:0.008444  RMSE:0.052273  CVRMSE:17.850651  R2:0.064958  MSE:0.002730  KL:1.688360  MAE:0.008464  RMSE:0.052251  CVRMSE:17.842900  R2:0.064856  MSE:0.002730  KL:1.657414  MAE:0.008444  RMSE:0.052254  CVRMSE:17.843870  R2:0.064869  MSE:0.002730  KL:1.729027  MAE:0.008463  RMSE:0.052253  CVRMSE:17.843741  train_loss:\n",
            "0.0027303943356395276\n",
            "R2:0.058536  MSE:0.002838  KL:0.105653  MAE:0.005822  RMSE:0.053277  CVRMSE:17.617721  test_loss:\n",
            "5.578207557727191e-06\n",
            "R2:0.065107  MSE:0.002730  KL:1.720739  MAE:0.008453  RMSE:0.052247  CVRMSE:17.841474  R2:0.064952  MSE:0.002730  KL:1.763707  MAE:0.008457  RMSE:0.052251  CVRMSE:17.842950  R2:0.065216  MSE:0.002729  KL:1.725312  MAE:0.008449  RMSE:0.052244  CVRMSE:17.840438  R2:0.064796  MSE:0.002731  KL:1.714522  MAE:0.008497  RMSE:0.052255  CVRMSE:17.844441  R2:0.064561  MSE:0.002731  KL:1.804187  MAE:0.008441  RMSE:0.052262  CVRMSE:17.846684  train_loss:\n",
            "0.002731294803988363\n",
            "R2:0.058216  MSE:0.002839  KL:0.105068  MAE:0.005601  RMSE:0.053286  CVRMSE:17.620717  test_loss:\n",
            "5.58013684098773e-06\n",
            "R2:0.064939  MSE:0.002730  KL:1.663067  MAE:0.008448  RMSE:0.052251  CVRMSE:17.843077  R2:0.064288  MSE:0.002732  KL:1.736287  MAE:0.008429  RMSE:0.052269  CVRMSE:17.849288  R2:0.064385  MSE:0.002732  KL:1.806806  MAE:0.008428  RMSE:0.052267  CVRMSE:17.848359  R2:0.064311  MSE:0.002732  KL:1.710324  MAE:0.008449  RMSE:0.052269  CVRMSE:17.849068  R2:0.065093  MSE:0.002730  KL:1.741876  MAE:0.008489  RMSE:0.052247  CVRMSE:17.841604  train_loss:\n",
            "0.0027297403431206925\n",
            "R2:0.057998  MSE:0.002840  KL:0.105469  MAE:0.005789  RMSE:0.053292  CVRMSE:17.622758  test_loss:\n",
            "5.581412084719874e-06\n",
            "R2:0.064229  MSE:0.002732  KL:1.734878  MAE:0.008424  RMSE:0.052271  CVRMSE:17.849851  R2:0.064454  MSE:0.002732  KL:1.699000  MAE:0.008432  RMSE:0.052265  CVRMSE:17.847706  R2:0.064650  MSE:0.002731  KL:1.603711  MAE:0.008456  RMSE:0.052259  CVRMSE:17.845839  R2:0.064931  MSE:0.002730  KL:1.741527  MAE:0.008466  RMSE:0.052251  CVRMSE:17.843153  R2:0.063759  MSE:0.002734  KL:1.648766  MAE:0.008450  RMSE:0.052284  CVRMSE:17.854333  train_loss:\n",
            "0.0027336366139315785\n",
            "R2:0.056284  MSE:0.002845  KL:0.101609  MAE:0.005656  RMSE:0.053341  CVRMSE:17.638776  test_loss:\n",
            "5.5916298944943015e-06\n",
            "R2:0.064626  MSE:0.002731  KL:1.791534  MAE:0.008473  RMSE:0.052260  CVRMSE:17.846065  R2:0.064404  MSE:0.002732  KL:1.754908  MAE:0.008428  RMSE:0.052266  CVRMSE:17.848186  R2:0.064342  MSE:0.002732  KL:1.615340  MAE:0.008433  RMSE:0.052268  CVRMSE:17.848769  R2:0.064916  MSE:0.002730  KL:1.763347  MAE:0.008433  RMSE:0.052252  CVRMSE:17.843301  R2:0.064526  MSE:0.002731  KL:1.620830  MAE:0.008421  RMSE:0.052263  CVRMSE:17.847020  train_loss:\n",
            "0.0027313978563197555\n",
            "R2:0.059228  MSE:0.002836  KL:0.106865  MAE:0.005774  RMSE:0.053257  CVRMSE:17.611245  test_loss:\n",
            "5.574090409887042e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.064303  MSE:0.002732  KL:1.664241  MAE:0.008439  RMSE:0.052269  CVRMSE:17.849148  R2:0.064583  MSE:0.002731  KL:1.576043  MAE:0.008445  RMSE:0.052261  CVRMSE:17.846474  R2:0.064168  MSE:0.002732  KL:1.796350  MAE:0.008418  RMSE:0.052273  CVRMSE:17.850430  R2:0.064346  MSE:0.002732  KL:1.714394  MAE:0.008441  RMSE:0.052268  CVRMSE:17.848737  R2:0.065047  MSE:0.002730  KL:1.782470  MAE:0.008482  RMSE:0.052248  CVRMSE:17.842051  train_loss:\n",
            "0.002729876980947571\n",
            "R2:0.058913  MSE:0.002837  KL:0.106155  MAE:0.006180  RMSE:0.053266  CVRMSE:17.614195  test_loss:\n",
            "5.5759237337438345e-06\n",
            "R2:0.064603  MSE:0.002731  KL:1.704743  MAE:0.008488  RMSE:0.052261  CVRMSE:17.846283  R2:0.065151  MSE:0.002730  KL:1.568244  MAE:0.008501  RMSE:0.052245  CVRMSE:17.841057  R2:0.064490  MSE:0.002732  KL:1.684507  MAE:0.008433  RMSE:0.052264  CVRMSE:17.847359  R2:0.064327  MSE:0.002732  KL:1.678769  MAE:0.008466  RMSE:0.052268  CVRMSE:17.848919  R2:0.064538  MSE:0.002731  KL:1.686070  MAE:0.008441  RMSE:0.052262  CVRMSE:17.846901  train_loss:\n",
            "0.0027313612116459606\n",
            "R2:0.055627  MSE:0.002847  KL:0.099574  MAE:0.005663  RMSE:0.053359  CVRMSE:17.644922  test_loss:\n",
            "5.595549419813841e-06\n",
            "R2:0.064881  MSE:0.002730  KL:1.692004  MAE:0.008489  RMSE:0.052253  CVRMSE:17.843635  R2:0.065096  MSE:0.002730  KL:1.629322  MAE:0.008492  RMSE:0.052247  CVRMSE:17.841580  R2:0.064702  MSE:0.002731  KL:1.670928  MAE:0.008465  RMSE:0.052258  CVRMSE:17.845336  R2:0.064777  MSE:0.002731  KL:1.693296  MAE:0.008478  RMSE:0.052256  CVRMSE:17.844622  R2:0.064471  MSE:0.002732  KL:1.626087  MAE:0.008459  RMSE:0.052264  CVRMSE:17.847547  train_loss:\n",
            "0.0027315591005940333\n",
            "R2:0.056882  MSE:0.002843  KL:0.100763  MAE:0.006066  RMSE:0.053324  CVRMSE:17.633189  test_loss:\n",
            "5.588037572830648e-06\n",
            "R2:0.064721  MSE:0.002731  KL:1.709157  MAE:0.008455  RMSE:0.052257  CVRMSE:17.845154  R2:0.064393  MSE:0.002732  KL:1.598958  MAE:0.008463  RMSE:0.052266  CVRMSE:17.848282  R2:0.064486  MSE:0.002732  KL:1.668785  MAE:0.008460  RMSE:0.052264  CVRMSE:17.847396  R2:0.064551  MSE:0.002731  KL:1.594904  MAE:0.008474  RMSE:0.052262  CVRMSE:17.846776  R2:0.065195  MSE:0.002729  KL:1.621939  MAE:0.008479  RMSE:0.052244  CVRMSE:17.840639  train_loss:\n",
            "0.002729444899718322\n",
            "R2:0.057466  MSE:0.002842  KL:0.106263  MAE:0.006308  RMSE:0.053307  CVRMSE:17.627729  test_loss:\n",
            "5.584525537993261e-06\n",
            "R2:0.065585  MSE:0.002728  KL:1.696895  MAE:0.008483  RMSE:0.052233  CVRMSE:17.836912  R2:0.064765  MSE:0.002731  KL:1.728180  MAE:0.008436  RMSE:0.052256  CVRMSE:17.844737  R2:0.064085  MSE:0.002733  KL:1.662856  MAE:0.008432  RMSE:0.052275  CVRMSE:17.851226  R2:0.064873  MSE:0.002730  KL:1.667781  MAE:0.008445  RMSE:0.052253  CVRMSE:17.843709  R2:0.064537  MSE:0.002731  KL:1.614564  MAE:0.008476  RMSE:0.052262  CVRMSE:17.846913  train_loss:\n",
            "0.002731365166207848\n",
            "R2:0.056005  MSE:0.002846  KL:0.101325  MAE:0.005435  RMSE:0.053348  CVRMSE:17.641391  test_loss:\n",
            "5.593323386569179e-06\n",
            "R2:0.064874  MSE:0.002730  KL:1.614117  MAE:0.008474  RMSE:0.052253  CVRMSE:17.843695  R2:0.065428  MSE:0.002729  KL:1.678390  MAE:0.008494  RMSE:0.052238  CVRMSE:17.838408  R2:0.064734  MSE:0.002731  KL:1.566564  MAE:0.008494  RMSE:0.052257  CVRMSE:17.845029  R2:0.064831  MSE:0.002731  KL:1.667537  MAE:0.008448  RMSE:0.052254  CVRMSE:17.844106  R2:0.064284  MSE:0.002732  KL:1.615862  MAE:0.008426  RMSE:0.052270  CVRMSE:17.849326  train_loss:\n",
            "0.0027321036289597676\n",
            "R2:0.055919  MSE:0.002846  KL:0.098134  MAE:0.005489  RMSE:0.053351  CVRMSE:17.642188  test_loss:\n",
            "5.593830542514569e-06\n",
            "R2:0.064102  MSE:0.002733  KL:1.734921  MAE:0.008456  RMSE:0.052275  CVRMSE:17.851065  R2:0.065052  MSE:0.002730  KL:1.636563  MAE:0.008430  RMSE:0.052248  CVRMSE:17.841999  R2:0.065003  MSE:0.002730  KL:1.759218  MAE:0.008448  RMSE:0.052249  CVRMSE:17.842471  R2:0.064959  MSE:0.002730  KL:1.715355  MAE:0.008431  RMSE:0.052251  CVRMSE:17.842890  R2:0.065016  MSE:0.002730  KL:1.719266  MAE:0.008451  RMSE:0.052249  CVRMSE:17.842347  train_loss:\n",
            "0.002729967643436505\n",
            "R2:0.057718  MSE:0.002841  KL:0.101965  MAE:0.005889  RMSE:0.053300  CVRMSE:17.625372  test_loss:\n",
            "5.583074587849222e-06\n",
            "R2:0.065050  MSE:0.002730  KL:1.629633  MAE:0.008458  RMSE:0.052248  CVRMSE:17.842014  R2:0.064473  MSE:0.002732  KL:1.788692  MAE:0.008483  RMSE:0.052264  CVRMSE:17.847523  R2:0.064969  MSE:0.002730  KL:1.691136  MAE:0.008445  RMSE:0.052250  CVRMSE:17.842787  R2:0.064622  MSE:0.002731  KL:1.709776  MAE:0.008410  RMSE:0.052260  CVRMSE:17.846102  R2:0.064569  MSE:0.002731  KL:1.708379  MAE:0.008417  RMSE:0.052262  CVRMSE:17.846604  train_loss:\n",
            "0.0027312705061385005\n",
            "R2:0.057166  MSE:0.002843  KL:0.101119  MAE:0.005679  RMSE:0.053316  CVRMSE:17.630540  test_loss:\n",
            "5.5863880369425645e-06\n",
            "R2:0.065108  MSE:0.002730  KL:1.691200  MAE:0.008460  RMSE:0.052247  CVRMSE:17.841462  R2:0.063752  MSE:0.002734  KL:1.733131  MAE:0.008404  RMSE:0.052284  CVRMSE:17.854399  R2:0.064319  MSE:0.002732  KL:1.660698  MAE:0.008465  RMSE:0.052269  CVRMSE:17.848991  R2:0.064792  MSE:0.002731  KL:1.552379  MAE:0.008474  RMSE:0.052255  CVRMSE:17.844480  R2:0.065318  MSE:0.002729  KL:1.680250  MAE:0.008504  RMSE:0.052241  CVRMSE:17.839459  train_loss:\n",
            "0.0027290839380758308\n",
            "R2:0.057053  MSE:0.002843  KL:0.102430  MAE:0.005539  RMSE:0.053319  CVRMSE:17.631593  test_loss:\n",
            "5.5870691964109625e-06\n",
            "R2:0.064729  MSE:0.002731  KL:1.749286  MAE:0.008418  RMSE:0.052257  CVRMSE:17.845083  R2:0.065105  MSE:0.002730  KL:1.593176  MAE:0.008480  RMSE:0.052247  CVRMSE:17.841492  R2:0.064811  MSE:0.002731  KL:1.654757  MAE:0.008450  RMSE:0.052255  CVRMSE:17.844300  R2:0.064529  MSE:0.002731  KL:1.580669  MAE:0.008469  RMSE:0.052263  CVRMSE:17.846993  R2:0.064667  MSE:0.002731  KL:1.741498  MAE:0.008460  RMSE:0.052259  CVRMSE:17.845673  train_loss:\n",
            "0.002730985447720919\n",
            "R2:0.056050  MSE:0.002846  KL:0.098864  MAE:0.005795  RMSE:0.053347  CVRMSE:17.640964  test_loss:\n",
            "5.593021141463168e-06\n",
            "R2:0.065145  MSE:0.002730  KL:1.636157  MAE:0.008458  RMSE:0.052245  CVRMSE:17.841112  R2:0.064358  MSE:0.002732  KL:1.728840  MAE:0.008432  RMSE:0.052267  CVRMSE:17.848619  R2:0.064692  MSE:0.002731  KL:1.708726  MAE:0.008437  RMSE:0.052258  CVRMSE:17.845431  R2:0.064572  MSE:0.002731  KL:1.697564  MAE:0.008465  RMSE:0.052261  CVRMSE:17.846583  R2:0.064802  MSE:0.002731  KL:1.730594  MAE:0.008440  RMSE:0.052255  CVRMSE:17.844384  train_loss:\n",
            "0.0027305908398629515\n",
            "R2:0.057932  MSE:0.002840  KL:0.105275  MAE:0.005720  RMSE:0.053294  CVRMSE:17.623372  test_loss:\n",
            "5.581813472600964e-06\n",
            "R2:0.065320  MSE:0.002729  KL:1.751046  MAE:0.008470  RMSE:0.052241  CVRMSE:17.839443  R2:0.064585  MSE:0.002731  KL:1.711296  MAE:0.008454  RMSE:0.052261  CVRMSE:17.846452  R2:0.064668  MSE:0.002731  KL:1.682905  MAE:0.008424  RMSE:0.052259  CVRMSE:17.845662  R2:0.064739  MSE:0.002731  KL:1.598468  MAE:0.008507  RMSE:0.052257  CVRMSE:17.844985  R2:0.063797  MSE:0.002734  KL:1.720436  MAE:0.008428  RMSE:0.052283  CVRMSE:17.853971  train_loss:\n",
            "0.002733525740380088\n",
            "R2:0.056474  MSE:0.002845  KL:0.102183  MAE:0.005503  RMSE:0.053335  CVRMSE:17.637004  test_loss:\n",
            "5.590517365418668e-06\n",
            "R2:0.064945  MSE:0.002730  KL:1.726274  MAE:0.008452  RMSE:0.052251  CVRMSE:17.843025  R2:0.064112  MSE:0.002733  KL:1.682746  MAE:0.008458  RMSE:0.052274  CVRMSE:17.850965  R2:0.064804  MSE:0.002731  KL:1.749214  MAE:0.008444  RMSE:0.052255  CVRMSE:17.844369  R2:0.064830  MSE:0.002731  KL:1.704940  MAE:0.008470  RMSE:0.052254  CVRMSE:17.844121  R2:0.064242  MSE:0.002732  KL:1.632634  MAE:0.008466  RMSE:0.052271  CVRMSE:17.849726  train_loss:\n",
            "0.0027322262019477623\n",
            "R2:0.053657  MSE:0.002853  KL:0.098915  MAE:0.005327  RMSE:0.053415  CVRMSE:17.663316  test_loss:\n",
            "5.607323789746236e-06\n",
            "R2:0.064221  MSE:0.002732  KL:1.587938  MAE:0.008453  RMSE:0.052271  CVRMSE:17.849931  R2:0.064542  MSE:0.002731  KL:1.664459  MAE:0.008464  RMSE:0.052262  CVRMSE:17.846861  R2:0.064712  MSE:0.002731  KL:1.650618  MAE:0.008474  RMSE:0.052258  CVRMSE:17.845247  R2:0.063966  MSE:0.002733  KL:1.679771  MAE:0.008440  RMSE:0.052278  CVRMSE:17.852359  R2:0.065092  MSE:0.002730  KL:1.665521  MAE:0.008459  RMSE:0.052247  CVRMSE:17.841617  train_loss:\n",
            "0.002729744173084756\n",
            "R2:0.057640  MSE:0.002841  KL:0.105145  MAE:0.005484  RMSE:0.053302  CVRMSE:17.626106  test_loss:\n",
            "5.583582497864061e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.064688  MSE:0.002731  KL:1.731077  MAE:0.008439  RMSE:0.052258  CVRMSE:17.845475  R2:0.064379  MSE:0.002732  KL:1.767459  MAE:0.008429  RMSE:0.052267  CVRMSE:17.848424  R2:0.064461  MSE:0.002732  KL:1.777455  MAE:0.008439  RMSE:0.052265  CVRMSE:17.847634  R2:0.064899  MSE:0.002730  KL:1.683874  MAE:0.008465  RMSE:0.052252  CVRMSE:17.843460  R2:0.063806  MSE:0.002733  KL:1.786041  MAE:0.008441  RMSE:0.052283  CVRMSE:17.853883  train_loss:\n",
            "0.002733499015558268\n",
            "R2:0.056011  MSE:0.002846  KL:0.099016  MAE:0.005763  RMSE:0.053348  CVRMSE:17.641328  test_loss:\n",
            "5.593251218773543e-06\n",
            "R2:0.064669  MSE:0.002731  KL:1.590624  MAE:0.008481  RMSE:0.052259  CVRMSE:17.845651  R2:0.064686  MSE:0.002731  KL:1.735661  MAE:0.008474  RMSE:0.052258  CVRMSE:17.845490  R2:0.065101  MSE:0.002730  KL:1.621649  MAE:0.008488  RMSE:0.052247  CVRMSE:17.841530  R2:0.064795  MSE:0.002731  KL:1.760341  MAE:0.008435  RMSE:0.052255  CVRMSE:17.844450  R2:0.064473  MSE:0.002732  KL:1.624434  MAE:0.008470  RMSE:0.052264  CVRMSE:17.847527  train_loss:\n",
            "0.0027315529838789813\n",
            "R2:0.056022  MSE:0.002846  KL:0.102847  MAE:0.005475  RMSE:0.053348  CVRMSE:17.641225  test_loss:\n",
            "5.593209324628976e-06\n",
            "R2:0.064888  MSE:0.002730  KL:1.784816  MAE:0.008441  RMSE:0.052253  CVRMSE:17.843569  R2:0.064476  MSE:0.002732  KL:1.577582  MAE:0.008478  RMSE:0.052264  CVRMSE:17.847492  R2:0.064659  MSE:0.002731  KL:1.685556  MAE:0.008480  RMSE:0.052259  CVRMSE:17.845748  R2:0.064667  MSE:0.002731  KL:1.729333  MAE:0.008454  RMSE:0.052259  CVRMSE:17.845675  R2:0.064767  MSE:0.002731  KL:1.715840  MAE:0.008494  RMSE:0.052256  CVRMSE:17.844719  train_loss:\n",
            "0.002730693440230334\n",
            "R2:0.057494  MSE:0.002842  KL:0.102949  MAE:0.005556  RMSE:0.053306  CVRMSE:17.627470  test_loss:\n",
            "5.584443543307622e-06\n",
            "R2:0.065348  MSE:0.002729  KL:1.634307  MAE:0.008484  RMSE:0.052240  CVRMSE:17.839170  R2:0.064330  MSE:0.002732  KL:1.759439  MAE:0.008438  RMSE:0.052268  CVRMSE:17.848886  R2:0.064924  MSE:0.002730  KL:1.633864  MAE:0.008442  RMSE:0.052252  CVRMSE:17.843219  R2:0.065477  MSE:0.002729  KL:1.622608  MAE:0.008508  RMSE:0.052236  CVRMSE:17.837939  R2:0.064564  MSE:0.002731  KL:1.742164  MAE:0.008451  RMSE:0.052262  CVRMSE:17.846652  train_loss:\n",
            "0.0027312850960671386\n",
            "R2:0.057167  MSE:0.002843  KL:0.102265  MAE:0.005604  RMSE:0.053316  CVRMSE:17.630524  test_loss:\n",
            "5.5863818494756735e-06\n",
            "R2:0.064631  MSE:0.002731  KL:1.677863  MAE:0.008460  RMSE:0.052260  CVRMSE:17.846020  R2:0.064964  MSE:0.002730  KL:1.632691  MAE:0.008458  RMSE:0.052251  CVRMSE:17.842840  R2:0.064466  MSE:0.002732  KL:1.655661  MAE:0.008454  RMSE:0.052264  CVRMSE:17.847595  R2:0.064554  MSE:0.002731  KL:1.742515  MAE:0.008480  RMSE:0.052262  CVRMSE:17.846749  R2:0.064445  MSE:0.002732  KL:1.559992  MAE:0.008456  RMSE:0.052265  CVRMSE:17.847793  train_loss:\n",
            "0.002731634258284535\n",
            "R2:0.057446  MSE:0.002842  KL:0.103365  MAE:0.005451  RMSE:0.053308  CVRMSE:17.627915  test_loss:\n",
            "5.584739565951664e-06\n",
            "R2:0.064623  MSE:0.002731  KL:1.783031  MAE:0.008466  RMSE:0.052260  CVRMSE:17.846094  R2:0.064554  MSE:0.002731  KL:1.629273  MAE:0.008455  RMSE:0.052262  CVRMSE:17.846755  R2:0.064558  MSE:0.002731  KL:1.640603  MAE:0.008466  RMSE:0.052262  CVRMSE:17.846717  R2:0.064830  MSE:0.002731  KL:1.723560  MAE:0.008484  RMSE:0.052254  CVRMSE:17.844116  R2:0.064219  MSE:0.002732  KL:1.539469  MAE:0.008470  RMSE:0.052271  CVRMSE:17.849949  train_loss:\n",
            "0.0027322943243339557\n",
            "R2:0.057344  MSE:0.002842  KL:0.101767  MAE:0.005935  RMSE:0.053311  CVRMSE:17.628876  test_loss:\n",
            "5.585302129990719e-06\n",
            "R2:0.064564  MSE:0.002731  KL:1.743849  MAE:0.008482  RMSE:0.052262  CVRMSE:17.846660  R2:0.064716  MSE:0.002731  KL:1.718560  MAE:0.008428  RMSE:0.052257  CVRMSE:17.845206  R2:0.064114  MSE:0.002733  KL:1.622179  MAE:0.008466  RMSE:0.052274  CVRMSE:17.850952  R2:0.064451  MSE:0.002732  KL:1.828933  MAE:0.008446  RMSE:0.052265  CVRMSE:17.847737  R2:0.064592  MSE:0.002731  KL:1.821282  MAE:0.008432  RMSE:0.052261  CVRMSE:17.846387  train_loss:\n",
            "0.0027312041319765786\n",
            "R2:0.057022  MSE:0.002843  KL:0.102018  MAE:0.005745  RMSE:0.053320  CVRMSE:17.631887  test_loss:\n",
            "5.587234231942514e-06\n",
            "R2:0.064571  MSE:0.002731  KL:1.759261  MAE:0.008478  RMSE:0.052262  CVRMSE:17.846588  R2:0.064411  MSE:0.002732  KL:1.588577  MAE:0.008465  RMSE:0.052266  CVRMSE:17.848112  R2:0.064917  MSE:0.002730  KL:1.671352  MAE:0.008462  RMSE:0.052252  CVRMSE:17.843292  R2:0.065154  MSE:0.002730  KL:1.579266  MAE:0.008482  RMSE:0.052245  CVRMSE:17.841026  R2:0.065069  MSE:0.002730  KL:1.634542  MAE:0.008469  RMSE:0.052248  CVRMSE:17.841833  train_loss:\n",
            "0.0027298102438719535\n",
            "R2:0.055951  MSE:0.002846  KL:0.099865  MAE:0.005352  RMSE:0.053350  CVRMSE:17.641894  test_loss:\n",
            "5.593656633134764e-06\n",
            "R2:0.065219  MSE:0.002729  KL:1.652297  MAE:0.008494  RMSE:0.052243  CVRMSE:17.840410  R2:0.064630  MSE:0.002731  KL:1.746327  MAE:0.008460  RMSE:0.052260  CVRMSE:17.846024  R2:0.064814  MSE:0.002731  KL:1.671631  MAE:0.008462  RMSE:0.052255  CVRMSE:17.844267  R2:0.064231  MSE:0.002732  KL:1.745607  MAE:0.008453  RMSE:0.052271  CVRMSE:17.849828  R2:0.064966  MSE:0.002730  KL:1.619141  MAE:0.008491  RMSE:0.052250  CVRMSE:17.842818  train_loss:\n",
            "0.0027301116588341933\n",
            "R2:0.058653  MSE:0.002838  KL:0.109905  MAE:0.005779  RMSE:0.053274  CVRMSE:17.616628  test_loss:\n",
            "5.577506289142657e-06\n",
            "R2:0.065372  MSE:0.002729  KL:1.634262  MAE:0.008498  RMSE:0.052239  CVRMSE:17.838950  R2:0.064861  MSE:0.002730  KL:1.685126  MAE:0.008458  RMSE:0.052253  CVRMSE:17.843819  R2:0.064940  MSE:0.002730  KL:1.667819  MAE:0.008472  RMSE:0.052251  CVRMSE:17.843064  R2:0.065240  MSE:0.002729  KL:1.740805  MAE:0.008500  RMSE:0.052243  CVRMSE:17.840208  R2:0.064854  MSE:0.002730  KL:1.688191  MAE:0.008462  RMSE:0.052254  CVRMSE:17.843889  train_loss:\n",
            "0.002730439365991501\n",
            "R2:0.057309  MSE:0.002842  KL:0.102537  MAE:0.005812  RMSE:0.053312  CVRMSE:17.629197  test_loss:\n",
            "5.5855147712000095e-06\n",
            "Model 0 has MSE on validset:0.030515  Model 0 has KL on validset:0.024179  Model 0 has ATE error on validset:1.008857  "
          ]
        }
      ],
      "source": [
        "from uuid import RFC_4122\n",
        "import torch\n",
        "import math\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "from torch_geometric.utils import add_self_loops,degree\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import ssl\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net,self).__init__()\n",
        "        self.gat1=GATConv(in_channels=25,out_channels=8,heads=8,dropout=0.6)\n",
        "        self.gat2=GATConv(in_channels=64,out_channels=10,heads=1,dropout=0.6)\n",
        "        self.f1 = torch.nn.Linear(140,32)\n",
        "        self.f2 = torch.nn.Linear(32,1)\n",
        "    def forward(self,data):\n",
        "        x,edge_index=data.x, data.edge_index\n",
        "        x=self.gat1(x,edge_index)\n",
        "        x=self.gat2(x,edge_index)\n",
        "        x=x.reshape(-1,140)\n",
        "        x = self.f1(x)\n",
        "        x = self.f2(x)\n",
        "        return x\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "def train():\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "    y_actual = []\n",
        "    y_predicted = []\n",
        "    loss_all=0\n",
        "    for data in train_loader:\n",
        "        loss = 0\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        label = data.y.to(device)\n",
        "        loss = crit(output, label)\n",
        "        loss.backward()\n",
        "        loss_all += data.num_graphs * loss.item()\n",
        "        optimizer.step()\n",
        "        y_actual +=(label).cpu().detach().ravel().tolist()\n",
        "        y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "    \n",
        "    \n",
        "    loss=loss_all/len(Mydata_train)\n",
        "    r2=R2(y_predicted, y_actual)\n",
        "    mse = MSE(y_predicted, y_actual)\n",
        "    kl=kl_divergence(y_predicted, y_actual)\n",
        "\n",
        "    print(\"R2:%f\" % (R2(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MSE:%f\" % (MSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"KL:%f\" % (kl_divergence(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MAE:%f\" % (MAE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"RMSE:%f\" % (RMSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"CVRMSE:%f\" % (CVRMSE(y_predicted, y_actual)),end='  ')\n",
        "\n",
        "    return loss,r2,mse,kl\n",
        "\n",
        "\n",
        "def val():\n",
        "    model.eval()\n",
        "    y_actual = []\n",
        "    y_predicted = []\n",
        "    loss_all=0\n",
        "    for data in test_loader:\n",
        "      loss = 0\n",
        "      data = data.to(device)\n",
        "      output = model(data)\n",
        "      label = data.y.to(device)\n",
        "      y_actual +=(label).cpu().detach().ravel().tolist()\n",
        "      y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "      loss = crit(output, label)\n",
        "      loss_all += loss.item()\n",
        "\n",
        "    \n",
        "    loss = loss_all / len(Mydata_test)\n",
        "    r2=R2(y_predicted, y_actual)\n",
        "    mse = MSE(y_predicted, y_actual)\n",
        "    kl=kl_divergence(y_predicted, y_actual)\n",
        "\n",
        "    print(\"R2:%f\" % (R2(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MSE:%f\" % (MSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"KL:%f\" % (kl_divergence(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MAE:%f\" % (MAE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"RMSE:%f\" % (RMSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"CVRMSE:%f\" % (CVRMSE(y_predicted, y_actual)),end='  ')\n",
        "\n",
        "    return loss,r2, mse,kl\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 256\n",
        "batch_size = 512\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=5e-4)\n",
        "crit = F.mse_loss\n",
        "train_loader = DataLoader(Mydata_train, batch_size=batch_size)\n",
        "test_loader = DataLoader(Mydata_test, batch_size=512)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss,r2,mse,kl=train()\n",
        "    if epoch %5==0:      \n",
        "        print('train_loss:')\n",
        "        print(loss)\n",
        "\n",
        "        loss,r2,mse,kl=val()\n",
        "        print('test_loss:')\n",
        "        print(loss)\n",
        "\n",
        "y_predicted = []\n",
        "for data in test_loader:\n",
        "    loss = 0\n",
        "    data = data.to(device)\n",
        "    output = model(data)\n",
        "    label = data.y.to(device)\n",
        "    y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "df_test[\"y_hat\"]=y_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2txUFKIhFkOA",
        "outputId": "e5f8effd-aa98-40fa-b8e3-610216200cdb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>auuc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y_hat</th>\n",
              "      <td>0.880700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random</th>\n",
              "      <td>0.499185</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            auuc\n",
              "y_hat   0.880700\n",
              "Random  0.499185"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# estimate area under the uplift curve (AUUC)\n",
        "from causalml.metrics import *\n",
        "uplift=df_test.copy()\n",
        "tau_hat=pd.concat([t0,t1], axis=0, join=\"inner\")\n",
        "uplift=pd.concat([uplift,tau_hat], axis=1, join=\"inner\")\n",
        "uplift = uplift.loc[:,~uplift.columns.duplicated()]\n",
        "\n",
        "auuc=auuc_score(uplift, outcome_col='y', treatment_col='T', treatment_effect_col='tau')\n",
        "gat_auuc=pd.DataFrame(auuc[[\"y_hat\",\"Random\"]],columns=['auuc'])\n",
        "gat_auuc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8sZB2KGFkOB",
        "outputId": "4fd406c1-016d-417e-f668-4e71ceada1e2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>AUUC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>S Learner(LR)</th>\n",
              "      <td>0.497983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S Learner(XGB)</th>\n",
              "      <td>0.875572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S Learner(LGBM)</th>\n",
              "      <td>0.883033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GCN (Struct)</th>\n",
              "      <td>0.501865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GCN (Struct+Feature)</th>\n",
              "      <td>0.721959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GCN (Struct+Causal Weighting)</th>\n",
              "      <td>0.732616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GAT (Struct)</th>\n",
              "      <td>0.544286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GAT (Struct+Feature)</th>\n",
              "      <td>0.84763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GAT (Struct+Causal Weighting)</th>\n",
              "      <td>0.8807</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   AUUC\n",
              "S Learner(LR)                  0.497983\n",
              "S Learner(XGB)                 0.875572\n",
              "S Learner(LGBM)                0.883033\n",
              "GCN (Struct)                   0.501865\n",
              "GCN (Struct+Feature)           0.721959\n",
              "GCN (Struct+Causal Weighting)  0.732616\n",
              "GAT (Struct)                   0.544286\n",
              "GAT (Struct+Feature)            0.84763\n",
              "GAT (Struct+Causal Weighting)    0.8807"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result=pd.DataFrame(columns=[['AUUC']])\n",
        "result.loc['S Learner(LR)','AUUC']=0.497983\n",
        "result.loc['S Learner(XGB)','AUUC']=0.875572\n",
        "result.loc['S Learner(LGBM)','AUUC']=0.883033\n",
        "\n",
        "result.loc['GCN (Struct)','AUUC']=0.501865\n",
        "result.loc['GCN (Struct+Feature)','AUUC']=0.721959\n",
        "result.loc['GCN (Struct+Causal Weighting)','AUUC']=gcn_auuc.loc[\"y_hat\"].values\n",
        "\n",
        "result.loc['GAT (Struct)','AUUC']=0.544286\n",
        "result.loc['GAT (Struct+Feature)','AUUC']=0.847630\n",
        "result.loc['GAT (Struct+Causal Weighting)','AUUC']=gat_auuc.loc[\"y_hat\"].values\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6E9txoQFkOB",
        "outputId": "34e37ced-f84c-44f7-a6a2-1b5ddf856ffe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>GCN</th>\n",
              "      <th>GAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>equal weighting</th>\n",
              "      <td>0.721959</td>\n",
              "      <td>0.84763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>causal weighting</th>\n",
              "      <td>0.732616</td>\n",
              "      <td>0.8807</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       GCN      GAT\n",
              "equal weighting   0.721959  0.84763\n",
              "causal weighting  0.732616   0.8807"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result=pd.DataFrame(columns=[['GCN','GAT']])\n",
        "result.loc['equal weighting','GCN']=0.721959\n",
        "result.loc['equal weighting','GAT']=0.847630\n",
        "\n",
        "result.loc['causal weighting','GCN']=gcn_auuc.loc[\"y_hat\"].values\n",
        "result.loc['causal weighting','GAT']=gat_auuc.loc[\"y_hat\"].values\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "causal_weighting_embedding(10d)_ate(13d).ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "cd9ce680b654e493fbbd5797259a0b454ac6d0effeb2d1358682b4d9ed73e0e9"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}