{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypgk9VLvSyWC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from gensim.models import word2vec\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import random\n",
        "random.seed(2022)\n",
        "np.random.seed(2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvg78-OAfWzb",
        "outputId": "7555f4c2-d89a-4d9b-b646-6ca50eda35d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: node2vec in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (0.4.4)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.1.2 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from node2vec) (4.2.0)\n",
            "Requirement already satisfied: networkx<3.0,>=2.5 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from node2vec) (2.6.3)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.5 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from node2vec) (1.20.3)\n",
            "Requirement already satisfied: joblib<2.0.0,>=1.1.0 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from node2vec) (1.1.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.55.1 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from node2vec) (4.62.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.7.1)\n",
            "Requirement already satisfied: gensim in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.20.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /Users/patricia/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qlU_k_HFkNy"
      },
      "outputs": [],
      "source": [
        "df_train=pd.read_csv(\"criteo_sampled/criteo_train.csv\",index_col=0)\n",
        "df_test=pd.read_csv(\"criteo_sampled/criteo_test.csv\",index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI3sbjmrSyWF",
        "outputId": "317acbb6-c602-43be-fc2a-63bc904637f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-0.01388679662724193,\n",
              " 0.05071538371534517,\n",
              " 0.001328853128405821,\n",
              " -0.009331114629769072,\n",
              " 0.1933108932875426,\n",
              " 0.0008912893595676433,\n",
              " -0.0002958249515911775,\n",
              " -0.009201954095841196,\n",
              " 0.1034574925533489,\n",
              " 0.0741312293943781,\n",
              " -0.01664079180275199,\n",
              " 0.1331562899243786,\n",
              " 0.9310119697738223,\n",
              " 0.007447354046319239]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load causal weighting\n",
        "ate_list=[]\n",
        "ate=pd.read_excel(\"feats_ate_x13.xlsx\",index_col=0)\n",
        "for i in [c for c in df_train.columns[:-4]]+[\"visit\",\"treatment\"]:\n",
        "    ate_list.append(float(ate[ate['Feature']==i][\"ATE\"].values))\n",
        "ate_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXWeRuMvFkN1"
      },
      "outputs": [],
      "source": [
        "# load edge index\n",
        "edge_index=pd.read_csv('edge_index_criteo.csv')\n",
        "edge_index=torch.from_numpy(np.transpose(np.array(edge_index)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-pWo7KzFkN1",
        "outputId": "ae7ec32a-143f-4ec3-c0d3-52f70ee54b5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load node embedding\n",
        "model_dw=word2vec.Word2Vec.load(\"deepwalk_10d_x13.model\")\n",
        "model_n2v=word2vec.Word2Vec.load(\"Node2Vec_10d_x13.model\")\n",
        "\n",
        "lst_dw=[]\n",
        "lst_n2v=[]\n",
        "for i in range(14):\n",
        "    lst_dw.append(model_dw.wv[i])\n",
        "    lst_n2v.append(model_n2v.wv[i])\n",
        "len(lst_n2v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gyw5M55FkN2"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import entropy\n",
        "def R2(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    R2 = 1 - np.sum(np.square(y_actual-y_predicted)) / np.sum(np.square(y_actual-np.mean(y_actual)))\n",
        "    return R2\n",
        "\n",
        "def MAE(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    MAE = np.mean(abs(y_actual-y_predicted))\n",
        "    return MAE\n",
        "\n",
        "def RMSE(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    RMSE = np.sqrt(np.mean(np.square(y_actual-y_predicted)))\n",
        "    return RMSE\n",
        "\n",
        "def CVRMSE(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    RMSE = np.sqrt(np.mean(np.square(y_actual-y_predicted)))\n",
        "    CVRMSE = RMSE/np.mean(y_actual)\n",
        "    return CVRMSE\n",
        "\n",
        "def MAPE(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    MAPE=np.mean(abs((y_actual-y_predicted)/y_actual))\n",
        "    return MAPE\n",
        "\n",
        "def MSE(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    MSE = np.mean(abs(y_actual-y_predicted)**2)\n",
        "    return MSE\n",
        "\n",
        "def kl_divergence(p, q):\n",
        "\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    q = np.asarray(q, dtype=float)\n",
        "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
        "\n",
        "def kl_divergence(y_predicted, y_actual):\n",
        "\n",
        "    stacked_values = np.hstack((y_predicted, y_actual))\n",
        "    stacked_low = np.percentile(stacked_values, 0.1)\n",
        "    stacked_high = np.percentile(stacked_values, 99.9)\n",
        "    bins = np.linspace(stacked_low, stacked_high, 100)\n",
        "\n",
        "    distr = np.histogram(y_predicted, bins=bins)[0]\n",
        "    distr = np.clip(distr / distr.sum(), 0.001, 0.999)\n",
        "    true_distr = np.histogram( y_actual, bins=bins)[0]\n",
        "    true_distr = np.clip(true_distr / true_distr.sum(), 0.001, 0.999)\n",
        "\n",
        "    kl = entropy(distr, true_distr)\n",
        "    return kl "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "H3emQp0YFkN3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.data import InMemoryDataset, download_url\n",
        "\n",
        "\n",
        "class Mytrain(InMemoryDataset):\n",
        "    def __init__(self, root,transform=None, pre_transform=None):\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "       # ,self.edge_index,self.y=x,edge_index,y\n",
        "        \n",
        "    \n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return ['some_file_1', 'some_file_2', ...]\n",
        "    \n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "\n",
        "    def process(self):\n",
        "        data_list=[]\n",
        "        weighted_feats=[]\n",
        "        for i in range(x_train.shape[0]):\n",
        "            Edge_index = edge_index.type(torch.long)\n",
        "            X =x_train[i]\n",
        "            if feats_mode =='causal':\n",
        "                t=torch.zeros(14,25) # 25 dimensions = 1+10+14 = node number, node embedding, features\n",
        "              \n",
        "                causal_weighted= np.multiply(np.array(X),np.array(ate_list))\n",
        "                weighted_feats.append(causal_weighted)\n",
        "                \n",
        "                for j in range(14): # for j-th feature\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        list(causal_weighted) # feature (10d), causally weighted features        \n",
        "                                                        ))) \n",
        "            elif feats_mode =='equal':\n",
        "                t=torch.zeros(14,25) # 25 dimensions = 1+10+14 = node number, node embedding, features\n",
        "              \n",
        "                weighted= np.array(X)\n",
        "                weighted_feats.append(weighted)\n",
        "                \n",
        "                for j in range(14): # for j-th feature\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        list(weighted) # feature (10d), equally weighted features        \n",
        "                                                        ))) \n",
        "            else:\n",
        "                t=torch.zeros(14,11) # 11 dimensions = 1+10 = node number, node embedding\n",
        "                                    \n",
        "                \n",
        "                for j in range(14):\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        ))) \n",
        "                  \n",
        "\n",
        "            Y = y_train[i].reshape(-1,1).to(torch.float32)\n",
        "            data = Data(x=t, edge_index=Edge_index, y=Y)\n",
        "      \n",
        "            data_list.append(data)\n",
        "\n",
        "        if self.pre_filter is not None:\n",
        "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data_list = [self.pre_transform(data) for data in data_list]\n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "\n",
        "class Mytest(InMemoryDataset):\n",
        "    def __init__(self, root,transform=None, pre_transform=None):\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "       # ,self.edge_index,self.y=x,edge_index,y\n",
        "        \n",
        "    \n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return ['some_file_1', 'some_file_2', ...]\n",
        "    \n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "\n",
        "    def process(self):\n",
        "        data_list=[]\n",
        "        weighted_feats=[]\n",
        "        for i in range(x_test.shape[0]):\n",
        "            Edge_index = edge_index.type(torch.long)\n",
        "            X =x_test[i]\n",
        "\n",
        "            if feats_mode =='causal':\n",
        "                t=torch.zeros(14,25) # 25 dimensions = 1+10+14 = node number, node embedding, features\n",
        "              \n",
        "                causal_weighted= np.multiply(np.array(X),np.array(ate_list))\n",
        "                weighted_feats.append(causal_weighted)\n",
        "                \n",
        "                for j in range(14): # for j-th feature\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        list(causal_weighted) # feature (10d), causally weighted features        \n",
        "                                                        ))) \n",
        "            elif feats_mode =='equal':\n",
        "                t=torch.zeros(14,25) # 25 dimensions = 1+10+14 = node number, node embedding, features\n",
        "              \n",
        "                weighted= np.array(X)\n",
        "                weighted_feats.append(weighted)\n",
        "                \n",
        "                for j in range(14): # for j-th feature\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        list(weighted) # feature (10d), equally weighted features        \n",
        "                                                        ))) \n",
        "            else:\n",
        "                t=torch.zeros(14,11) # 11 dimensions = 1+10 = node number, node embedding\n",
        "                                    \n",
        "                \n",
        "                for j in range(14):\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        ))) \n",
        "            \n",
        "            Y = y_test[i].reshape(-1,1).to(torch.float32)\n",
        "            data = Data(x=t, edge_index=Edge_index, y=Y)\n",
        "        \n",
        "            data_list.append(data)\n",
        "\n",
        "        if self.pre_filter is not None:\n",
        "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data_list = [self.pre_transform(data) for data in data_list]\n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp0L9nr_FkN5",
        "outputId": "f91b4852-8263-4dab-da20-775132fe9b64"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n",
            "Done!\n",
            "Processing...\n",
            "Done!\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "feats_mode='causal'\n",
        "#feats_mode ='equal' \n",
        "#feats_mode ='noweighting' \n",
        "\n",
        "col=df_train.columns\n",
        "y_train=torch.from_numpy(np.array(df_train['y'])).reshape(df_train.shape[0],1).to(torch.float32)\n",
        "x_train=torch.from_numpy(np.array(df_train[[i for i in col[:-4]]+[\"visit\",\"T\"]])).to(torch.float32)\n",
        "Mydata_train=Mytrain(\".\\mydata_sampled_ate\\MYdata_train\")\n",
        "\n",
        "y_test=torch.from_numpy(np.array(df_test['y'])).reshape(df_test.shape[0],1).to(torch.float32)\n",
        "x_test=torch.from_numpy(np.array(df_test[[i for i in col[:-4]]+[\"visit\",\"T\"]])).to(torch.float32)\n",
        "Mydata_test=Mytest(\".\\mydata_sampled_ate\\MYdata_test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dDvnHBgFkN6"
      },
      "source": [
        "# GCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKmQvnebFkN7"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "    y_actual = []\n",
        "    y_predicted = []\n",
        "    loss_all=0\n",
        "    for data in train_loader:\n",
        "        loss = 0\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        label = data.y.to(device)\n",
        "        loss = crit(output, label)\n",
        "        loss.backward()\n",
        "        loss_all += data.num_graphs * loss.item()\n",
        "        optimizer.step()\n",
        "        y_actual +=(label).cpu().detach().ravel().tolist()\n",
        "        y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "    \n",
        "    \n",
        "    loss=loss_all/len(Mydata_train)\n",
        "    r2=R2(y_predicted, y_actual)\n",
        "    mse = MSE(y_predicted, y_actual)\n",
        "    kl=kl_divergence(y_predicted, y_actual)\n",
        "\n",
        "    print(\"R2:%f\" % (R2(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MSE:%f\" % (MSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"KL:%f\" % (kl_divergence(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MAE:%f\" % (MAE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"RMSE:%f\" % (RMSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"CVRMSE:%f\" % (CVRMSE(y_predicted, y_actual)),end='  ')\n",
        "\n",
        "    return loss,r2,mse,kl\n",
        "\n",
        "\n",
        "def val():\n",
        "    model.eval()\n",
        "    y_actual = []\n",
        "    y_predicted = []\n",
        "    loss_all=0\n",
        "    for data in test_loader:\n",
        "      loss = 0\n",
        "      data = data.to(device)\n",
        "      output = model(data)\n",
        "      label = data.y.to(device)\n",
        "      y_actual +=(label).cpu().detach().ravel().tolist()\n",
        "      y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "      loss = crit(output, label)\n",
        "      loss_all += loss.item()\n",
        "\n",
        "    \n",
        "    loss = loss_all / len(Mydata_test)\n",
        "    r2=R2(y_predicted, y_actual)\n",
        "    mse = MSE(y_predicted, y_actual)\n",
        "    kl=kl_divergence(y_predicted, y_actual)\n",
        "\n",
        "    print(\"R2:%f\" % (R2(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MSE:%f\" % (MSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"KL:%f\" % (kl_divergence(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MAE:%f\" % (MAE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"RMSE:%f\" % (RMSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"CVRMSE:%f\" % (CVRMSE(y_predicted, y_actual)),end='  ')\n",
        "\n",
        "    return loss,r2, mse,kl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8VbbpTvFkN8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_scatter import scatter_add\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = GCNConv(25, 64)\n",
        "        self.conv2 = GCNConv(64, 10)\n",
        "        # self-attention layer\n",
        "        \n",
        "        self.f1 = torch.nn.Linear(140,32)\n",
        "        self.f2 = torch.nn.Linear(32,1)\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.attention\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = x.reshape(-1,140)\n",
        "        x = self.f1(x)\n",
        "        x = self.f2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvzm6UHUFkN8",
        "outputId": "4801aa26-a485-40c6-c0d9-b5b0df9963b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:-0.024036  MSE:0.002990  KL:1.247097  MAE:0.009826  RMSE:0.054681  CVRMSE:18.672716  train_loss:\n",
            "0.002989981582357814\n",
            "R2:0.072559  MSE:0.002796  KL:0.121532  MAE:0.006606  RMSE:0.052879  CVRMSE:17.486027  test_loss:\n",
            "5.494521631923732e-06\n",
            "R2:0.087444  MSE:0.002664  KL:2.115198  MAE:0.007876  RMSE:0.051619  CVRMSE:17.627042  R2:0.100940  MSE:0.002625  KL:1.866433  MAE:0.007405  RMSE:0.051235  CVRMSE:17.496212  R2:0.103677  MSE:0.002617  KL:1.270331  MAE:0.007245  RMSE:0.051157  CVRMSE:17.469562  R2:0.104247  MSE:0.002615  KL:1.318326  MAE:0.007226  RMSE:0.051141  CVRMSE:17.464006  R2:0.103408  MSE:0.002618  KL:1.308779  MAE:0.007226  RMSE:0.051165  CVRMSE:17.472183  train_loss:\n",
            "0.0026178686612746737\n",
            "R2:0.118098  MSE:0.002659  KL:0.053999  MAE:0.005711  RMSE:0.051564  CVRMSE:17.051327  test_loss:\n",
            "5.2227962901602e-06\n",
            "R2:0.106504  MSE:0.002609  KL:1.383942  MAE:0.007235  RMSE:0.051077  CVRMSE:17.441997  R2:0.105358  MSE:0.002612  KL:1.017340  MAE:0.007164  RMSE:0.051109  CVRMSE:17.453179  R2:0.105063  MSE:0.002613  KL:0.969366  MAE:0.007183  RMSE:0.051118  CVRMSE:17.456057  R2:0.105053  MSE:0.002613  KL:0.949471  MAE:0.007192  RMSE:0.051118  CVRMSE:17.456153  R2:0.104307  MSE:0.002615  KL:0.863016  MAE:0.007199  RMSE:0.051139  CVRMSE:17.463422  train_loss:\n",
            "0.0026152442046849687\n",
            "R2:0.117509  MSE:0.002661  KL:0.054740  MAE:0.005315  RMSE:0.051581  CVRMSE:17.057016  test_loss:\n",
            "5.226357080701829e-06\n",
            "R2:0.106275  MSE:0.002609  KL:1.228233  MAE:0.007159  RMSE:0.051083  CVRMSE:17.444228  R2:0.103919  MSE:0.002616  KL:0.637054  MAE:0.007138  RMSE:0.051151  CVRMSE:17.467208  R2:0.105311  MSE:0.002612  KL:0.588532  MAE:0.007144  RMSE:0.051111  CVRMSE:17.453630  R2:0.107098  MSE:0.002607  KL:0.660836  MAE:0.007117  RMSE:0.051060  CVRMSE:17.436191  R2:0.106052  MSE:0.002610  KL:0.819293  MAE:0.007112  RMSE:0.051090  CVRMSE:17.446405  train_loss:\n",
            "0.002610149880220272\n",
            "R2:0.115166  MSE:0.002668  KL:0.055360  MAE:0.005367  RMSE:0.051650  CVRMSE:17.079640  test_loss:\n",
            "5.240413291924206e-06\n",
            "R2:0.107532  MSE:0.002606  KL:0.666219  MAE:0.007105  RMSE:0.051047  CVRMSE:17.431954  R2:0.105487  MSE:0.002612  KL:0.954677  MAE:0.007067  RMSE:0.051106  CVRMSE:17.451914  R2:0.105561  MSE:0.002612  KL:0.738524  MAE:0.007109  RMSE:0.051104  CVRMSE:17.451199  R2:0.106656  MSE:0.002608  KL:0.745225  MAE:0.007126  RMSE:0.051072  CVRMSE:17.440505  R2:0.104681  MSE:0.002614  KL:0.950583  MAE:0.007149  RMSE:0.051129  CVRMSE:17.459780  train_loss:\n",
            "0.002614153423418712\n",
            "R2:0.118154  MSE:0.002659  KL:0.053490  MAE:0.005444  RMSE:0.051562  CVRMSE:17.050784  test_loss:\n",
            "5.222450172184827e-06\n",
            "R2:0.106647  MSE:0.002608  KL:0.705550  MAE:0.007123  RMSE:0.051073  CVRMSE:17.440599  R2:0.108868  MSE:0.002602  KL:0.594718  MAE:0.007109  RMSE:0.051009  CVRMSE:17.418907  R2:0.106806  MSE:0.002608  KL:0.610171  MAE:0.007115  RMSE:0.051068  CVRMSE:17.439044  R2:0.105819  MSE:0.002611  KL:0.706310  MAE:0.007102  RMSE:0.051096  CVRMSE:17.448681  R2:0.107110  MSE:0.002607  KL:0.885996  MAE:0.007107  RMSE:0.051059  CVRMSE:17.436078  train_loss:\n",
            "0.002607060741288338\n",
            "R2:0.116259  MSE:0.002664  KL:0.052473  MAE:0.005006  RMSE:0.051618  CVRMSE:17.069090  test_loss:\n",
            "5.233913777963869e-06\n",
            "R2:0.105598  MSE:0.002611  KL:0.722003  MAE:0.007130  RMSE:0.051103  CVRMSE:17.450829  R2:0.106765  MSE:0.002608  KL:0.719740  MAE:0.007081  RMSE:0.051069  CVRMSE:17.439448  R2:0.105062  MSE:0.002613  KL:0.691810  MAE:0.007120  RMSE:0.051118  CVRMSE:17.456058  R2:0.106499  MSE:0.002609  KL:0.661909  MAE:0.007069  RMSE:0.051077  CVRMSE:17.442043  R2:0.105410  MSE:0.002612  KL:0.757444  MAE:0.007109  RMSE:0.051108  CVRMSE:17.452664  train_loss:\n",
            "0.002612022911837946\n",
            "R2:0.117695  MSE:0.002660  KL:0.052092  MAE:0.005598  RMSE:0.051576  CVRMSE:17.055218  test_loss:\n",
            "5.225351272823325e-06\n",
            "R2:0.107357  MSE:0.002606  KL:0.985376  MAE:0.007124  RMSE:0.051052  CVRMSE:17.433669  R2:0.107834  MSE:0.002605  KL:0.535522  MAE:0.007076  RMSE:0.051039  CVRMSE:17.429007  R2:0.106496  MSE:0.002609  KL:0.901962  MAE:0.007109  RMSE:0.051077  CVRMSE:17.442069  R2:0.105165  MSE:0.002613  KL:0.632436  MAE:0.007142  RMSE:0.051115  CVRMSE:17.455053  R2:0.104908  MSE:0.002613  KL:1.279966  MAE:0.007125  RMSE:0.051122  CVRMSE:17.457563  train_loss:\n",
            "0.0026134894200734457\n",
            "R2:0.110685  MSE:0.002681  KL:0.052734  MAE:0.004776  RMSE:0.051780  CVRMSE:17.122833  test_loss:\n",
            "5.267270523906558e-06\n",
            "R2:0.103286  MSE:0.002618  KL:0.899157  MAE:0.007166  RMSE:0.051169  CVRMSE:17.473373  R2:0.106050  MSE:0.002610  KL:0.491584  MAE:0.007097  RMSE:0.051090  CVRMSE:17.446419  R2:0.105711  MSE:0.002611  KL:0.627320  MAE:0.007124  RMSE:0.051099  CVRMSE:17.449730  R2:0.107484  MSE:0.002606  KL:0.729548  MAE:0.007100  RMSE:0.051049  CVRMSE:17.432426  R2:0.108523  MSE:0.002603  KL:0.867919  MAE:0.007122  RMSE:0.051019  CVRMSE:17.422273  train_loss:\n",
            "0.0026029339698018764\n",
            "R2:0.119140  MSE:0.002656  KL:0.054463  MAE:0.005314  RMSE:0.051534  CVRMSE:17.041248  test_loss:\n",
            "5.216587512391313e-06\n",
            "R2:0.104691  MSE:0.002614  KL:0.707341  MAE:0.007075  RMSE:0.051128  CVRMSE:17.459678  R2:0.107102  MSE:0.002607  KL:0.643275  MAE:0.007110  RMSE:0.051060  CVRMSE:17.436151  R2:0.107706  MSE:0.002605  KL:0.850607  MAE:0.007102  RMSE:0.051042  CVRMSE:17.430255  R2:0.106683  MSE:0.002608  KL:1.325800  MAE:0.007126  RMSE:0.051072  CVRMSE:17.440249  R2:0.106025  MSE:0.002610  KL:0.872132  MAE:0.007082  RMSE:0.051090  CVRMSE:17.446667  train_loss:\n",
            "0.0026102283103123596\n",
            "R2:0.120127  MSE:0.002653  KL:0.049625  MAE:0.005070  RMSE:0.051505  CVRMSE:17.031695  test_loss:\n",
            "5.210797328238225e-06\n",
            "R2:0.105858  MSE:0.002611  KL:0.824477  MAE:0.007106  RMSE:0.051095  CVRMSE:17.448296  R2:0.105860  MSE:0.002611  KL:0.821959  MAE:0.007122  RMSE:0.051095  CVRMSE:17.448273  R2:0.106372  MSE:0.002609  KL:1.252348  MAE:0.007135  RMSE:0.051080  CVRMSE:17.443284  R2:0.105381  MSE:0.002612  KL:1.064849  MAE:0.007096  RMSE:0.051109  CVRMSE:17.452949  R2:0.105573  MSE:0.002612  KL:0.738389  MAE:0.007121  RMSE:0.051103  CVRMSE:17.451082  train_loss:\n",
            "0.0026115494704313826\n",
            "R2:0.116988  MSE:0.002662  KL:0.054287  MAE:0.005915  RMSE:0.051596  CVRMSE:17.062055  test_loss:\n",
            "5.229460007918806e-06\n",
            "R2:0.104756  MSE:0.002614  KL:0.697336  MAE:0.007134  RMSE:0.051127  CVRMSE:17.459041  R2:0.105531  MSE:0.002612  KL:0.541627  MAE:0.007113  RMSE:0.051105  CVRMSE:17.451486  R2:0.105219  MSE:0.002613  KL:0.868070  MAE:0.007112  RMSE:0.051113  CVRMSE:17.454534  R2:0.107005  MSE:0.002607  KL:1.043744  MAE:0.007131  RMSE:0.051062  CVRMSE:17.437100  R2:0.106460  MSE:0.002609  KL:0.565921  MAE:0.007096  RMSE:0.051078  CVRMSE:17.442420  train_loss:\n",
            "0.002608957690045045\n",
            "R2:0.121224  MSE:0.002649  KL:0.052221  MAE:0.005536  RMSE:0.051473  CVRMSE:17.021079  test_loss:\n",
            "5.204137649688485e-06\n",
            "R2:0.106048  MSE:0.002610  KL:0.644381  MAE:0.007133  RMSE:0.051090  CVRMSE:17.446438  R2:0.105448  MSE:0.002612  KL:0.818505  MAE:0.007119  RMSE:0.051107  CVRMSE:17.452292  R2:0.107089  MSE:0.002607  KL:0.793730  MAE:0.007116  RMSE:0.051060  CVRMSE:17.436278  R2:0.106670  MSE:0.002608  KL:0.723367  MAE:0.007127  RMSE:0.051072  CVRMSE:17.440373  R2:0.106887  MSE:0.002608  KL:0.634621  MAE:0.007089  RMSE:0.051066  CVRMSE:17.438252  train_loss:\n",
            "0.0026077109692225955\n",
            "R2:0.116634  MSE:0.002663  KL:0.054771  MAE:0.005109  RMSE:0.051607  CVRMSE:17.065467  test_loss:\n",
            "5.231627641455757e-06\n",
            "R2:0.107194  MSE:0.002607  KL:0.603530  MAE:0.007151  RMSE:0.051057  CVRMSE:17.435257  R2:0.107600  MSE:0.002606  KL:0.633773  MAE:0.007118  RMSE:0.051045  CVRMSE:17.431291  R2:0.105464  MSE:0.002612  KL:0.822473  MAE:0.007095  RMSE:0.051106  CVRMSE:17.452140  R2:0.105154  MSE:0.002613  KL:0.698737  MAE:0.007127  RMSE:0.051115  CVRMSE:17.455166  R2:0.104544  MSE:0.002615  KL:0.728937  MAE:0.007135  RMSE:0.051133  CVRMSE:17.461116  train_loss:\n",
            "0.002614553358323284\n",
            "R2:0.116692  MSE:0.002663  KL:0.051965  MAE:0.005156  RMSE:0.051605  CVRMSE:17.064905  test_loss:\n",
            "5.231329218421923e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.104883  MSE:0.002614  KL:0.500720  MAE:0.007134  RMSE:0.051123  CVRMSE:17.457811  R2:0.105849  MSE:0.002611  KL:0.589760  MAE:0.007085  RMSE:0.051095  CVRMSE:17.448388  R2:0.105549  MSE:0.002612  KL:0.829491  MAE:0.007133  RMSE:0.051104  CVRMSE:17.451313  R2:0.106632  MSE:0.002608  KL:0.835054  MAE:0.007149  RMSE:0.051073  CVRMSE:17.440747  R2:0.105095  MSE:0.002613  KL:0.666143  MAE:0.007129  RMSE:0.051117  CVRMSE:17.455743  train_loss:\n",
            "0.0026129447496954484\n",
            "R2:0.118220  MSE:0.002658  KL:0.056525  MAE:0.005706  RMSE:0.051560  CVRMSE:17.050140  test_loss:\n",
            "5.221919713687387e-06\n",
            "R2:0.105654  MSE:0.002611  KL:0.826030  MAE:0.007149  RMSE:0.051101  CVRMSE:17.450283  R2:0.104463  MSE:0.002615  KL:0.742459  MAE:0.007128  RMSE:0.051135  CVRMSE:17.461899  R2:0.106275  MSE:0.002609  KL:0.950716  MAE:0.007139  RMSE:0.051083  CVRMSE:17.444227  R2:0.109134  MSE:0.002601  KL:0.631706  MAE:0.007132  RMSE:0.051001  CVRMSE:17.416301  R2:0.107887  MSE:0.002605  KL:0.613254  MAE:0.007109  RMSE:0.051037  CVRMSE:17.428489  train_loss:\n",
            "0.0026047915921787604\n",
            "R2:0.119056  MSE:0.002656  KL:0.052123  MAE:0.005363  RMSE:0.051536  CVRMSE:17.042058  test_loss:\n",
            "5.217088077694498e-06\n",
            "R2:0.105677  MSE:0.002611  KL:0.850734  MAE:0.007132  RMSE:0.051100  CVRMSE:17.450060  R2:0.106785  MSE:0.002608  KL:0.901331  MAE:0.007140  RMSE:0.051069  CVRMSE:17.439247  R2:0.106069  MSE:0.002610  KL:0.835638  MAE:0.007160  RMSE:0.051089  CVRMSE:17.446238  R2:0.108008  MSE:0.002604  KL:0.718198  MAE:0.007112  RMSE:0.051034  CVRMSE:17.427302  R2:0.105508  MSE:0.002612  KL:0.750716  MAE:0.007097  RMSE:0.051105  CVRMSE:17.451707  train_loss:\n",
            "0.0026117366329458606\n",
            "R2:0.121077  MSE:0.002650  KL:0.050786  MAE:0.005596  RMSE:0.051477  CVRMSE:17.022501  test_loss:\n",
            "5.20504073249685e-06\n",
            "R2:0.106853  MSE:0.002608  KL:0.882962  MAE:0.007110  RMSE:0.051067  CVRMSE:17.438582  R2:0.105964  MSE:0.002610  KL:0.726564  MAE:0.007128  RMSE:0.051092  CVRMSE:17.447264  R2:0.107513  MSE:0.002606  KL:0.771164  MAE:0.007152  RMSE:0.051048  CVRMSE:17.432138  R2:0.106365  MSE:0.002609  KL:0.644175  MAE:0.007129  RMSE:0.051081  CVRMSE:17.443347  R2:0.108036  MSE:0.002604  KL:0.849082  MAE:0.007159  RMSE:0.051033  CVRMSE:17.427037  train_loss:\n",
            "0.002604357761251016\n",
            "R2:0.122185  MSE:0.002647  KL:0.050799  MAE:0.005351  RMSE:0.051444  CVRMSE:17.011768  test_loss:\n",
            "5.1984150760271535e-06\n",
            "R2:0.106175  MSE:0.002610  KL:0.631008  MAE:0.007079  RMSE:0.051086  CVRMSE:17.445199  R2:0.106862  MSE:0.002608  KL:0.992281  MAE:0.007112  RMSE:0.051066  CVRMSE:17.438497  R2:0.106954  MSE:0.002608  KL:0.792490  MAE:0.007090  RMSE:0.051064  CVRMSE:17.437600  R2:0.107467  MSE:0.002606  KL:0.509982  MAE:0.007092  RMSE:0.051049  CVRMSE:17.432588  R2:0.106427  MSE:0.002609  KL:0.833514  MAE:0.007107  RMSE:0.051079  CVRMSE:17.442748  train_loss:\n",
            "0.002609055587782925\n",
            "R2:0.114945  MSE:0.002668  KL:0.052022  MAE:0.004960  RMSE:0.051656  CVRMSE:17.081776  test_loss:\n",
            "5.241812182246964e-06\n",
            "R2:0.106405  MSE:0.002609  KL:0.773914  MAE:0.007127  RMSE:0.051080  CVRMSE:17.442963  R2:0.106128  MSE:0.002610  KL:0.675069  MAE:0.007144  RMSE:0.051087  CVRMSE:17.445659  R2:0.107572  MSE:0.002606  KL:0.732632  MAE:0.007091  RMSE:0.051046  CVRMSE:17.431561  R2:0.107823  MSE:0.002605  KL:0.868372  MAE:0.007104  RMSE:0.051039  CVRMSE:17.429116  R2:0.107108  MSE:0.002607  KL:0.799952  MAE:0.007113  RMSE:0.051059  CVRMSE:17.436094  train_loss:\n",
            "0.0026070655483540536\n",
            "R2:0.116451  MSE:0.002664  KL:0.055529  MAE:0.005517  RMSE:0.051612  CVRMSE:17.067237  test_loss:\n",
            "5.232624728247104e-06\n",
            "R2:0.108160  MSE:0.002604  KL:0.765643  MAE:0.007094  RMSE:0.051029  CVRMSE:17.425819  R2:0.105984  MSE:0.002610  KL:0.574133  MAE:0.007139  RMSE:0.051092  CVRMSE:17.447070  R2:0.104517  MSE:0.002615  KL:0.690885  MAE:0.007137  RMSE:0.051133  CVRMSE:17.461378  R2:0.106679  MSE:0.002608  KL:1.037483  MAE:0.007076  RMSE:0.051072  CVRMSE:17.440285  R2:0.106325  MSE:0.002609  KL:1.053703  MAE:0.007128  RMSE:0.051082  CVRMSE:17.443738  train_loss:\n",
            "0.0026093517657144573\n",
            "R2:0.118077  MSE:0.002659  KL:0.049334  MAE:0.004920  RMSE:0.051565  CVRMSE:17.051522  test_loss:\n",
            "5.223157035203367e-06\n",
            "R2:0.105530  MSE:0.002612  KL:0.825985  MAE:0.007149  RMSE:0.051105  CVRMSE:17.451500  R2:0.107045  MSE:0.002607  KL:0.883140  MAE:0.007124  RMSE:0.051061  CVRMSE:17.436711  R2:0.106247  MSE:0.002610  KL:0.730935  MAE:0.007093  RMSE:0.051084  CVRMSE:17.444504  R2:0.105329  MSE:0.002612  KL:0.696332  MAE:0.007077  RMSE:0.051110  CVRMSE:17.453453  R2:0.105219  MSE:0.002613  KL:0.587314  MAE:0.007150  RMSE:0.051113  CVRMSE:17.454532  train_loss:\n",
            "0.0026125822383552028\n",
            "R2:0.119202  MSE:0.002656  KL:0.055383  MAE:0.005912  RMSE:0.051532  CVRMSE:17.040647  test_loss:\n",
            "5.216066611226133e-06\n",
            "R2:0.105526  MSE:0.002612  KL:0.615238  MAE:0.007108  RMSE:0.051105  CVRMSE:17.451540  R2:0.108343  MSE:0.002603  KL:0.671393  MAE:0.007103  RMSE:0.051024  CVRMSE:17.424032  R2:0.106769  MSE:0.002608  KL:0.719625  MAE:0.007140  RMSE:0.051069  CVRMSE:17.439410  R2:0.108219  MSE:0.002604  KL:0.806350  MAE:0.007088  RMSE:0.051028  CVRMSE:17.425250  R2:0.105992  MSE:0.002610  KL:0.726007  MAE:0.007103  RMSE:0.051091  CVRMSE:17.446992  train_loss:\n",
            "0.002610325501545792\n",
            "R2:0.116156  MSE:0.002665  KL:0.053132  MAE:0.005288  RMSE:0.051621  CVRMSE:17.070086  test_loss:\n",
            "5.2345279204548305e-06\n",
            "R2:0.104678  MSE:0.002614  KL:0.842293  MAE:0.007123  RMSE:0.051129  CVRMSE:17.459803  R2:0.105170  MSE:0.002613  KL:1.019765  MAE:0.007137  RMSE:0.051115  CVRMSE:17.455009  R2:0.107454  MSE:0.002606  KL:0.654783  MAE:0.007109  RMSE:0.051050  CVRMSE:17.432716  R2:0.106164  MSE:0.002610  KL:0.716626  MAE:0.007108  RMSE:0.051086  CVRMSE:17.445315  R2:0.105083  MSE:0.002613  KL:0.784912  MAE:0.007093  RMSE:0.051117  CVRMSE:17.455860  train_loss:\n",
            "0.0026129797974826716\n",
            "R2:0.116102  MSE:0.002665  KL:0.053065  MAE:0.005040  RMSE:0.051622  CVRMSE:17.070607  test_loss:\n",
            "5.234830282168815e-06\n",
            "R2:0.104545  MSE:0.002615  KL:0.946615  MAE:0.007128  RMSE:0.051133  CVRMSE:17.461107  R2:0.107013  MSE:0.002607  KL:0.670600  MAE:0.007100  RMSE:0.051062  CVRMSE:17.437019  R2:0.105981  MSE:0.002610  KL:0.830541  MAE:0.007144  RMSE:0.051092  CVRMSE:17.447093  R2:0.106097  MSE:0.002610  KL:0.721074  MAE:0.007159  RMSE:0.051088  CVRMSE:17.445965  R2:0.106073  MSE:0.002610  KL:0.845915  MAE:0.007134  RMSE:0.051089  CVRMSE:17.446203  train_loss:\n",
            "0.0026100894669650316\n",
            "R2:0.117238  MSE:0.002661  KL:0.054022  MAE:0.005281  RMSE:0.051589  CVRMSE:17.059638  test_loss:\n",
            "5.22804052071852e-06\n",
            "R2:0.107405  MSE:0.002606  KL:0.745832  MAE:0.007083  RMSE:0.051051  CVRMSE:17.433193  R2:0.105741  MSE:0.002611  KL:0.852052  MAE:0.007107  RMSE:0.051099  CVRMSE:17.449442  R2:0.106192  MSE:0.002610  KL:0.767923  MAE:0.007105  RMSE:0.051086  CVRMSE:17.445040  R2:0.106108  MSE:0.002610  KL:0.872100  MAE:0.007131  RMSE:0.051088  CVRMSE:17.445855  R2:0.106573  MSE:0.002609  KL:0.664687  MAE:0.007103  RMSE:0.051075  CVRMSE:17.441317  train_loss:\n",
            "0.0026086276315706695\n",
            "R2:0.115842  MSE:0.002666  KL:0.054045  MAE:0.005442  RMSE:0.051630  CVRMSE:17.073118  test_loss:\n",
            "5.23641493518722e-06\n",
            "R2:0.104540  MSE:0.002615  KL:0.732588  MAE:0.007121  RMSE:0.051133  CVRMSE:17.461148  R2:0.103556  MSE:0.002617  KL:0.851917  MAE:0.007137  RMSE:0.051161  CVRMSE:17.470739  R2:0.106072  MSE:0.002610  KL:0.602258  MAE:0.007112  RMSE:0.051089  CVRMSE:17.446208  R2:0.104640  MSE:0.002614  KL:0.891346  MAE:0.007168  RMSE:0.051130  CVRMSE:17.460175  R2:0.106423  MSE:0.002609  KL:0.837677  MAE:0.007157  RMSE:0.051079  CVRMSE:17.442781  train_loss:\n",
            "0.002609065589350698\n",
            "R2:0.119074  MSE:0.002656  KL:0.054812  MAE:0.005260  RMSE:0.051535  CVRMSE:17.041883  test_loss:\n",
            "5.216950892360911e-06\n",
            "R2:0.108188  MSE:0.002604  KL:0.572219  MAE:0.007138  RMSE:0.051029  CVRMSE:17.425545  R2:0.105803  MSE:0.002611  KL:0.916927  MAE:0.007086  RMSE:0.051097  CVRMSE:17.448838  R2:0.106433  MSE:0.002609  KL:0.961690  MAE:0.007122  RMSE:0.051079  CVRMSE:17.442682  R2:0.107187  MSE:0.002607  KL:0.741810  MAE:0.007093  RMSE:0.051057  CVRMSE:17.435326  R2:0.106031  MSE:0.002610  KL:0.792469  MAE:0.007115  RMSE:0.051090  CVRMSE:17.446606  train_loss:\n",
            "0.0026102099501247467\n",
            "R2:0.114185  MSE:0.002671  KL:0.055425  MAE:0.005036  RMSE:0.051678  CVRMSE:17.089105  test_loss:\n",
            "5.246274602380618e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.106180  MSE:0.002610  KL:0.856136  MAE:0.007103  RMSE:0.051086  CVRMSE:17.445153  R2:0.107398  MSE:0.002606  KL:0.757668  MAE:0.007099  RMSE:0.051051  CVRMSE:17.433269  R2:0.106007  MSE:0.002610  KL:0.819236  MAE:0.007143  RMSE:0.051091  CVRMSE:17.446839  R2:0.104498  MSE:0.002615  KL:0.734635  MAE:0.007113  RMSE:0.051134  CVRMSE:17.461565  R2:0.104122  MSE:0.002616  KL:0.695199  MAE:0.007152  RMSE:0.051145  CVRMSE:17.465231  train_loss:\n",
            "0.002615786017936153\n",
            "R2:0.116946  MSE:0.002662  KL:0.054055  MAE:0.005199  RMSE:0.051598  CVRMSE:17.062456  test_loss:\n",
            "5.229684696113949e-06\n",
            "R2:0.104932  MSE:0.002613  KL:0.640522  MAE:0.007125  RMSE:0.051122  CVRMSE:17.457330  R2:0.104949  MSE:0.002613  KL:0.636439  MAE:0.007101  RMSE:0.051121  CVRMSE:17.457166  R2:0.104573  MSE:0.002614  KL:0.703129  MAE:0.007103  RMSE:0.051132  CVRMSE:17.460831  R2:0.106391  MSE:0.002609  KL:0.750826  MAE:0.007116  RMSE:0.051080  CVRMSE:17.443098  R2:0.106912  MSE:0.002608  KL:0.610168  MAE:0.007062  RMSE:0.051065  CVRMSE:17.438011  train_loss:\n",
            "0.002607638697241116\n",
            "R2:0.118030  MSE:0.002659  KL:0.056702  MAE:0.005661  RMSE:0.051566  CVRMSE:17.051984  test_loss:\n",
            "5.223027062235762e-06\n",
            "R2:0.104939  MSE:0.002613  KL:0.726894  MAE:0.007155  RMSE:0.051121  CVRMSE:17.457259  R2:0.106503  MSE:0.002609  KL:0.889413  MAE:0.007142  RMSE:0.051077  CVRMSE:17.442003  R2:0.106212  MSE:0.002610  KL:0.674261  MAE:0.007110  RMSE:0.051085  CVRMSE:17.444841  R2:0.106657  MSE:0.002608  KL:0.826961  MAE:0.007077  RMSE:0.051072  CVRMSE:17.440502  R2:0.106959  MSE:0.002608  KL:0.767749  MAE:0.007078  RMSE:0.051064  CVRMSE:17.437549  train_loss:\n",
            "0.0026075005088159616\n",
            "R2:0.119220  MSE:0.002655  KL:0.053887  MAE:0.005650  RMSE:0.051531  CVRMSE:17.040473  test_loss:\n",
            "5.2160522006314285e-06\n",
            "R2:0.105393  MSE:0.002612  KL:0.807011  MAE:0.007149  RMSE:0.051108  CVRMSE:17.452835  R2:0.106143  MSE:0.002610  KL:0.580049  MAE:0.007121  RMSE:0.051087  CVRMSE:17.445513  R2:0.105328  MSE:0.002612  KL:0.575252  MAE:0.007093  RMSE:0.051110  CVRMSE:17.453468  R2:0.104668  MSE:0.002614  KL:0.802837  MAE:0.007105  RMSE:0.051129  CVRMSE:17.459903  R2:0.105448  MSE:0.002612  KL:0.637048  MAE:0.007102  RMSE:0.051107  CVRMSE:17.452298  train_loss:\n",
            "0.0026119132486908973\n",
            "R2:0.120287  MSE:0.002652  KL:0.053470  MAE:0.005682  RMSE:0.051500  CVRMSE:17.030151  test_loss:\n",
            "5.209567049701377e-06\n",
            "R2:0.105549  MSE:0.002612  KL:0.917342  MAE:0.007145  RMSE:0.051104  CVRMSE:17.451316  R2:0.105511  MSE:0.002612  KL:0.741200  MAE:0.007114  RMSE:0.051105  CVRMSE:17.451679  R2:0.105460  MSE:0.002612  KL:0.722455  MAE:0.007092  RMSE:0.051107  CVRMSE:17.452176  R2:0.104496  MSE:0.002615  KL:0.772782  MAE:0.007081  RMSE:0.051134  CVRMSE:17.461583  R2:0.106465  MSE:0.002609  KL:0.652938  MAE:0.007119  RMSE:0.051078  CVRMSE:17.442370  train_loss:\n",
            "0.0026089426717952626\n",
            "R2:0.121032  MSE:0.002650  KL:0.050931  MAE:0.005446  RMSE:0.051478  CVRMSE:17.022934  test_loss:\n",
            "5.205308752022436e-06\n",
            "R2:0.105311  MSE:0.002612  KL:1.250018  MAE:0.007108  RMSE:0.051111  CVRMSE:17.453634  R2:0.104203  MSE:0.002616  KL:0.815319  MAE:0.007118  RMSE:0.051142  CVRMSE:17.464440  R2:0.106032  MSE:0.002610  KL:0.891330  MAE:0.007095  RMSE:0.051090  CVRMSE:17.446603  R2:0.104451  MSE:0.002615  KL:0.819830  MAE:0.007139  RMSE:0.051135  CVRMSE:17.462024  R2:0.104736  MSE:0.002614  KL:0.554266  MAE:0.007097  RMSE:0.051127  CVRMSE:17.459241  train_loss:\n",
            "0.0026139921324910584\n",
            "R2:0.118260  MSE:0.002658  KL:0.052649  MAE:0.005165  RMSE:0.051559  CVRMSE:17.049758  test_loss:\n",
            "5.221859620846342e-06\n",
            "R2:0.105298  MSE:0.002612  KL:0.534224  MAE:0.007082  RMSE:0.051111  CVRMSE:17.453759  R2:0.104834  MSE:0.002614  KL:0.927264  MAE:0.007085  RMSE:0.051124  CVRMSE:17.458288  R2:0.105505  MSE:0.002612  KL:0.680067  MAE:0.007060  RMSE:0.051105  CVRMSE:17.451742  R2:0.106745  MSE:0.002608  KL:0.690286  MAE:0.007064  RMSE:0.051070  CVRMSE:17.439637  R2:0.105863  MSE:0.002611  KL:0.747510  MAE:0.007106  RMSE:0.051095  CVRMSE:17.448249  train_loss:\n",
            "0.002610701452929502\n",
            "R2:0.120268  MSE:0.002652  KL:0.053285  MAE:0.005447  RMSE:0.051501  CVRMSE:17.030328  test_loss:\n",
            "5.2097746066052365e-06\n",
            "R2:0.103686  MSE:0.002617  KL:0.642276  MAE:0.007092  RMSE:0.051157  CVRMSE:17.469476  R2:0.106211  MSE:0.002610  KL:0.752939  MAE:0.007111  RMSE:0.051085  CVRMSE:17.444852  R2:0.105083  MSE:0.002613  KL:0.716960  MAE:0.007130  RMSE:0.051117  CVRMSE:17.455856  R2:0.105282  MSE:0.002612  KL:0.692644  MAE:0.007054  RMSE:0.051112  CVRMSE:17.453916  R2:0.106984  MSE:0.002607  KL:0.944437  MAE:0.007092  RMSE:0.051063  CVRMSE:17.437308  train_loss:\n",
            "0.002607428634813078\n",
            "R2:0.115471  MSE:0.002667  KL:0.053276  MAE:0.005002  RMSE:0.051641  CVRMSE:17.076699  test_loss:\n",
            "5.238625371280212e-06\n",
            "R2:0.105305  MSE:0.002612  KL:0.833132  MAE:0.007083  RMSE:0.051111  CVRMSE:17.453688  R2:0.105275  MSE:0.002612  KL:0.633193  MAE:0.007112  RMSE:0.051112  CVRMSE:17.453986  R2:0.106382  MSE:0.002609  KL:0.725526  MAE:0.007090  RMSE:0.051080  CVRMSE:17.443186  R2:0.107203  MSE:0.002607  KL:0.931680  MAE:0.007038  RMSE:0.051057  CVRMSE:17.435166  R2:0.105018  MSE:0.002613  KL:0.644406  MAE:0.007079  RMSE:0.051119  CVRMSE:17.456495  train_loss:\n",
            "0.0026131697917722552\n",
            "R2:0.117056  MSE:0.002662  KL:0.052239  MAE:0.005053  RMSE:0.051594  CVRMSE:17.061389  test_loss:\n",
            "5.2291497110528685e-06\n",
            "R2:0.106184  MSE:0.002610  KL:0.678346  MAE:0.007105  RMSE:0.051086  CVRMSE:17.445111  R2:0.105203  MSE:0.002613  KL:0.660326  MAE:0.007073  RMSE:0.051114  CVRMSE:17.454690  R2:0.106299  MSE:0.002609  KL:0.632967  MAE:0.007095  RMSE:0.051083  CVRMSE:17.443991  R2:0.104627  MSE:0.002614  KL:0.983856  MAE:0.007088  RMSE:0.051130  CVRMSE:17.460300  R2:0.104570  MSE:0.002614  KL:0.854828  MAE:0.007116  RMSE:0.051132  CVRMSE:17.460862  train_loss:\n",
            "0.002614477443299938\n",
            "R2:0.120533  MSE:0.002652  KL:0.052740  MAE:0.005560  RMSE:0.051493  CVRMSE:17.027770  test_loss:\n",
            "5.208197027280179e-06\n",
            "R2:0.106453  MSE:0.002609  KL:0.759171  MAE:0.007139  RMSE:0.051078  CVRMSE:17.442494  R2:0.105805  MSE:0.002611  KL:0.869957  MAE:0.007107  RMSE:0.051097  CVRMSE:17.448811  R2:0.106900  MSE:0.002608  KL:0.627728  MAE:0.007080  RMSE:0.051065  CVRMSE:17.438126  R2:0.105534  MSE:0.002612  KL:0.667475  MAE:0.007120  RMSE:0.051104  CVRMSE:17.451461  R2:0.107223  MSE:0.002607  KL:0.818159  MAE:0.007051  RMSE:0.051056  CVRMSE:17.434977  train_loss:\n",
            "0.002606731285165027\n",
            "R2:0.116755  MSE:0.002663  KL:0.054826  MAE:0.005254  RMSE:0.051603  CVRMSE:17.064299  test_loss:\n",
            "5.23082895996813e-06\n",
            "R2:0.105943  MSE:0.002610  KL:0.816293  MAE:0.007097  RMSE:0.051093  CVRMSE:17.447464  R2:0.103716  MSE:0.002617  KL:0.938823  MAE:0.007123  RMSE:0.051156  CVRMSE:17.469188  R2:0.104276  MSE:0.002615  KL:0.825555  MAE:0.007129  RMSE:0.051140  CVRMSE:17.463723  R2:0.107419  MSE:0.002606  KL:0.944487  MAE:0.007099  RMSE:0.051051  CVRMSE:17.433063  R2:0.107542  MSE:0.002606  KL:0.824487  MAE:0.007086  RMSE:0.051047  CVRMSE:17.431861  train_loss:\n",
            "0.002605799821182614\n",
            "R2:0.116322  MSE:0.002664  KL:0.053628  MAE:0.005525  RMSE:0.051616  CVRMSE:17.068483  test_loss:\n",
            "5.233482938393174e-06\n",
            "R2:0.103457  MSE:0.002618  KL:0.728735  MAE:0.007086  RMSE:0.051164  CVRMSE:17.471709  R2:0.104179  MSE:0.002616  KL:0.667498  MAE:0.007138  RMSE:0.051143  CVRMSE:17.464670  R2:0.105667  MSE:0.002611  KL:0.667711  MAE:0.007085  RMSE:0.051101  CVRMSE:17.450163  R2:0.106328  MSE:0.002609  KL:0.879435  MAE:0.007083  RMSE:0.051082  CVRMSE:17.443707  R2:0.105892  MSE:0.002611  KL:1.280511  MAE:0.007136  RMSE:0.051094  CVRMSE:17.447962  train_loss:\n",
            "0.002610615780939337\n",
            "R2:0.122853  MSE:0.002645  KL:0.051534  MAE:0.006255  RMSE:0.051425  CVRMSE:17.005297  test_loss:\n",
            "5.194271163200803e-06\n",
            "R2:0.105471  MSE:0.002612  KL:0.807906  MAE:0.007070  RMSE:0.051106  CVRMSE:17.452070  R2:0.104741  MSE:0.002614  KL:0.756192  MAE:0.007085  RMSE:0.051127  CVRMSE:17.459195  R2:0.106569  MSE:0.002609  KL:0.556455  MAE:0.007066  RMSE:0.051075  CVRMSE:17.441357  R2:0.107456  MSE:0.002606  KL:0.533763  MAE:0.007073  RMSE:0.051049  CVRMSE:17.432700  R2:0.107761  MSE:0.002605  KL:0.616822  MAE:0.007097  RMSE:0.051041  CVRMSE:17.429724  train_loss:\n",
            "0.0026051609084739654\n",
            "R2:0.120052  MSE:0.002653  KL:0.053370  MAE:0.005566  RMSE:0.051507  CVRMSE:17.032423  test_loss:\n",
            "5.21110966701072e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.106764  MSE:0.002608  KL:0.846119  MAE:0.007085  RMSE:0.051069  CVRMSE:17.439458  R2:0.107219  MSE:0.002607  KL:0.808079  MAE:0.007093  RMSE:0.051056  CVRMSE:17.435009  R2:0.106676  MSE:0.002608  KL:0.662996  MAE:0.007062  RMSE:0.051072  CVRMSE:17.440311  R2:0.104700  MSE:0.002614  KL:0.694636  MAE:0.007126  RMSE:0.051128  CVRMSE:17.459592  R2:0.107643  MSE:0.002606  KL:0.604215  MAE:0.007070  RMSE:0.051044  CVRMSE:17.430875  train_loss:\n",
            "0.0026055049804096686\n",
            "R2:0.116828  MSE:0.002663  KL:0.053670  MAE:0.005272  RMSE:0.051601  CVRMSE:17.063594  test_loss:\n",
            "5.230510571119722e-06\n",
            "R2:0.106566  MSE:0.002609  KL:0.685018  MAE:0.007103  RMSE:0.051075  CVRMSE:17.441388  R2:0.106207  MSE:0.002610  KL:1.524329  MAE:0.007126  RMSE:0.051085  CVRMSE:17.444888  R2:0.104788  MSE:0.002614  KL:0.640959  MAE:0.007101  RMSE:0.051126  CVRMSE:17.458738  R2:0.105944  MSE:0.002610  KL:0.835302  MAE:0.007112  RMSE:0.051093  CVRMSE:17.447459  R2:0.106574  MSE:0.002609  KL:0.676395  MAE:0.007100  RMSE:0.051075  CVRMSE:17.441310  train_loss:\n",
            "0.002608625555615002\n",
            "R2:0.119461  MSE:0.002655  KL:0.053718  MAE:0.005849  RMSE:0.051524  CVRMSE:17.038145  test_loss:\n",
            "5.214602218222646e-06\n",
            "R2:0.105200  MSE:0.002613  KL:0.798372  MAE:0.007103  RMSE:0.051114  CVRMSE:17.454719  R2:0.105401  MSE:0.002612  KL:0.830411  MAE:0.007076  RMSE:0.051108  CVRMSE:17.452756  R2:0.104657  MSE:0.002614  KL:0.733765  MAE:0.007130  RMSE:0.051129  CVRMSE:17.460008  R2:0.103774  MSE:0.002617  KL:0.954268  MAE:0.007133  RMSE:0.051155  CVRMSE:17.468615  R2:0.104060  MSE:0.002616  KL:0.953423  MAE:0.007106  RMSE:0.051146  CVRMSE:17.465827  train_loss:\n",
            "0.002615964436194748\n",
            "R2:0.117224  MSE:0.002661  KL:0.049735  MAE:0.004702  RMSE:0.051590  CVRMSE:17.059773  test_loss:\n",
            "5.228253265423202e-06\n",
            "R2:0.107086  MSE:0.002607  KL:0.798207  MAE:0.007097  RMSE:0.051060  CVRMSE:17.436306  R2:0.104902  MSE:0.002614  KL:0.917947  MAE:0.007106  RMSE:0.051122  CVRMSE:17.457624  R2:0.106216  MSE:0.002610  KL:0.697609  MAE:0.007125  RMSE:0.051085  CVRMSE:17.444805  R2:0.106300  MSE:0.002609  KL:0.787926  MAE:0.007080  RMSE:0.051083  CVRMSE:17.443986  R2:0.104219  MSE:0.002616  KL:1.020352  MAE:0.007088  RMSE:0.051142  CVRMSE:17.464280  train_loss:\n",
            "0.002615500933973841\n",
            "R2:0.117781  MSE:0.002660  KL:0.051477  MAE:0.005199  RMSE:0.051573  CVRMSE:17.054388  test_loss:\n",
            "5.224792424374862e-06\n",
            "R2:0.106513  MSE:0.002609  KL:0.670934  MAE:0.007089  RMSE:0.051076  CVRMSE:17.441901  R2:0.105599  MSE:0.002611  KL:0.725494  MAE:0.007117  RMSE:0.051103  CVRMSE:17.450821  R2:0.105954  MSE:0.002610  KL:1.058056  MAE:0.007074  RMSE:0.051092  CVRMSE:17.447356  R2:0.104974  MSE:0.002613  KL:0.709477  MAE:0.007131  RMSE:0.051120  CVRMSE:17.456923  R2:0.104989  MSE:0.002613  KL:0.580005  MAE:0.007097  RMSE:0.051120  CVRMSE:17.456773  train_loss:\n",
            "0.0026132530234465176\n",
            "R2:0.116310  MSE:0.002664  KL:0.054800  MAE:0.005288  RMSE:0.051616  CVRMSE:17.068602  test_loss:\n",
            "5.23350416756977e-06\n",
            "R2:0.105469  MSE:0.002612  KL:0.822222  MAE:0.007094  RMSE:0.051106  CVRMSE:17.452095  R2:0.107778  MSE:0.002605  KL:1.015718  MAE:0.007075  RMSE:0.051040  CVRMSE:17.429556  R2:0.107509  MSE:0.002606  KL:0.619642  MAE:0.007037  RMSE:0.051048  CVRMSE:17.432184  R2:0.104059  MSE:0.002616  KL:1.099600  MAE:0.007093  RMSE:0.051147  CVRMSE:17.465845  R2:0.106770  MSE:0.002608  KL:0.537882  MAE:0.007125  RMSE:0.051069  CVRMSE:17.439400  train_loss:\n",
            "0.0026080541366670204\n",
            "R2:0.121288  MSE:0.002649  KL:0.049774  MAE:0.005095  RMSE:0.051471  CVRMSE:17.020458  test_loss:\n",
            "5.203895411689154e-06\n",
            "R2:0.105610  MSE:0.002611  KL:0.899380  MAE:0.007133  RMSE:0.051102  CVRMSE:17.450712  R2:0.107105  MSE:0.002607  KL:0.848418  MAE:0.007098  RMSE:0.051060  CVRMSE:17.436129  R2:0.105931  MSE:0.002611  KL:0.822974  MAE:0.007101  RMSE:0.051093  CVRMSE:17.447580  R2:0.105190  MSE:0.002613  KL:1.144507  MAE:0.007077  RMSE:0.051114  CVRMSE:17.454816  R2:0.104966  MSE:0.002613  KL:0.640467  MAE:0.007094  RMSE:0.051121  CVRMSE:17.457001  train_loss:\n",
            "0.0026133212380775577\n",
            "R2:0.117731  MSE:0.002660  KL:0.052136  MAE:0.005009  RMSE:0.051575  CVRMSE:17.054870  test_loss:\n",
            "5.225102639953198e-06\n",
            "R2:0.106435  MSE:0.002609  KL:0.798052  MAE:0.007080  RMSE:0.051079  CVRMSE:17.442669  R2:0.105186  MSE:0.002613  KL:0.857030  MAE:0.007115  RMSE:0.051114  CVRMSE:17.454854  R2:0.106127  MSE:0.002610  KL:0.831002  MAE:0.007103  RMSE:0.051087  CVRMSE:17.445668  R2:0.105573  MSE:0.002612  KL:0.613529  MAE:0.007080  RMSE:0.051103  CVRMSE:17.451081  R2:0.104955  MSE:0.002613  KL:1.432486  MAE:0.007163  RMSE:0.051121  CVRMSE:17.457109  train_loss:\n",
            "0.0026133534716396402\n",
            "R2:0.119752  MSE:0.002654  KL:0.050527  MAE:0.005149  RMSE:0.051516  CVRMSE:17.035321  test_loss:\n",
            "5.213009914251493e-06\n",
            "R2:0.104517  MSE:0.002615  KL:0.919514  MAE:0.007102  RMSE:0.051133  CVRMSE:17.461374  R2:0.106474  MSE:0.002609  KL:0.799353  MAE:0.007078  RMSE:0.051078  CVRMSE:17.442289  R2:0.106293  MSE:0.002609  KL:0.860033  MAE:0.007092  RMSE:0.051083  CVRMSE:17.444048  R2:0.104609  MSE:0.002614  KL:0.479695  MAE:0.007059  RMSE:0.051131  CVRMSE:17.460478  R2:0.107771  MSE:0.002605  KL:0.895205  MAE:0.007097  RMSE:0.051040  CVRMSE:17.429617  train_loss:\n",
            "0.0026051290428669126\n",
            "R2:0.120955  MSE:0.002650  KL:0.052781  MAE:0.005537  RMSE:0.051480  CVRMSE:17.023681  test_loss:\n",
            "5.205753977452515e-06\n",
            "R2:0.105928  MSE:0.002611  KL:0.716006  MAE:0.007106  RMSE:0.051093  CVRMSE:17.447614  R2:0.105281  MSE:0.002612  KL:0.796859  MAE:0.007093  RMSE:0.051112  CVRMSE:17.453921  R2:0.107342  MSE:0.002606  KL:0.792235  MAE:0.007091  RMSE:0.051053  CVRMSE:17.433815  R2:0.107020  MSE:0.002607  KL:0.602871  MAE:0.007084  RMSE:0.051062  CVRMSE:17.436957  R2:0.105506  MSE:0.002612  KL:0.693889  MAE:0.007108  RMSE:0.051105  CVRMSE:17.451730  train_loss:\n",
            "0.0026117433330382703\n",
            "R2:0.117863  MSE:0.002660  KL:0.052445  MAE:0.005189  RMSE:0.051571  CVRMSE:17.053590  test_loss:\n",
            "5.22421671525982e-06\n",
            "R2:0.106481  MSE:0.002609  KL:0.902188  MAE:0.007076  RMSE:0.051077  CVRMSE:17.442221  R2:0.106151  MSE:0.002610  KL:0.723008  MAE:0.007101  RMSE:0.051087  CVRMSE:17.445439  R2:0.104426  MSE:0.002615  KL:0.968108  MAE:0.007085  RMSE:0.051136  CVRMSE:17.462265  R2:0.107143  MSE:0.002607  KL:0.519453  MAE:0.007040  RMSE:0.051058  CVRMSE:17.435758  R2:0.105121  MSE:0.002613  KL:0.675470  MAE:0.007106  RMSE:0.051116  CVRMSE:17.455484  train_loss:\n",
            "0.0026128671295813795\n",
            "R2:0.118390  MSE:0.002658  KL:0.051344  MAE:0.005257  RMSE:0.051556  CVRMSE:17.048499  test_loss:\n",
            "5.221196647723028e-06\n",
            "R2:0.107907  MSE:0.002605  KL:0.583054  MAE:0.007039  RMSE:0.051037  CVRMSE:17.428291  R2:0.105213  MSE:0.002613  KL:0.558817  MAE:0.007079  RMSE:0.051114  CVRMSE:17.454592  R2:0.103249  MSE:0.002618  KL:0.713597  MAE:0.007101  RMSE:0.051170  CVRMSE:17.473732  R2:0.106724  MSE:0.002608  KL:1.030483  MAE:0.007119  RMSE:0.051070  CVRMSE:17.439844  R2:0.106771  MSE:0.002608  KL:0.882365  MAE:0.007090  RMSE:0.051069  CVRMSE:17.439385  train_loss:\n",
            "0.002608049613932392\n",
            "R2:0.117434  MSE:0.002661  KL:0.051793  MAE:0.005097  RMSE:0.051583  CVRMSE:17.057737  test_loss:\n",
            "5.226882008018591e-06\n",
            "R2:0.104379  MSE:0.002615  KL:0.679375  MAE:0.007121  RMSE:0.051137  CVRMSE:17.462720  R2:0.104602  MSE:0.002614  KL:0.759312  MAE:0.007117  RMSE:0.051131  CVRMSE:17.460547  R2:0.107024  MSE:0.002607  KL:0.719659  MAE:0.007105  RMSE:0.051062  CVRMSE:17.436916  R2:0.106547  MSE:0.002609  KL:0.640724  MAE:0.007107  RMSE:0.051075  CVRMSE:17.441572  R2:0.106961  MSE:0.002607  KL:1.014883  MAE:0.007071  RMSE:0.051064  CVRMSE:17.437533  train_loss:\n",
            "0.0026074958373105777\n",
            "R2:0.116312  MSE:0.002664  KL:0.055970  MAE:0.005521  RMSE:0.051616  CVRMSE:17.068582  test_loss:\n",
            "5.23343497984026e-06\n",
            "R2:0.104898  MSE:0.002614  KL:0.739424  MAE:0.007115  RMSE:0.051123  CVRMSE:17.457661  R2:0.105975  MSE:0.002610  KL:0.684274  MAE:0.007100  RMSE:0.051092  CVRMSE:17.447153  R2:0.105089  MSE:0.002613  KL:1.049838  MAE:0.007108  RMSE:0.051117  CVRMSE:17.455803  R2:0.105149  MSE:0.002613  KL:0.629884  MAE:0.007136  RMSE:0.051115  CVRMSE:17.455214  R2:0.105225  MSE:0.002613  KL:0.916743  MAE:0.007099  RMSE:0.051113  CVRMSE:17.454475  train_loss:\n",
            "0.0026125650327473303\n",
            "R2:0.121532  MSE:0.002648  KL:0.051001  MAE:0.005404  RMSE:0.051464  CVRMSE:17.018090  test_loss:\n",
            "5.202247272582236e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.104038  MSE:0.002616  KL:0.868195  MAE:0.007098  RMSE:0.051147  CVRMSE:17.466047  R2:0.106811  MSE:0.002608  KL:0.937965  MAE:0.007090  RMSE:0.051068  CVRMSE:17.438996  R2:0.106750  MSE:0.002608  KL:0.870858  MAE:0.007098  RMSE:0.051070  CVRMSE:17.439596  R2:0.105701  MSE:0.002611  KL:0.561586  MAE:0.007106  RMSE:0.051100  CVRMSE:17.449833  R2:0.105838  MSE:0.002611  KL:0.723978  MAE:0.007069  RMSE:0.051096  CVRMSE:17.448488  train_loss:\n",
            "0.002610773017572402\n",
            "R2:0.118499  MSE:0.002658  KL:0.052539  MAE:0.005082  RMSE:0.051552  CVRMSE:17.047445  test_loss:\n",
            "5.2205035878948385e-06\n",
            "R2:0.106504  MSE:0.002609  KL:0.951499  MAE:0.007085  RMSE:0.051077  CVRMSE:17.441997  R2:0.104698  MSE:0.002614  KL:0.741627  MAE:0.007110  RMSE:0.051128  CVRMSE:17.459616  R2:0.105433  MSE:0.002612  KL:0.691565  MAE:0.007083  RMSE:0.051107  CVRMSE:17.452446  R2:0.106823  MSE:0.002608  KL:0.909206  MAE:0.007063  RMSE:0.051068  CVRMSE:17.438880  R2:0.105448  MSE:0.002612  KL:0.714220  MAE:0.007123  RMSE:0.051107  CVRMSE:17.452301  train_loss:\n",
            "0.0026119143938853036\n",
            "R2:0.118295  MSE:0.002658  KL:0.054366  MAE:0.005235  RMSE:0.051558  CVRMSE:17.049418  test_loss:\n",
            "5.221578314133782e-06\n",
            "R2:0.106036  MSE:0.002610  KL:0.563801  MAE:0.007060  RMSE:0.051090  CVRMSE:17.446562  R2:0.105037  MSE:0.002613  KL:0.919010  MAE:0.007106  RMSE:0.051119  CVRMSE:17.456308  R2:0.105493  MSE:0.002612  KL:0.755915  MAE:0.007120  RMSE:0.051106  CVRMSE:17.451862  R2:0.104704  MSE:0.002614  KL:0.700817  MAE:0.007100  RMSE:0.051128  CVRMSE:17.459555  R2:0.105768  MSE:0.002611  KL:0.715036  MAE:0.007066  RMSE:0.051098  CVRMSE:17.449177  train_loss:\n",
            "0.0026109794082901814\n",
            "R2:0.115388  MSE:0.002667  KL:0.053646  MAE:0.005145  RMSE:0.051643  CVRMSE:17.077505  test_loss:\n",
            "5.239080617820052e-06\n",
            "R2:0.103517  MSE:0.002618  KL:0.641136  MAE:0.007073  RMSE:0.051162  CVRMSE:17.471119  R2:0.104713  MSE:0.002614  KL:0.609057  MAE:0.007057  RMSE:0.051128  CVRMSE:17.459464  R2:0.107757  MSE:0.002605  KL:1.044841  MAE:0.007097  RMSE:0.051041  CVRMSE:17.429759  R2:0.105591  MSE:0.002611  KL:0.639754  MAE:0.007084  RMSE:0.051103  CVRMSE:17.450904  R2:0.103911  MSE:0.002616  KL:0.823212  MAE:0.007079  RMSE:0.051151  CVRMSE:17.467287  train_loss:\n",
            "0.002616401847140576\n",
            "R2:0.118068  MSE:0.002659  KL:0.053359  MAE:0.005549  RMSE:0.051565  CVRMSE:17.051613  test_loss:\n",
            "5.222974055732223e-06\n",
            "R2:0.105911  MSE:0.002611  KL:0.722495  MAE:0.007109  RMSE:0.051094  CVRMSE:17.447776  R2:0.104814  MSE:0.002614  KL:0.831761  MAE:0.007104  RMSE:0.051125  CVRMSE:17.458476  R2:0.105973  MSE:0.002610  KL:1.254677  MAE:0.007123  RMSE:0.051092  CVRMSE:17.447175  R2:0.104698  MSE:0.002614  KL:1.071420  MAE:0.007125  RMSE:0.051128  CVRMSE:17.459615  R2:0.106670  MSE:0.002608  KL:0.829686  MAE:0.007088  RMSE:0.051072  CVRMSE:17.440369  train_loss:\n",
            "0.0026083441127658386\n",
            "R2:0.120176  MSE:0.002653  KL:0.051404  MAE:0.005195  RMSE:0.051503  CVRMSE:17.031224  test_loss:\n",
            "5.210466348630278e-06\n",
            "R2:0.107188  MSE:0.002607  KL:0.667928  MAE:0.007103  RMSE:0.051057  CVRMSE:17.435315  R2:0.106762  MSE:0.002608  KL:0.934719  MAE:0.007111  RMSE:0.051069  CVRMSE:17.439470  R2:0.106780  MSE:0.002608  KL:0.788939  MAE:0.007036  RMSE:0.051069  CVRMSE:17.439297  R2:0.106279  MSE:0.002609  KL:0.589845  MAE:0.007075  RMSE:0.051083  CVRMSE:17.444186  R2:0.105908  MSE:0.002611  KL:0.889844  MAE:0.007061  RMSE:0.051094  CVRMSE:17.447804  train_loss:\n",
            "0.0026105685091078767\n",
            "R2:0.121302  MSE:0.002649  KL:0.050953  MAE:0.005335  RMSE:0.051470  CVRMSE:17.020318  test_loss:\n",
            "5.203680620468505e-06\n",
            "R2:0.106107  MSE:0.002610  KL:0.698713  MAE:0.007077  RMSE:0.051088  CVRMSE:17.445866  R2:0.106617  MSE:0.002609  KL:0.618842  MAE:0.007095  RMSE:0.051073  CVRMSE:17.440894  R2:0.106106  MSE:0.002610  KL:0.993388  MAE:0.007102  RMSE:0.051088  CVRMSE:17.445882  R2:0.103165  MSE:0.002619  KL:0.500292  MAE:0.007089  RMSE:0.051172  CVRMSE:17.474548  R2:0.105588  MSE:0.002612  KL:0.917218  MAE:0.007118  RMSE:0.051103  CVRMSE:17.450933  train_loss:\n",
            "0.0026115048712368766\n",
            "R2:0.116348  MSE:0.002664  KL:0.055666  MAE:0.005759  RMSE:0.051615  CVRMSE:17.068232  test_loss:\n",
            "5.233182877174589e-06\n",
            "R2:0.107369  MSE:0.002606  KL:0.773291  MAE:0.007063  RMSE:0.051052  CVRMSE:17.433545  R2:0.106169  MSE:0.002610  KL:0.780971  MAE:0.007111  RMSE:0.051086  CVRMSE:17.445259  R2:0.105602  MSE:0.002611  KL:0.753468  MAE:0.007096  RMSE:0.051102  CVRMSE:17.450794  R2:0.104744  MSE:0.002614  KL:0.684157  MAE:0.007094  RMSE:0.051127  CVRMSE:17.459159  R2:0.105501  MSE:0.002612  KL:0.944497  MAE:0.007090  RMSE:0.051105  CVRMSE:17.451779  train_loss:\n",
            "0.00261175791892505\n",
            "R2:0.116759  MSE:0.002663  KL:0.055907  MAE:0.005396  RMSE:0.051603  CVRMSE:17.064262  test_loss:\n",
            "5.230780238348146e-06\n",
            "R2:0.106763  MSE:0.002608  KL:0.652984  MAE:0.007083  RMSE:0.051069  CVRMSE:17.439464  R2:0.107380  MSE:0.002606  KL:0.703891  MAE:0.007110  RMSE:0.051052  CVRMSE:17.433436  R2:0.106494  MSE:0.002609  KL:0.794002  MAE:0.007100  RMSE:0.051077  CVRMSE:17.442092  R2:0.107526  MSE:0.002606  KL:0.582006  MAE:0.007104  RMSE:0.051047  CVRMSE:17.432016  R2:0.108087  MSE:0.002604  KL:1.023152  MAE:0.007087  RMSE:0.051031  CVRMSE:17.426539  train_loss:\n",
            "0.0026042088431501723\n",
            "R2:0.118289  MSE:0.002658  KL:0.053663  MAE:0.005588  RMSE:0.051558  CVRMSE:17.049472  test_loss:\n",
            "5.221671630576347e-06\n",
            "R2:0.105608  MSE:0.002611  KL:1.016173  MAE:0.007105  RMSE:0.051102  CVRMSE:17.450735  R2:0.105126  MSE:0.002613  KL:0.662798  MAE:0.007153  RMSE:0.051116  CVRMSE:17.455438  R2:0.107336  MSE:0.002606  KL:0.695503  MAE:0.007092  RMSE:0.051053  CVRMSE:17.433873  R2:0.106816  MSE:0.002608  KL:1.338581  MAE:0.007128  RMSE:0.051068  CVRMSE:17.438946  R2:0.105699  MSE:0.002611  KL:0.962030  MAE:0.007093  RMSE:0.051100  CVRMSE:17.449853  train_loss:\n",
            "0.002611181549974479\n",
            "R2:0.116237  MSE:0.002664  KL:0.052166  MAE:0.004990  RMSE:0.051618  CVRMSE:17.069305  test_loss:\n",
            "5.234078471058915e-06\n",
            "R2:0.107290  MSE:0.002607  KL:0.818718  MAE:0.007057  RMSE:0.051054  CVRMSE:17.434318  R2:0.108024  MSE:0.002604  KL:1.054169  MAE:0.007094  RMSE:0.051033  CVRMSE:17.427148  R2:0.106948  MSE:0.002608  KL:0.674551  MAE:0.007096  RMSE:0.051064  CVRMSE:17.437656  R2:0.104501  MSE:0.002615  KL:0.717644  MAE:0.007088  RMSE:0.051134  CVRMSE:17.461531  R2:0.106012  MSE:0.002610  KL:0.598661  MAE:0.007118  RMSE:0.051091  CVRMSE:17.446795  train_loss:\n",
            "0.0026102665472525766\n",
            "R2:0.116359  MSE:0.002664  KL:0.051494  MAE:0.004836  RMSE:0.051615  CVRMSE:17.068121  test_loss:\n",
            "5.233373086984182e-06\n",
            "R2:0.106855  MSE:0.002608  KL:0.724461  MAE:0.007103  RMSE:0.051067  CVRMSE:17.438567  R2:0.104962  MSE:0.002613  KL:0.702617  MAE:0.007086  RMSE:0.051121  CVRMSE:17.457035  R2:0.104893  MSE:0.002614  KL:0.759302  MAE:0.007127  RMSE:0.051123  CVRMSE:17.457710  R2:0.106716  MSE:0.002608  KL:0.879582  MAE:0.007065  RMSE:0.051071  CVRMSE:17.439922  R2:0.105225  MSE:0.002613  KL:0.778799  MAE:0.007115  RMSE:0.051113  CVRMSE:17.454474  train_loss:\n",
            "0.002612564691984614\n",
            "R2:0.119157  MSE:0.002656  KL:0.053763  MAE:0.005920  RMSE:0.051533  CVRMSE:17.041086  test_loss:\n",
            "5.2164004810606375e-06\n",
            "R2:0.104874  MSE:0.002614  KL:0.598823  MAE:0.007128  RMSE:0.051123  CVRMSE:17.457891  R2:0.104285  MSE:0.002615  KL:0.880691  MAE:0.007126  RMSE:0.051140  CVRMSE:17.463637  R2:0.105133  MSE:0.002613  KL:0.852903  MAE:0.007093  RMSE:0.051116  CVRMSE:17.455369  R2:0.105976  MSE:0.002610  KL:0.624043  MAE:0.007057  RMSE:0.051092  CVRMSE:17.447149  R2:0.103800  MSE:0.002617  KL:0.695713  MAE:0.007124  RMSE:0.051154  CVRMSE:17.468366  train_loss:\n",
            "0.002616725020767519\n",
            "R2:0.118064  MSE:0.002659  KL:0.051878  MAE:0.005114  RMSE:0.051565  CVRMSE:17.051656  test_loss:\n",
            "5.223104436660209e-06\n",
            "R2:0.106705  MSE:0.002608  KL:0.582124  MAE:0.007072  RMSE:0.051071  CVRMSE:17.440027  R2:0.105247  MSE:0.002612  KL:0.787481  MAE:0.007087  RMSE:0.051113  CVRMSE:17.454255  R2:0.107892  MSE:0.002605  KL:0.620410  MAE:0.007056  RMSE:0.051037  CVRMSE:17.428443  R2:0.104816  MSE:0.002614  KL:0.816334  MAE:0.007133  RMSE:0.051125  CVRMSE:17.458461  R2:0.107219  MSE:0.002607  KL:0.903486  MAE:0.007094  RMSE:0.051056  CVRMSE:17.435013  train_loss:\n",
            "0.0026067423031263534\n",
            "R2:0.122048  MSE:0.002647  KL:0.048973  MAE:0.005725  RMSE:0.051448  CVRMSE:17.013096  test_loss:\n",
            "5.199318894369743e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.107037  MSE:0.002607  KL:0.959610  MAE:0.007068  RMSE:0.051061  CVRMSE:17.436792  R2:0.104786  MSE:0.002614  KL:0.745187  MAE:0.007112  RMSE:0.051126  CVRMSE:17.458754  R2:0.106993  MSE:0.002607  KL:0.935247  MAE:0.007078  RMSE:0.051063  CVRMSE:17.437223  R2:0.104253  MSE:0.002615  KL:0.718808  MAE:0.007107  RMSE:0.051141  CVRMSE:17.463951  R2:0.104076  MSE:0.002616  KL:0.748343  MAE:0.007080  RMSE:0.051146  CVRMSE:17.465678  train_loss:\n",
            "0.002615919916818568\n",
            "R2:0.114113  MSE:0.002671  KL:0.052568  MAE:0.005230  RMSE:0.051680  CVRMSE:17.089801  test_loss:\n",
            "5.246769149613519e-06\n",
            "R2:0.104236  MSE:0.002615  KL:0.911292  MAE:0.007099  RMSE:0.051142  CVRMSE:17.464120  R2:0.104551  MSE:0.002615  KL:0.710039  MAE:0.007107  RMSE:0.051133  CVRMSE:17.461049  R2:0.104578  MSE:0.002614  KL:0.756043  MAE:0.007111  RMSE:0.051132  CVRMSE:17.460782  R2:0.106367  MSE:0.002609  KL:0.569489  MAE:0.007053  RMSE:0.051081  CVRMSE:17.443333  R2:0.104595  MSE:0.002614  KL:0.835764  MAE:0.007108  RMSE:0.051131  CVRMSE:17.460618  train_loss:\n",
            "0.002614404450586466\n",
            "R2:0.116708  MSE:0.002663  KL:0.055773  MAE:0.005729  RMSE:0.051605  CVRMSE:17.064759  test_loss:\n",
            "5.2310840688795505e-06\n",
            "R2:0.105530  MSE:0.002612  KL:0.847952  MAE:0.007082  RMSE:0.051105  CVRMSE:17.451495  R2:0.106249  MSE:0.002610  KL:0.879560  MAE:0.007092  RMSE:0.051084  CVRMSE:17.444483  R2:0.102953  MSE:0.002619  KL:0.610805  MAE:0.007093  RMSE:0.051178  CVRMSE:17.476614  R2:0.104606  MSE:0.002614  KL:0.579429  MAE:0.007108  RMSE:0.051131  CVRMSE:17.460513  R2:0.104665  MSE:0.002614  KL:0.904091  MAE:0.007136  RMSE:0.051129  CVRMSE:17.459935  train_loss:\n",
            "0.0026141998229732215\n",
            "R2:0.117551  MSE:0.002660  KL:0.051654  MAE:0.005068  RMSE:0.051580  CVRMSE:17.056609  test_loss:\n",
            "5.22620294340478e-06\n",
            "R2:0.105869  MSE:0.002611  KL:0.715019  MAE:0.007116  RMSE:0.051095  CVRMSE:17.448186  R2:0.105607  MSE:0.002611  KL:0.773419  MAE:0.007105  RMSE:0.051102  CVRMSE:17.450746  R2:0.105599  MSE:0.002611  KL:0.890297  MAE:0.007085  RMSE:0.051103  CVRMSE:17.450827  R2:0.106364  MSE:0.002609  KL:0.740845  MAE:0.007081  RMSE:0.051081  CVRMSE:17.443357  R2:0.105496  MSE:0.002612  KL:0.947028  MAE:0.007047  RMSE:0.051106  CVRMSE:17.451830  train_loss:\n",
            "0.0026117734239550702\n",
            "R2:0.116497  MSE:0.002664  KL:0.054825  MAE:0.005818  RMSE:0.051611  CVRMSE:17.066790  test_loss:\n",
            "5.2323736696305535e-06\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/0s/r_ncjf_j67z8qchrjzz_8j640000gn/T/ipykernel_1307/2696285520.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mreverse_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMytest_reverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_loss:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/0s/r_ncjf_j67z8qchrjzz_8j640000gn/T/ipykernel_1307/1706515214.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss_all\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_graphs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 2560\n",
        "batch_size = 512\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=5e-4)\n",
        "crit = F.mse_loss\n",
        "train_loader = DataLoader(Mydata_train, batch_size=batch_size)\n",
        "test_loader = DataLoader(Mydata_test, batch_size=512)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss,r2,mse,kl=train()\n",
        "    if epoch %5==0:      \n",
        "        print('train_loss:')\n",
        "        print(loss)\n",
        "\n",
        "        loss,r2,mse,kl=val()\n",
        "        print('test_loss:')\n",
        "        print(loss)\n",
        "\n",
        "y_predicted = []\n",
        "for data in test_loader:\n",
        "    loss = 0\n",
        "    data = data.to(device)\n",
        "    output = model(data)\n",
        "    label = data.y.to(device)\n",
        "    y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "df_test[\"y_hat\"]=y_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezCCwtkgFkN_",
        "outputId": "3d36ffca-1759-428f-e452-5c169b248ce9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>auuc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y_hat</th>\n",
              "      <td>0.732616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random</th>\n",
              "      <td>0.499185</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            auuc\n",
              "y_hat   0.732616\n",
              "Random  0.499185"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# estimate area under the uplift curve (AUUC)\n",
        "from causalml.metrics import *\n",
        "uplift=df_test.copy()\n",
        "tau_hat=pd.concat([t0,t1], axis=0, join=\"inner\")\n",
        "uplift=pd.concat([uplift,tau_hat], axis=1, join=\"inner\")\n",
        "uplift = uplift.loc[:,~uplift.columns.duplicated()]\n",
        "\n",
        "auuc=auuc_score(uplift, outcome_col='y', treatment_col='T')\n",
        "gcn_auuc=pd.DataFrame(auuc[[\"y_hat\",\"Random\"]],columns=['auuc'])\n",
        "gcn_auuc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGF8AUtZFkN_"
      },
      "source": [
        "# GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1jwzm-7FkN_",
        "outputId": "cbcfc005-886e-4d9c-c50c-2dd5ccb5394c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:-0.813384  MSE:0.005295  KL:2.094833  MAE:0.017525  RMSE:0.072765  CVRMSE:24.848187  train_loss:\n",
            "0.005294719280111404\n",
            "R2:0.072054  MSE:0.002798  KL:0.126590  MAE:0.005984  RMSE:0.052893  CVRMSE:17.490782  test_loss:\n",
            "5.497692996261892e-06\n",
            "R2:0.060488  MSE:0.002743  KL:2.104842  MAE:0.009919  RMSE:0.052375  CVRMSE:17.885496  R2:0.057913  MSE:0.002751  KL:1.238602  MAE:0.008663  RMSE:0.052447  CVRMSE:17.909985  R2:0.060253  MSE:0.002744  KL:1.248182  MAE:0.008433  RMSE:0.052382  CVRMSE:17.887734  R2:0.064215  MSE:0.002732  KL:1.533469  MAE:0.008456  RMSE:0.052271  CVRMSE:17.849982  R2:0.064579  MSE:0.002731  KL:1.586445  MAE:0.008461  RMSE:0.052261  CVRMSE:17.846508  train_loss:\n",
            "0.0027312411740449236\n",
            "R2:0.056647  MSE:0.002844  KL:0.100752  MAE:0.005517  RMSE:0.053330  CVRMSE:17.635387  test_loss:\n",
            "5.589489941400884e-06\n",
            "R2:0.064812  MSE:0.002731  KL:1.744580  MAE:0.008459  RMSE:0.052255  CVRMSE:17.844289  R2:0.065108  MSE:0.002730  KL:1.738378  MAE:0.008456  RMSE:0.052247  CVRMSE:17.841467  R2:0.064888  MSE:0.002730  KL:1.717766  MAE:0.008453  RMSE:0.052253  CVRMSE:17.843561  R2:0.064689  MSE:0.002731  KL:1.686471  MAE:0.008447  RMSE:0.052258  CVRMSE:17.845459  R2:0.064419  MSE:0.002732  KL:1.771654  MAE:0.008445  RMSE:0.052266  CVRMSE:17.848038  train_loss:\n",
            "0.0027317093230096023\n",
            "R2:0.057155  MSE:0.002843  KL:0.105881  MAE:0.005612  RMSE:0.053316  CVRMSE:17.630644  test_loss:\n",
            "5.586452004709481e-06\n",
            "R2:0.064848  MSE:0.002730  KL:1.700174  MAE:0.008435  RMSE:0.052254  CVRMSE:17.843949  R2:0.064525  MSE:0.002731  KL:1.587066  MAE:0.008468  RMSE:0.052263  CVRMSE:17.847029  R2:0.064762  MSE:0.002731  KL:1.600062  MAE:0.008477  RMSE:0.052256  CVRMSE:17.844764  R2:0.064245  MSE:0.002732  KL:1.584316  MAE:0.008466  RMSE:0.052271  CVRMSE:17.849697  R2:0.064774  MSE:0.002731  KL:1.571855  MAE:0.008457  RMSE:0.052256  CVRMSE:17.844656  train_loss:\n",
            "0.0027306743580779755\n",
            "R2:0.057591  MSE:0.002841  KL:0.101139  MAE:0.005591  RMSE:0.053304  CVRMSE:17.626561  test_loss:\n",
            "5.583864519005802e-06\n",
            "R2:0.064878  MSE:0.002730  KL:1.737214  MAE:0.008471  RMSE:0.052253  CVRMSE:17.843661  R2:0.064846  MSE:0.002730  KL:1.639765  MAE:0.008485  RMSE:0.052254  CVRMSE:17.843967  R2:0.064428  MSE:0.002732  KL:1.637591  MAE:0.008440  RMSE:0.052266  CVRMSE:17.847957  R2:0.065146  MSE:0.002730  KL:1.762952  MAE:0.008471  RMSE:0.052245  CVRMSE:17.841106  R2:0.064568  MSE:0.002731  KL:1.704276  MAE:0.008404  RMSE:0.052262  CVRMSE:17.846619  train_loss:\n",
            "0.002731274975775923\n",
            "R2:0.057446  MSE:0.002842  KL:0.101947  MAE:0.005929  RMSE:0.053308  CVRMSE:17.627918  test_loss:\n",
            "5.584688566759714e-06\n",
            "R2:0.064857  MSE:0.002730  KL:1.758027  MAE:0.008460  RMSE:0.052254  CVRMSE:17.843857  R2:0.064371  MSE:0.002732  KL:1.720052  MAE:0.008430  RMSE:0.052267  CVRMSE:17.848500  R2:0.065613  MSE:0.002728  KL:1.600872  MAE:0.008512  RMSE:0.052232  CVRMSE:17.836646  R2:0.064834  MSE:0.002730  KL:1.677273  MAE:0.008472  RMSE:0.052254  CVRMSE:17.844078  R2:0.064796  MSE:0.002731  KL:1.681250  MAE:0.008489  RMSE:0.052255  CVRMSE:17.844446  train_loss:\n",
            "0.002730610110640133\n",
            "R2:0.056179  MSE:0.002846  KL:0.102293  MAE:0.005667  RMSE:0.053344  CVRMSE:17.639761  test_loss:\n",
            "5.592255682604433e-06\n",
            "R2:0.065086  MSE:0.002730  KL:1.751158  MAE:0.008489  RMSE:0.052247  CVRMSE:17.841678  R2:0.065139  MSE:0.002730  KL:1.681575  MAE:0.008498  RMSE:0.052246  CVRMSE:17.841166  R2:0.064590  MSE:0.002731  KL:1.666795  MAE:0.008445  RMSE:0.052261  CVRMSE:17.846411  R2:0.064134  MSE:0.002733  KL:1.727385  MAE:0.008471  RMSE:0.052274  CVRMSE:17.850753  R2:0.064288  MSE:0.002732  KL:1.771836  MAE:0.008466  RMSE:0.052269  CVRMSE:17.849289  train_loss:\n",
            "0.002732092466848261\n",
            "R2:0.056223  MSE:0.002845  KL:0.101320  MAE:0.005534  RMSE:0.053342  CVRMSE:17.639352  test_loss:\n",
            "5.592012955994695e-06\n",
            "R2:0.064721  MSE:0.002731  KL:1.690621  MAE:0.008480  RMSE:0.052257  CVRMSE:17.845161  R2:0.064351  MSE:0.002732  KL:1.781035  MAE:0.008435  RMSE:0.052268  CVRMSE:17.848689  R2:0.065132  MSE:0.002730  KL:1.767785  MAE:0.008495  RMSE:0.052246  CVRMSE:17.841232  R2:0.064726  MSE:0.002731  KL:1.778767  MAE:0.008454  RMSE:0.052257  CVRMSE:17.845108  R2:0.064703  MSE:0.002731  KL:1.771874  MAE:0.008453  RMSE:0.052258  CVRMSE:17.845326  train_loss:\n",
            "0.0027308791713068718\n",
            "R2:0.057436  MSE:0.002842  KL:0.101456  MAE:0.005700  RMSE:0.053308  CVRMSE:17.628015  test_loss:\n",
            "5.58477663659164e-06\n",
            "R2:0.064470  MSE:0.002732  KL:1.705905  MAE:0.008504  RMSE:0.052264  CVRMSE:17.847553  R2:0.064227  MSE:0.002732  KL:1.700385  MAE:0.008481  RMSE:0.052271  CVRMSE:17.849874  R2:0.064825  MSE:0.002731  KL:1.662722  MAE:0.008450  RMSE:0.052254  CVRMSE:17.844161  R2:0.064688  MSE:0.002731  KL:1.691676  MAE:0.008421  RMSE:0.052258  CVRMSE:17.845468  R2:0.064823  MSE:0.002731  KL:1.694118  MAE:0.008454  RMSE:0.052254  CVRMSE:17.844189  train_loss:\n",
            "0.0027305313805355584\n",
            "R2:0.055858  MSE:0.002846  KL:0.101368  MAE:0.005397  RMSE:0.053353  CVRMSE:17.642758  test_loss:\n",
            "5.5941998764456e-06\n",
            "R2:0.064248  MSE:0.002732  KL:1.797117  MAE:0.008437  RMSE:0.052271  CVRMSE:17.849672  R2:0.064843  MSE:0.002730  KL:1.655213  MAE:0.008473  RMSE:0.052254  CVRMSE:17.843990  R2:0.064444  MSE:0.002732  KL:1.651202  MAE:0.008439  RMSE:0.052265  CVRMSE:17.847803  R2:0.064508  MSE:0.002731  KL:1.614444  MAE:0.008476  RMSE:0.052263  CVRMSE:17.847187  R2:0.064360  MSE:0.002732  KL:1.642559  MAE:0.008464  RMSE:0.052267  CVRMSE:17.848602  train_loss:\n",
            "0.0027318819484405702\n",
            "R2:0.055815  MSE:0.002847  KL:0.099192  MAE:0.005478  RMSE:0.053354  CVRMSE:17.643168  test_loss:\n",
            "5.594451637670164e-06\n",
            "R2:0.064737  MSE:0.002731  KL:1.703784  MAE:0.008439  RMSE:0.052257  CVRMSE:17.845003  R2:0.064424  MSE:0.002732  KL:1.719806  MAE:0.008453  RMSE:0.052266  CVRMSE:17.847992  R2:0.064858  MSE:0.002730  KL:1.760289  MAE:0.008442  RMSE:0.052254  CVRMSE:17.843853  R2:0.064447  MSE:0.002732  KL:1.555441  MAE:0.008456  RMSE:0.052265  CVRMSE:17.847773  R2:0.065163  MSE:0.002730  KL:1.668799  MAE:0.008479  RMSE:0.052245  CVRMSE:17.840941  train_loss:\n",
            "0.002729537395457832\n",
            "R2:0.057650  MSE:0.002841  KL:0.102368  MAE:0.005638  RMSE:0.053302  CVRMSE:17.626010  test_loss:\n",
            "5.583506820034072e-06\n",
            "R2:0.064775  MSE:0.002731  KL:1.590389  MAE:0.008488  RMSE:0.052256  CVRMSE:17.844646  R2:0.064599  MSE:0.002731  KL:1.794874  MAE:0.008455  RMSE:0.052261  CVRMSE:17.846317  R2:0.064761  MSE:0.002731  KL:1.739834  MAE:0.008467  RMSE:0.052256  CVRMSE:17.844779  R2:0.064410  MSE:0.002732  KL:1.575354  MAE:0.008464  RMSE:0.052266  CVRMSE:17.848126  R2:0.064437  MSE:0.002732  KL:1.689963  MAE:0.008430  RMSE:0.052265  CVRMSE:17.847866  train_loss:\n",
            "0.002731656665804788\n",
            "R2:0.055927  MSE:0.002846  KL:0.099755  MAE:0.005441  RMSE:0.053351  CVRMSE:17.642119  test_loss:\n",
            "5.593786711159169e-06\n",
            "R2:0.064171  MSE:0.002732  KL:1.776943  MAE:0.008430  RMSE:0.052273  CVRMSE:17.850408  R2:0.064861  MSE:0.002730  KL:1.656352  MAE:0.008475  RMSE:0.052253  CVRMSE:17.843823  R2:0.064593  MSE:0.002731  KL:1.806040  MAE:0.008420  RMSE:0.052261  CVRMSE:17.846378  R2:0.064720  MSE:0.002731  KL:1.621032  MAE:0.008485  RMSE:0.052257  CVRMSE:17.845166  R2:0.063987  MSE:0.002733  KL:1.650493  MAE:0.008423  RMSE:0.052278  CVRMSE:17.852159  train_loss:\n",
            "0.002732970946834444\n",
            "R2:0.056401  MSE:0.002845  KL:0.101754  MAE:0.005423  RMSE:0.053337  CVRMSE:17.637684  test_loss:\n",
            "5.590964178772395e-06\n",
            "R2:0.064141  MSE:0.002733  KL:1.713022  MAE:0.008435  RMSE:0.052274  CVRMSE:17.850694  R2:0.064961  MSE:0.002730  KL:1.662093  MAE:0.008489  RMSE:0.052251  CVRMSE:17.842868  R2:0.064520  MSE:0.002731  KL:1.641923  MAE:0.008471  RMSE:0.052263  CVRMSE:17.847071  R2:0.064870  MSE:0.002730  KL:1.722191  MAE:0.008447  RMSE:0.052253  CVRMSE:17.843740  R2:0.065105  MSE:0.002730  KL:1.701498  MAE:0.008500  RMSE:0.052247  CVRMSE:17.841497  train_loss:\n",
            "0.0027297074964130513\n",
            "R2:0.056810  MSE:0.002844  KL:0.104619  MAE:0.005862  RMSE:0.053326  CVRMSE:17.633864  test_loss:\n",
            "5.588473594955181e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.064456  MSE:0.002732  KL:1.592869  MAE:0.008475  RMSE:0.052265  CVRMSE:17.847686  R2:0.064599  MSE:0.002731  KL:1.694082  MAE:0.008464  RMSE:0.052261  CVRMSE:17.846322  R2:0.064635  MSE:0.002731  KL:1.742830  MAE:0.008471  RMSE:0.052260  CVRMSE:17.845979  R2:0.064600  MSE:0.002731  KL:1.617236  MAE:0.008486  RMSE:0.052261  CVRMSE:17.846311  R2:0.064951  MSE:0.002730  KL:1.751280  MAE:0.008426  RMSE:0.052251  CVRMSE:17.842962  train_loss:\n",
            "0.002730155742312368\n",
            "R2:0.056884  MSE:0.002843  KL:0.100740  MAE:0.005550  RMSE:0.053324  CVRMSE:17.633171  test_loss:\n",
            "5.588077468592933e-06\n",
            "R2:0.064745  MSE:0.002731  KL:1.648253  MAE:0.008481  RMSE:0.052257  CVRMSE:17.844927  R2:0.064765  MSE:0.002731  KL:1.741393  MAE:0.008428  RMSE:0.052256  CVRMSE:17.844735  R2:0.065008  MSE:0.002730  KL:1.755607  MAE:0.008450  RMSE:0.052249  CVRMSE:17.842421  R2:0.064726  MSE:0.002731  KL:1.748324  MAE:0.008494  RMSE:0.052257  CVRMSE:17.845111  R2:0.064690  MSE:0.002731  KL:1.677063  MAE:0.008428  RMSE:0.052258  CVRMSE:17.845457  train_loss:\n",
            "0.002730919377886199\n",
            "R2:0.057752  MSE:0.002841  KL:0.101738  MAE:0.005950  RMSE:0.053299  CVRMSE:17.625056  test_loss:\n",
            "5.582871943918687e-06\n",
            "R2:0.064818  MSE:0.002731  KL:1.682847  MAE:0.008463  RMSE:0.052255  CVRMSE:17.844230  R2:0.064639  MSE:0.002731  KL:1.611868  MAE:0.008476  RMSE:0.052260  CVRMSE:17.845942  R2:0.064877  MSE:0.002730  KL:1.727803  MAE:0.008470  RMSE:0.052253  CVRMSE:17.843669  R2:0.064792  MSE:0.002731  KL:1.647050  MAE:0.008507  RMSE:0.052255  CVRMSE:17.844479  R2:0.064296  MSE:0.002732  KL:1.670514  MAE:0.008417  RMSE:0.052269  CVRMSE:17.849212  train_loss:\n",
            "0.0027320687873782336\n",
            "R2:0.056703  MSE:0.002844  KL:0.099643  MAE:0.005468  RMSE:0.053329  CVRMSE:17.634862  test_loss:\n",
            "5.5891627580563256e-06\n",
            "R2:0.064435  MSE:0.002732  KL:1.785619  MAE:0.008453  RMSE:0.052265  CVRMSE:17.847890  R2:0.064455  MSE:0.002732  KL:1.638296  MAE:0.008452  RMSE:0.052265  CVRMSE:17.847695  R2:0.064677  MSE:0.002731  KL:1.647132  MAE:0.008501  RMSE:0.052259  CVRMSE:17.845576  R2:0.064177  MSE:0.002732  KL:1.655989  MAE:0.008419  RMSE:0.052273  CVRMSE:17.850350  R2:0.064499  MSE:0.002731  KL:1.699778  MAE:0.008460  RMSE:0.052264  CVRMSE:17.847277  train_loss:\n",
            "0.0027314764952395573\n",
            "R2:0.056021  MSE:0.002846  KL:0.099867  MAE:0.005634  RMSE:0.053348  CVRMSE:17.641239  test_loss:\n",
            "5.593206878486648e-06\n",
            "R2:0.064360  MSE:0.002732  KL:1.766848  MAE:0.008418  RMSE:0.052267  CVRMSE:17.848604  R2:0.065001  MSE:0.002730  KL:1.761646  MAE:0.008470  RMSE:0.052249  CVRMSE:17.842486  R2:0.064799  MSE:0.002731  KL:1.598646  MAE:0.008444  RMSE:0.052255  CVRMSE:17.844410  R2:0.064598  MSE:0.002731  KL:1.779596  MAE:0.008444  RMSE:0.052261  CVRMSE:17.846327  R2:0.065238  MSE:0.002729  KL:1.784559  MAE:0.008451  RMSE:0.052243  CVRMSE:17.840222  train_loss:\n",
            "0.0027293174442505237\n",
            "R2:0.057232  MSE:0.002842  KL:0.099907  MAE:0.005697  RMSE:0.053314  CVRMSE:17.629924  test_loss:\n",
            "5.585997541951403e-06\n",
            "R2:0.065105  MSE:0.002730  KL:1.657976  MAE:0.008486  RMSE:0.052247  CVRMSE:17.841493  R2:0.064110  MSE:0.002733  KL:1.639189  MAE:0.008443  RMSE:0.052274  CVRMSE:17.850989  R2:0.064214  MSE:0.002732  KL:1.686351  MAE:0.008430  RMSE:0.052271  CVRMSE:17.849989  R2:0.064490  MSE:0.002732  KL:1.645759  MAE:0.008516  RMSE:0.052264  CVRMSE:17.847363  R2:0.064638  MSE:0.002731  KL:1.626688  MAE:0.008490  RMSE:0.052260  CVRMSE:17.845946  train_loss:\n",
            "0.0027310689594696774\n",
            "R2:0.055709  MSE:0.002847  KL:0.098952  MAE:0.005477  RMSE:0.053357  CVRMSE:17.644153  test_loss:\n",
            "5.595081306262181e-06\n",
            "R2:0.064428  MSE:0.002732  KL:1.654667  MAE:0.008476  RMSE:0.052266  CVRMSE:17.847951  R2:0.065021  MSE:0.002730  KL:1.610360  MAE:0.008462  RMSE:0.052249  CVRMSE:17.842298  R2:0.064430  MSE:0.002732  KL:1.718909  MAE:0.008440  RMSE:0.052265  CVRMSE:17.847933  R2:0.065105  MSE:0.002730  KL:1.674069  MAE:0.008469  RMSE:0.052247  CVRMSE:17.841495  R2:0.064707  MSE:0.002731  KL:1.719127  MAE:0.008444  RMSE:0.052258  CVRMSE:17.845289  train_loss:\n",
            "0.002730867949597845\n",
            "R2:0.058644  MSE:0.002838  KL:0.107497  MAE:0.005566  RMSE:0.053274  CVRMSE:17.616709  test_loss:\n",
            "5.5775863336024675e-06\n",
            "R2:0.064353  MSE:0.002732  KL:1.727802  MAE:0.008409  RMSE:0.052268  CVRMSE:17.848665  R2:0.064387  MSE:0.002732  KL:1.726828  MAE:0.008449  RMSE:0.052267  CVRMSE:17.848347  R2:0.064499  MSE:0.002731  KL:1.736829  MAE:0.008437  RMSE:0.052264  CVRMSE:17.847275  R2:0.065037  MSE:0.002730  KL:1.669933  MAE:0.008503  RMSE:0.052248  CVRMSE:17.842142  R2:0.064688  MSE:0.002731  KL:1.624539  MAE:0.008475  RMSE:0.052258  CVRMSE:17.845472  train_loss:\n",
            "0.0027309240503649882\n",
            "R2:0.057302  MSE:0.002842  KL:0.103007  MAE:0.005942  RMSE:0.053312  CVRMSE:17.629261  test_loss:\n",
            "5.585540136620533e-06\n",
            "R2:0.064534  MSE:0.002731  KL:1.610256  MAE:0.008472  RMSE:0.052263  CVRMSE:17.846942  R2:0.064132  MSE:0.002733  KL:1.626739  MAE:0.008468  RMSE:0.052274  CVRMSE:17.850777  R2:0.064346  MSE:0.002732  KL:1.635022  MAE:0.008432  RMSE:0.052268  CVRMSE:17.848739  R2:0.065261  MSE:0.002729  KL:1.727270  MAE:0.008456  RMSE:0.052242  CVRMSE:17.840005  R2:0.064944  MSE:0.002730  KL:1.641109  MAE:0.008486  RMSE:0.052251  CVRMSE:17.843026  train_loss:\n",
            "0.0027301755236011\n",
            "R2:0.058072  MSE:0.002840  KL:0.102273  MAE:0.005725  RMSE:0.053290  CVRMSE:17.622061  test_loss:\n",
            "5.580986415471979e-06\n",
            "R2:0.064712  MSE:0.002731  KL:1.655417  MAE:0.008482  RMSE:0.052258  CVRMSE:17.845246  R2:0.064145  MSE:0.002733  KL:1.641376  MAE:0.008444  RMSE:0.052273  CVRMSE:17.850651  R2:0.064958  MSE:0.002730  KL:1.688360  MAE:0.008464  RMSE:0.052251  CVRMSE:17.842900  R2:0.064856  MSE:0.002730  KL:1.657414  MAE:0.008444  RMSE:0.052254  CVRMSE:17.843870  R2:0.064869  MSE:0.002730  KL:1.729027  MAE:0.008463  RMSE:0.052253  CVRMSE:17.843741  train_loss:\n",
            "0.0027303943356395276\n",
            "R2:0.058536  MSE:0.002838  KL:0.105653  MAE:0.005822  RMSE:0.053277  CVRMSE:17.617721  test_loss:\n",
            "5.578207557727191e-06\n",
            "R2:0.065107  MSE:0.002730  KL:1.720739  MAE:0.008453  RMSE:0.052247  CVRMSE:17.841474  R2:0.064952  MSE:0.002730  KL:1.763707  MAE:0.008457  RMSE:0.052251  CVRMSE:17.842950  R2:0.065216  MSE:0.002729  KL:1.725312  MAE:0.008449  RMSE:0.052244  CVRMSE:17.840438  R2:0.064796  MSE:0.002731  KL:1.714522  MAE:0.008497  RMSE:0.052255  CVRMSE:17.844441  R2:0.064561  MSE:0.002731  KL:1.804187  MAE:0.008441  RMSE:0.052262  CVRMSE:17.846684  train_loss:\n",
            "0.002731294803988363\n",
            "R2:0.058216  MSE:0.002839  KL:0.105068  MAE:0.005601  RMSE:0.053286  CVRMSE:17.620717  test_loss:\n",
            "5.58013684098773e-06\n",
            "R2:0.064939  MSE:0.002730  KL:1.663067  MAE:0.008448  RMSE:0.052251  CVRMSE:17.843077  R2:0.064288  MSE:0.002732  KL:1.736287  MAE:0.008429  RMSE:0.052269  CVRMSE:17.849288  R2:0.064385  MSE:0.002732  KL:1.806806  MAE:0.008428  RMSE:0.052267  CVRMSE:17.848359  R2:0.064311  MSE:0.002732  KL:1.710324  MAE:0.008449  RMSE:0.052269  CVRMSE:17.849068  R2:0.065093  MSE:0.002730  KL:1.741876  MAE:0.008489  RMSE:0.052247  CVRMSE:17.841604  train_loss:\n",
            "0.0027297403431206925\n",
            "R2:0.057998  MSE:0.002840  KL:0.105469  MAE:0.005789  RMSE:0.053292  CVRMSE:17.622758  test_loss:\n",
            "5.581412084719874e-06\n",
            "R2:0.064229  MSE:0.002732  KL:1.734878  MAE:0.008424  RMSE:0.052271  CVRMSE:17.849851  R2:0.064454  MSE:0.002732  KL:1.699000  MAE:0.008432  RMSE:0.052265  CVRMSE:17.847706  R2:0.064650  MSE:0.002731  KL:1.603711  MAE:0.008456  RMSE:0.052259  CVRMSE:17.845839  R2:0.064931  MSE:0.002730  KL:1.741527  MAE:0.008466  RMSE:0.052251  CVRMSE:17.843153  R2:0.063759  MSE:0.002734  KL:1.648766  MAE:0.008450  RMSE:0.052284  CVRMSE:17.854333  train_loss:\n",
            "0.0027336366139315785\n",
            "R2:0.056284  MSE:0.002845  KL:0.101609  MAE:0.005656  RMSE:0.053341  CVRMSE:17.638776  test_loss:\n",
            "5.5916298944943015e-06\n",
            "R2:0.064626  MSE:0.002731  KL:1.791534  MAE:0.008473  RMSE:0.052260  CVRMSE:17.846065  R2:0.064404  MSE:0.002732  KL:1.754908  MAE:0.008428  RMSE:0.052266  CVRMSE:17.848186  R2:0.064342  MSE:0.002732  KL:1.615340  MAE:0.008433  RMSE:0.052268  CVRMSE:17.848769  R2:0.064916  MSE:0.002730  KL:1.763347  MAE:0.008433  RMSE:0.052252  CVRMSE:17.843301  R2:0.064526  MSE:0.002731  KL:1.620830  MAE:0.008421  RMSE:0.052263  CVRMSE:17.847020  train_loss:\n",
            "0.0027313978563197555\n",
            "R2:0.059228  MSE:0.002836  KL:0.106865  MAE:0.005774  RMSE:0.053257  CVRMSE:17.611245  test_loss:\n",
            "5.574090409887042e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.064303  MSE:0.002732  KL:1.664241  MAE:0.008439  RMSE:0.052269  CVRMSE:17.849148  R2:0.064583  MSE:0.002731  KL:1.576043  MAE:0.008445  RMSE:0.052261  CVRMSE:17.846474  R2:0.064168  MSE:0.002732  KL:1.796350  MAE:0.008418  RMSE:0.052273  CVRMSE:17.850430  R2:0.064346  MSE:0.002732  KL:1.714394  MAE:0.008441  RMSE:0.052268  CVRMSE:17.848737  R2:0.065047  MSE:0.002730  KL:1.782470  MAE:0.008482  RMSE:0.052248  CVRMSE:17.842051  train_loss:\n",
            "0.002729876980947571\n",
            "R2:0.058913  MSE:0.002837  KL:0.106155  MAE:0.006180  RMSE:0.053266  CVRMSE:17.614195  test_loss:\n",
            "5.5759237337438345e-06\n",
            "R2:0.064603  MSE:0.002731  KL:1.704743  MAE:0.008488  RMSE:0.052261  CVRMSE:17.846283  R2:0.065151  MSE:0.002730  KL:1.568244  MAE:0.008501  RMSE:0.052245  CVRMSE:17.841057  R2:0.064490  MSE:0.002732  KL:1.684507  MAE:0.008433  RMSE:0.052264  CVRMSE:17.847359  R2:0.064327  MSE:0.002732  KL:1.678769  MAE:0.008466  RMSE:0.052268  CVRMSE:17.848919  R2:0.064538  MSE:0.002731  KL:1.686070  MAE:0.008441  RMSE:0.052262  CVRMSE:17.846901  train_loss:\n",
            "0.0027313612116459606\n",
            "R2:0.055627  MSE:0.002847  KL:0.099574  MAE:0.005663  RMSE:0.053359  CVRMSE:17.644922  test_loss:\n",
            "5.595549419813841e-06\n",
            "R2:0.064881  MSE:0.002730  KL:1.692004  MAE:0.008489  RMSE:0.052253  CVRMSE:17.843635  R2:0.065096  MSE:0.002730  KL:1.629322  MAE:0.008492  RMSE:0.052247  CVRMSE:17.841580  R2:0.064702  MSE:0.002731  KL:1.670928  MAE:0.008465  RMSE:0.052258  CVRMSE:17.845336  R2:0.064777  MSE:0.002731  KL:1.693296  MAE:0.008478  RMSE:0.052256  CVRMSE:17.844622  R2:0.064471  MSE:0.002732  KL:1.626087  MAE:0.008459  RMSE:0.052264  CVRMSE:17.847547  train_loss:\n",
            "0.0027315591005940333\n",
            "R2:0.056882  MSE:0.002843  KL:0.100763  MAE:0.006066  RMSE:0.053324  CVRMSE:17.633189  test_loss:\n",
            "5.588037572830648e-06\n",
            "R2:0.064721  MSE:0.002731  KL:1.709157  MAE:0.008455  RMSE:0.052257  CVRMSE:17.845154  R2:0.064393  MSE:0.002732  KL:1.598958  MAE:0.008463  RMSE:0.052266  CVRMSE:17.848282  R2:0.064486  MSE:0.002732  KL:1.668785  MAE:0.008460  RMSE:0.052264  CVRMSE:17.847396  R2:0.064551  MSE:0.002731  KL:1.594904  MAE:0.008474  RMSE:0.052262  CVRMSE:17.846776  R2:0.065195  MSE:0.002729  KL:1.621939  MAE:0.008479  RMSE:0.052244  CVRMSE:17.840639  train_loss:\n",
            "0.002729444899718322\n",
            "R2:0.057466  MSE:0.002842  KL:0.106263  MAE:0.006308  RMSE:0.053307  CVRMSE:17.627729  test_loss:\n",
            "5.584525537993261e-06\n",
            "R2:0.065585  MSE:0.002728  KL:1.696895  MAE:0.008483  RMSE:0.052233  CVRMSE:17.836912  R2:0.064765  MSE:0.002731  KL:1.728180  MAE:0.008436  RMSE:0.052256  CVRMSE:17.844737  R2:0.064085  MSE:0.002733  KL:1.662856  MAE:0.008432  RMSE:0.052275  CVRMSE:17.851226  R2:0.064873  MSE:0.002730  KL:1.667781  MAE:0.008445  RMSE:0.052253  CVRMSE:17.843709  R2:0.064537  MSE:0.002731  KL:1.614564  MAE:0.008476  RMSE:0.052262  CVRMSE:17.846913  train_loss:\n",
            "0.002731365166207848\n",
            "R2:0.056005  MSE:0.002846  KL:0.101325  MAE:0.005435  RMSE:0.053348  CVRMSE:17.641391  test_loss:\n",
            "5.593323386569179e-06\n",
            "R2:0.064874  MSE:0.002730  KL:1.614117  MAE:0.008474  RMSE:0.052253  CVRMSE:17.843695  R2:0.065428  MSE:0.002729  KL:1.678390  MAE:0.008494  RMSE:0.052238  CVRMSE:17.838408  R2:0.064734  MSE:0.002731  KL:1.566564  MAE:0.008494  RMSE:0.052257  CVRMSE:17.845029  R2:0.064831  MSE:0.002731  KL:1.667537  MAE:0.008448  RMSE:0.052254  CVRMSE:17.844106  R2:0.064284  MSE:0.002732  KL:1.615862  MAE:0.008426  RMSE:0.052270  CVRMSE:17.849326  train_loss:\n",
            "0.0027321036289597676\n",
            "R2:0.055919  MSE:0.002846  KL:0.098134  MAE:0.005489  RMSE:0.053351  CVRMSE:17.642188  test_loss:\n",
            "5.593830542514569e-06\n",
            "R2:0.064102  MSE:0.002733  KL:1.734921  MAE:0.008456  RMSE:0.052275  CVRMSE:17.851065  R2:0.065052  MSE:0.002730  KL:1.636563  MAE:0.008430  RMSE:0.052248  CVRMSE:17.841999  R2:0.065003  MSE:0.002730  KL:1.759218  MAE:0.008448  RMSE:0.052249  CVRMSE:17.842471  R2:0.064959  MSE:0.002730  KL:1.715355  MAE:0.008431  RMSE:0.052251  CVRMSE:17.842890  R2:0.065016  MSE:0.002730  KL:1.719266  MAE:0.008451  RMSE:0.052249  CVRMSE:17.842347  train_loss:\n",
            "0.002729967643436505\n",
            "R2:0.057718  MSE:0.002841  KL:0.101965  MAE:0.005889  RMSE:0.053300  CVRMSE:17.625372  test_loss:\n",
            "5.583074587849222e-06\n",
            "R2:0.065050  MSE:0.002730  KL:1.629633  MAE:0.008458  RMSE:0.052248  CVRMSE:17.842014  R2:0.064473  MSE:0.002732  KL:1.788692  MAE:0.008483  RMSE:0.052264  CVRMSE:17.847523  R2:0.064969  MSE:0.002730  KL:1.691136  MAE:0.008445  RMSE:0.052250  CVRMSE:17.842787  R2:0.064622  MSE:0.002731  KL:1.709776  MAE:0.008410  RMSE:0.052260  CVRMSE:17.846102  R2:0.064569  MSE:0.002731  KL:1.708379  MAE:0.008417  RMSE:0.052262  CVRMSE:17.846604  train_loss:\n",
            "0.0027312705061385005\n",
            "R2:0.057166  MSE:0.002843  KL:0.101119  MAE:0.005679  RMSE:0.053316  CVRMSE:17.630540  test_loss:\n",
            "5.5863880369425645e-06\n",
            "R2:0.065108  MSE:0.002730  KL:1.691200  MAE:0.008460  RMSE:0.052247  CVRMSE:17.841462  R2:0.063752  MSE:0.002734  KL:1.733131  MAE:0.008404  RMSE:0.052284  CVRMSE:17.854399  R2:0.064319  MSE:0.002732  KL:1.660698  MAE:0.008465  RMSE:0.052269  CVRMSE:17.848991  R2:0.064792  MSE:0.002731  KL:1.552379  MAE:0.008474  RMSE:0.052255  CVRMSE:17.844480  R2:0.065318  MSE:0.002729  KL:1.680250  MAE:0.008504  RMSE:0.052241  CVRMSE:17.839459  train_loss:\n",
            "0.0027290839380758308\n",
            "R2:0.057053  MSE:0.002843  KL:0.102430  MAE:0.005539  RMSE:0.053319  CVRMSE:17.631593  test_loss:\n",
            "5.5870691964109625e-06\n",
            "R2:0.064729  MSE:0.002731  KL:1.749286  MAE:0.008418  RMSE:0.052257  CVRMSE:17.845083  R2:0.065105  MSE:0.002730  KL:1.593176  MAE:0.008480  RMSE:0.052247  CVRMSE:17.841492  R2:0.064811  MSE:0.002731  KL:1.654757  MAE:0.008450  RMSE:0.052255  CVRMSE:17.844300  R2:0.064529  MSE:0.002731  KL:1.580669  MAE:0.008469  RMSE:0.052263  CVRMSE:17.846993  R2:0.064667  MSE:0.002731  KL:1.741498  MAE:0.008460  RMSE:0.052259  CVRMSE:17.845673  train_loss:\n",
            "0.002730985447720919\n",
            "R2:0.056050  MSE:0.002846  KL:0.098864  MAE:0.005795  RMSE:0.053347  CVRMSE:17.640964  test_loss:\n",
            "5.593021141463168e-06\n",
            "R2:0.065145  MSE:0.002730  KL:1.636157  MAE:0.008458  RMSE:0.052245  CVRMSE:17.841112  R2:0.064358  MSE:0.002732  KL:1.728840  MAE:0.008432  RMSE:0.052267  CVRMSE:17.848619  R2:0.064692  MSE:0.002731  KL:1.708726  MAE:0.008437  RMSE:0.052258  CVRMSE:17.845431  R2:0.064572  MSE:0.002731  KL:1.697564  MAE:0.008465  RMSE:0.052261  CVRMSE:17.846583  R2:0.064802  MSE:0.002731  KL:1.730594  MAE:0.008440  RMSE:0.052255  CVRMSE:17.844384  train_loss:\n",
            "0.0027305908398629515\n",
            "R2:0.057932  MSE:0.002840  KL:0.105275  MAE:0.005720  RMSE:0.053294  CVRMSE:17.623372  test_loss:\n",
            "5.581813472600964e-06\n",
            "R2:0.065320  MSE:0.002729  KL:1.751046  MAE:0.008470  RMSE:0.052241  CVRMSE:17.839443  R2:0.064585  MSE:0.002731  KL:1.711296  MAE:0.008454  RMSE:0.052261  CVRMSE:17.846452  R2:0.064668  MSE:0.002731  KL:1.682905  MAE:0.008424  RMSE:0.052259  CVRMSE:17.845662  R2:0.064739  MSE:0.002731  KL:1.598468  MAE:0.008507  RMSE:0.052257  CVRMSE:17.844985  R2:0.063797  MSE:0.002734  KL:1.720436  MAE:0.008428  RMSE:0.052283  CVRMSE:17.853971  train_loss:\n",
            "0.002733525740380088\n",
            "R2:0.056474  MSE:0.002845  KL:0.102183  MAE:0.005503  RMSE:0.053335  CVRMSE:17.637004  test_loss:\n",
            "5.590517365418668e-06\n",
            "R2:0.064945  MSE:0.002730  KL:1.726274  MAE:0.008452  RMSE:0.052251  CVRMSE:17.843025  R2:0.064112  MSE:0.002733  KL:1.682746  MAE:0.008458  RMSE:0.052274  CVRMSE:17.850965  R2:0.064804  MSE:0.002731  KL:1.749214  MAE:0.008444  RMSE:0.052255  CVRMSE:17.844369  R2:0.064830  MSE:0.002731  KL:1.704940  MAE:0.008470  RMSE:0.052254  CVRMSE:17.844121  R2:0.064242  MSE:0.002732  KL:1.632634  MAE:0.008466  RMSE:0.052271  CVRMSE:17.849726  train_loss:\n",
            "0.0027322262019477623\n",
            "R2:0.053657  MSE:0.002853  KL:0.098915  MAE:0.005327  RMSE:0.053415  CVRMSE:17.663316  test_loss:\n",
            "5.607323789746236e-06\n",
            "R2:0.064221  MSE:0.002732  KL:1.587938  MAE:0.008453  RMSE:0.052271  CVRMSE:17.849931  R2:0.064542  MSE:0.002731  KL:1.664459  MAE:0.008464  RMSE:0.052262  CVRMSE:17.846861  R2:0.064712  MSE:0.002731  KL:1.650618  MAE:0.008474  RMSE:0.052258  CVRMSE:17.845247  R2:0.063966  MSE:0.002733  KL:1.679771  MAE:0.008440  RMSE:0.052278  CVRMSE:17.852359  R2:0.065092  MSE:0.002730  KL:1.665521  MAE:0.008459  RMSE:0.052247  CVRMSE:17.841617  train_loss:\n",
            "0.002729744173084756\n",
            "R2:0.057640  MSE:0.002841  KL:0.105145  MAE:0.005484  RMSE:0.053302  CVRMSE:17.626106  test_loss:\n",
            "5.583582497864061e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.064688  MSE:0.002731  KL:1.731077  MAE:0.008439  RMSE:0.052258  CVRMSE:17.845475  R2:0.064379  MSE:0.002732  KL:1.767459  MAE:0.008429  RMSE:0.052267  CVRMSE:17.848424  R2:0.064461  MSE:0.002732  KL:1.777455  MAE:0.008439  RMSE:0.052265  CVRMSE:17.847634  R2:0.064899  MSE:0.002730  KL:1.683874  MAE:0.008465  RMSE:0.052252  CVRMSE:17.843460  R2:0.063806  MSE:0.002733  KL:1.786041  MAE:0.008441  RMSE:0.052283  CVRMSE:17.853883  train_loss:\n",
            "0.002733499015558268\n",
            "R2:0.056011  MSE:0.002846  KL:0.099016  MAE:0.005763  RMSE:0.053348  CVRMSE:17.641328  test_loss:\n",
            "5.593251218773543e-06\n",
            "R2:0.064669  MSE:0.002731  KL:1.590624  MAE:0.008481  RMSE:0.052259  CVRMSE:17.845651  R2:0.064686  MSE:0.002731  KL:1.735661  MAE:0.008474  RMSE:0.052258  CVRMSE:17.845490  R2:0.065101  MSE:0.002730  KL:1.621649  MAE:0.008488  RMSE:0.052247  CVRMSE:17.841530  R2:0.064795  MSE:0.002731  KL:1.760341  MAE:0.008435  RMSE:0.052255  CVRMSE:17.844450  R2:0.064473  MSE:0.002732  KL:1.624434  MAE:0.008470  RMSE:0.052264  CVRMSE:17.847527  train_loss:\n",
            "0.0027315529838789813\n",
            "R2:0.056022  MSE:0.002846  KL:0.102847  MAE:0.005475  RMSE:0.053348  CVRMSE:17.641225  test_loss:\n",
            "5.593209324628976e-06\n",
            "R2:0.064888  MSE:0.002730  KL:1.784816  MAE:0.008441  RMSE:0.052253  CVRMSE:17.843569  R2:0.064476  MSE:0.002732  KL:1.577582  MAE:0.008478  RMSE:0.052264  CVRMSE:17.847492  R2:0.064659  MSE:0.002731  KL:1.685556  MAE:0.008480  RMSE:0.052259  CVRMSE:17.845748  R2:0.064667  MSE:0.002731  KL:1.729333  MAE:0.008454  RMSE:0.052259  CVRMSE:17.845675  R2:0.064767  MSE:0.002731  KL:1.715840  MAE:0.008494  RMSE:0.052256  CVRMSE:17.844719  train_loss:\n",
            "0.002730693440230334\n",
            "R2:0.057494  MSE:0.002842  KL:0.102949  MAE:0.005556  RMSE:0.053306  CVRMSE:17.627470  test_loss:\n",
            "5.584443543307622e-06\n",
            "R2:0.065348  MSE:0.002729  KL:1.634307  MAE:0.008484  RMSE:0.052240  CVRMSE:17.839170  R2:0.064330  MSE:0.002732  KL:1.759439  MAE:0.008438  RMSE:0.052268  CVRMSE:17.848886  R2:0.064924  MSE:0.002730  KL:1.633864  MAE:0.008442  RMSE:0.052252  CVRMSE:17.843219  R2:0.065477  MSE:0.002729  KL:1.622608  MAE:0.008508  RMSE:0.052236  CVRMSE:17.837939  R2:0.064564  MSE:0.002731  KL:1.742164  MAE:0.008451  RMSE:0.052262  CVRMSE:17.846652  train_loss:\n",
            "0.0027312850960671386\n",
            "R2:0.057167  MSE:0.002843  KL:0.102265  MAE:0.005604  RMSE:0.053316  CVRMSE:17.630524  test_loss:\n",
            "5.5863818494756735e-06\n",
            "R2:0.064631  MSE:0.002731  KL:1.677863  MAE:0.008460  RMSE:0.052260  CVRMSE:17.846020  R2:0.064964  MSE:0.002730  KL:1.632691  MAE:0.008458  RMSE:0.052251  CVRMSE:17.842840  R2:0.064466  MSE:0.002732  KL:1.655661  MAE:0.008454  RMSE:0.052264  CVRMSE:17.847595  R2:0.064554  MSE:0.002731  KL:1.742515  MAE:0.008480  RMSE:0.052262  CVRMSE:17.846749  R2:0.064445  MSE:0.002732  KL:1.559992  MAE:0.008456  RMSE:0.052265  CVRMSE:17.847793  train_loss:\n",
            "0.002731634258284535\n",
            "R2:0.057446  MSE:0.002842  KL:0.103365  MAE:0.005451  RMSE:0.053308  CVRMSE:17.627915  test_loss:\n",
            "5.584739565951664e-06\n",
            "R2:0.064623  MSE:0.002731  KL:1.783031  MAE:0.008466  RMSE:0.052260  CVRMSE:17.846094  R2:0.064554  MSE:0.002731  KL:1.629273  MAE:0.008455  RMSE:0.052262  CVRMSE:17.846755  R2:0.064558  MSE:0.002731  KL:1.640603  MAE:0.008466  RMSE:0.052262  CVRMSE:17.846717  R2:0.064830  MSE:0.002731  KL:1.723560  MAE:0.008484  RMSE:0.052254  CVRMSE:17.844116  R2:0.064219  MSE:0.002732  KL:1.539469  MAE:0.008470  RMSE:0.052271  CVRMSE:17.849949  train_loss:\n",
            "0.0027322943243339557\n",
            "R2:0.057344  MSE:0.002842  KL:0.101767  MAE:0.005935  RMSE:0.053311  CVRMSE:17.628876  test_loss:\n",
            "5.585302129990719e-06\n",
            "R2:0.064564  MSE:0.002731  KL:1.743849  MAE:0.008482  RMSE:0.052262  CVRMSE:17.846660  R2:0.064716  MSE:0.002731  KL:1.718560  MAE:0.008428  RMSE:0.052257  CVRMSE:17.845206  R2:0.064114  MSE:0.002733  KL:1.622179  MAE:0.008466  RMSE:0.052274  CVRMSE:17.850952  R2:0.064451  MSE:0.002732  KL:1.828933  MAE:0.008446  RMSE:0.052265  CVRMSE:17.847737  R2:0.064592  MSE:0.002731  KL:1.821282  MAE:0.008432  RMSE:0.052261  CVRMSE:17.846387  train_loss:\n",
            "0.0027312041319765786\n",
            "R2:0.057022  MSE:0.002843  KL:0.102018  MAE:0.005745  RMSE:0.053320  CVRMSE:17.631887  test_loss:\n",
            "5.587234231942514e-06\n",
            "R2:0.064571  MSE:0.002731  KL:1.759261  MAE:0.008478  RMSE:0.052262  CVRMSE:17.846588  R2:0.064411  MSE:0.002732  KL:1.588577  MAE:0.008465  RMSE:0.052266  CVRMSE:17.848112  R2:0.064917  MSE:0.002730  KL:1.671352  MAE:0.008462  RMSE:0.052252  CVRMSE:17.843292  R2:0.065154  MSE:0.002730  KL:1.579266  MAE:0.008482  RMSE:0.052245  CVRMSE:17.841026  R2:0.065069  MSE:0.002730  KL:1.634542  MAE:0.008469  RMSE:0.052248  CVRMSE:17.841833  train_loss:\n",
            "0.0027298102438719535\n",
            "R2:0.055951  MSE:0.002846  KL:0.099865  MAE:0.005352  RMSE:0.053350  CVRMSE:17.641894  test_loss:\n",
            "5.593656633134764e-06\n",
            "R2:0.065219  MSE:0.002729  KL:1.652297  MAE:0.008494  RMSE:0.052243  CVRMSE:17.840410  R2:0.064630  MSE:0.002731  KL:1.746327  MAE:0.008460  RMSE:0.052260  CVRMSE:17.846024  R2:0.064814  MSE:0.002731  KL:1.671631  MAE:0.008462  RMSE:0.052255  CVRMSE:17.844267  R2:0.064231  MSE:0.002732  KL:1.745607  MAE:0.008453  RMSE:0.052271  CVRMSE:17.849828  R2:0.064966  MSE:0.002730  KL:1.619141  MAE:0.008491  RMSE:0.052250  CVRMSE:17.842818  train_loss:\n",
            "0.0027301116588341933\n",
            "R2:0.058653  MSE:0.002838  KL:0.109905  MAE:0.005779  RMSE:0.053274  CVRMSE:17.616628  test_loss:\n",
            "5.577506289142657e-06\n",
            "R2:0.065372  MSE:0.002729  KL:1.634262  MAE:0.008498  RMSE:0.052239  CVRMSE:17.838950  R2:0.064861  MSE:0.002730  KL:1.685126  MAE:0.008458  RMSE:0.052253  CVRMSE:17.843819  R2:0.064940  MSE:0.002730  KL:1.667819  MAE:0.008472  RMSE:0.052251  CVRMSE:17.843064  R2:0.065240  MSE:0.002729  KL:1.740805  MAE:0.008500  RMSE:0.052243  CVRMSE:17.840208  R2:0.064854  MSE:0.002730  KL:1.688191  MAE:0.008462  RMSE:0.052254  CVRMSE:17.843889  train_loss:\n",
            "0.002730439365991501\n",
            "R2:0.057309  MSE:0.002842  KL:0.102537  MAE:0.005812  RMSE:0.053312  CVRMSE:17.629197  test_loss:\n",
            "5.5855147712000095e-06\n",
            "Model 0 has MSE on validset:0.030515  Model 0 has KL on validset:0.024179  Model 0 has ATE error on validset:1.008857  "
          ]
        }
      ],
      "source": [
        "from uuid import RFC_4122\n",
        "import torch\n",
        "import math\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "from torch_geometric.utils import add_self_loops,degree\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import ssl\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net,self).__init__()\n",
        "        self.gat1=GATConv(in_channels=25,out_channels=8,heads=8,dropout=0.6)\n",
        "        self.gat2=GATConv(in_channels=64,out_channels=10,heads=1,dropout=0.6)\n",
        "        self.f1 = torch.nn.Linear(140,32)\n",
        "        self.f2 = torch.nn.Linear(32,1)\n",
        "    def forward(self,data):\n",
        "        x,edge_index=data.x, data.edge_index\n",
        "        x=self.gat1(x,edge_index)\n",
        "        x=self.gat2(x,edge_index)\n",
        "        x=x.reshape(-1,140)\n",
        "        x = self.f1(x)\n",
        "        x = self.f2(x)\n",
        "        return x\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "def train():\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "    y_actual = []\n",
        "    y_predicted = []\n",
        "    loss_all=0\n",
        "    for data in train_loader:\n",
        "        loss = 0\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        label = data.y.to(device)\n",
        "        loss = crit(output, label)\n",
        "        loss.backward()\n",
        "        loss_all += data.num_graphs * loss.item()\n",
        "        optimizer.step()\n",
        "        y_actual +=(label).cpu().detach().ravel().tolist()\n",
        "        y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "    \n",
        "    \n",
        "    loss=loss_all/len(Mydata_train)\n",
        "    r2=R2(y_predicted, y_actual)\n",
        "    mse = MSE(y_predicted, y_actual)\n",
        "    kl=kl_divergence(y_predicted, y_actual)\n",
        "\n",
        "    print(\"R2:%f\" % (R2(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MSE:%f\" % (MSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"KL:%f\" % (kl_divergence(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MAE:%f\" % (MAE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"RMSE:%f\" % (RMSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"CVRMSE:%f\" % (CVRMSE(y_predicted, y_actual)),end='  ')\n",
        "\n",
        "    return loss,r2,mse,kl\n",
        "\n",
        "\n",
        "def val():\n",
        "    model.eval()\n",
        "    y_actual = []\n",
        "    y_predicted = []\n",
        "    loss_all=0\n",
        "    for data in test_loader:\n",
        "      loss = 0\n",
        "      data = data.to(device)\n",
        "      output = model(data)\n",
        "      label = data.y.to(device)\n",
        "      y_actual +=(label).cpu().detach().ravel().tolist()\n",
        "      y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "      loss = crit(output, label)\n",
        "      loss_all += loss.item()\n",
        "\n",
        "    \n",
        "    loss = loss_all / len(Mydata_test)\n",
        "    r2=R2(y_predicted, y_actual)\n",
        "    mse = MSE(y_predicted, y_actual)\n",
        "    kl=kl_divergence(y_predicted, y_actual)\n",
        "\n",
        "    print(\"R2:%f\" % (R2(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MSE:%f\" % (MSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"KL:%f\" % (kl_divergence(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MAE:%f\" % (MAE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"RMSE:%f\" % (RMSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"CVRMSE:%f\" % (CVRMSE(y_predicted, y_actual)),end='  ')\n",
        "\n",
        "    return loss,r2, mse,kl\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 256\n",
        "batch_size = 512\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=5e-4)\n",
        "crit = F.mse_loss\n",
        "train_loader = DataLoader(Mydata_train, batch_size=batch_size)\n",
        "test_loader = DataLoader(Mydata_test, batch_size=512)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss,r2,mse,kl=train()\n",
        "    if epoch %5==0:      \n",
        "        print('train_loss:')\n",
        "        print(loss)\n",
        "\n",
        "        loss,r2,mse,kl=val()\n",
        "        print('test_loss:')\n",
        "        print(loss)\n",
        "\n",
        "y_predicted = []\n",
        "for data in test_loader:\n",
        "    loss = 0\n",
        "    data = data.to(device)\n",
        "    output = model(data)\n",
        "    label = data.y.to(device)\n",
        "    y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "df_test[\"y_hat\"]=y_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2txUFKIhFkOA",
        "outputId": "e5f8effd-aa98-40fa-b8e3-610216200cdb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>auuc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y_hat</th>\n",
              "      <td>0.880700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random</th>\n",
              "      <td>0.499185</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            auuc\n",
              "y_hat   0.880700\n",
              "Random  0.499185"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# estimate area under the uplift curve (AUUC)\n",
        "from causalml.metrics import *\n",
        "uplift=df_test.copy()\n",
        "tau_hat=pd.concat([t0,t1], axis=0, join=\"inner\")\n",
        "uplift=pd.concat([uplift,tau_hat], axis=1, join=\"inner\")\n",
        "uplift = uplift.loc[:,~uplift.columns.duplicated()]\n",
        "\n",
        "auuc=auuc_score(uplift, outcome_col='y', treatment_col='T', treatment_effect_col='tau')\n",
        "gat_auuc=pd.DataFrame(auuc[[\"y_hat\",\"Random\"]],columns=['auuc'])\n",
        "gat_auuc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8sZB2KGFkOB",
        "outputId": "4fd406c1-016d-417e-f668-4e71ceada1e2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>AUUC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>S Learner(LR)</th>\n",
              "      <td>0.497983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S Learner(XGB)</th>\n",
              "      <td>0.875572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S Learner(LGBM)</th>\n",
              "      <td>0.883033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GCN (Struct)</th>\n",
              "      <td>0.501865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GCN (Struct+Feature)</th>\n",
              "      <td>0.721959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GCN (Struct+Causal Weighting)</th>\n",
              "      <td>0.732616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GAT (Struct)</th>\n",
              "      <td>0.544286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GAT (Struct+Feature)</th>\n",
              "      <td>0.84763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GAT (Struct+Causal Weighting)</th>\n",
              "      <td>0.8807</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   AUUC\n",
              "S Learner(LR)                  0.497983\n",
              "S Learner(XGB)                 0.875572\n",
              "S Learner(LGBM)                0.883033\n",
              "GCN (Struct)                   0.501865\n",
              "GCN (Struct+Feature)           0.721959\n",
              "GCN (Struct+Causal Weighting)  0.732616\n",
              "GAT (Struct)                   0.544286\n",
              "GAT (Struct+Feature)            0.84763\n",
              "GAT (Struct+Causal Weighting)    0.8807"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result=pd.DataFrame(columns=[['AUUC']])\n",
        "result.loc['S Learner(LR)','AUUC']=0.497983\n",
        "result.loc['S Learner(XGB)','AUUC']=0.875572\n",
        "result.loc['S Learner(LGBM)','AUUC']=0.883033\n",
        "\n",
        "result.loc['GCN (Struct)','AUUC']=0.501865\n",
        "result.loc['GCN (Struct+Feature)','AUUC']=0.721959\n",
        "result.loc['GCN (Struct+Causal Weighting)','AUUC']=gcn_auuc.loc[\"y_hat\"].values\n",
        "\n",
        "result.loc['GAT (Struct)','AUUC']=0.544286\n",
        "result.loc['GAT (Struct+Feature)','AUUC']=0.847630\n",
        "result.loc['GAT (Struct+Causal Weighting)','AUUC']=gat_auuc.loc[\"y_hat\"].values\n",
        "\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "causal_weighting_embedding(10d)_ate(14d).ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "cd9ce680b654e493fbbd5797259a0b454ac6d0effeb2d1358682b4d9ed73e0e9"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}